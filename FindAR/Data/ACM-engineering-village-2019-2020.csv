Title,Author,Author affiliation,Source,Sponsor,Publisher,Volume and Issue,Pages,Issue date,Monograph title,Publication year,Language,ISSN,E-ISSN,ISBN13,DOI,Article number,Conference name,Conference date,Conference location,Abstract,Number of references,Main Heading,Controlled/Subject terms,Uncontrolled terms,Classification code,IPC code,Treatment,Funding details,Funding text,Access type,Database,Copyright,Data Provider,,,
Come to the table! Haere mai ki te tepu!,"Gunn, Mairi (1); Bai, Huidong (2); Sasikumar, Prasanth (2) ","(1) Elam School of Fine Arts, University of Auckland, Auckland, New Zealand (2) Auckland Bioengineering Inst University of Auckland, Auckland, New Zealand ","SIGGRAPH Asia 2019 XR, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 4-5,17-Nov-19,"SIGGRAPH Asia 2019 XR, SA 2019",2019,,,,9.78145E+12,10.1145/3355355.3361898,,"SIGGRAPH Asia 2019 XR - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"Come to the Table! explores whether extended realities (XR) can create a bridge between indigenous people (Maori), descendants from European settlers (Pakeha) and people from other ethnicities, by practicing social inclusion. The experience uses real time depth sensing technology and AR/VR displays to enable participants to view and be part of tabletop conversations with people from different cultural backgrounds, in a playful, explorative and powerful way. © 2019 Association for Computing Machinery.",2,Interactive computer graphics,Augmented reality - Virtual reality,Cultural backgrounds - Indigenous - Indigenous people - Intercultural - Real time - Social inclusion,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Porton: Portable mid-air imaging optical system on glossy materials,"Sano, Ayaka (1); Koizumi, Naoya (2) ","(1) University of Electro-Communications, Tokyo, Japan (2) University of Electro-Communications, JST PRESTO, Tokyo, Japan ","SIGGRAPH Asia 2019 Emerging Technologies, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 38-39,17-Nov-19,"SIGGRAPH Asia 2019 Emerging Technologies, SA 2019",2019,,,,9.78145E+12,10.1145/3355049.3360527,,"SIGGRAPH Asia 2019 Emerging Technologies - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"PortOn is a portable optical system that can form mid-air images that stand on a glossy surface such as a table or the floor. PortOn is composed of micro-mirror array plates (MMAPs), an image light source, a mirror, and polarizing elements. PortOn projects light to form a mid-air image at a position that is easy for a human to see when it is placed on a flat surface. Our contribution is a practical optical design that can be easily installed. We designed the arrangement of the MMAPs, mirror and light source to form a mid-air image by placing it on a flat and glossy surface. The advantage of our method is to erase unnecessary light and show beautiful mid-air image clearly by applying view angle control and polarization operating to the mid-air imaging system. With this method, it is possible to display computer graphics in the real world easily and realize mixed reality interaction. © 2019 Association for Computing Machinery.",2,Optical systems,Augmented reality - Interactive computer graphics - Light sources - Mirrors - Mixed reality - Optical design,Flat surfaces - Glossy surface - Imaging optical systems - Micromirror array - Mid-air image - Polarizing elements - Tabletop displays - View angles,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 741.1 Light/Optics - 741.3 Optical Devices and Systems",,,"Number: JPMJPR16D5, Acronym: JST, Sponsor: Japan Science and Technology Corporation; Number: -, Acronym: PRESTO, Sponsor: Precursory Research for Embryonic Science and Technology; ","This research was supported by PRESTO, JST (JPMJPR16D5).",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
I'm tired of demos: An adaptive MR remote collaborative platform,"Wang, Peng (1, 2); Bai, Xiaoliang (1, 2); Billinghurst, Mark (1, 2, 3); Han, Dechuan (1, 2); Zhang, Shusheng (1, 2); He, Weiping (1, 2); Zhang, Xiangyu (1); Yan, Yuxiang (1) ","(1) Northwestern Polytechnical University, Xi'an, China (2) Cyber-Reality Innovation Center, Nanjing, China (3) University of South Australia, Mawson Lakes, Australia ","SIGGRAPH Asia 2019 XR, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 17-18,17-Nov-19,"SIGGRAPH Asia 2019 XR, SA 2019",2019,,,,9.78145E+12,10.1145/3355355.3361878,,"SIGGRAPH Asia 2019 XR - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"In AR/MR remote collaboration, a remote expert often has to demonstrate tasks for a local worker. To make this easier, we have developed a new adaptive MR remote collaborative architecture which enables a remote expert to guide a local user in physical assembly and training tasks. A remote user can activate the instructions through simple and intuitive interaction, then clear instructions are shown in both AR (local) and VR (remote) views, enabling a local worker to operate the tool following the instructions. In the demonstration we use a hammer operation as a physical task, showing the benefits of the adaptive MR remote collaborative platform. © 2019 Association for Computing Machinery.",3,Mixed reality,Augmented reality - Interactive computer graphics,Collaborative architectures - Collaborative platform - Intuitive interaction - Physical assemblies - Physical task - Remote collaboration - Remote experts - Remote users,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Virtual reality driving simulator for user studies on automated driving,"Riegler, Andreas (1); Riener, Andreas (2); Holzmann, Clemens (3) ","(1) University of Applied Sciences Upper Austria, Johannes Kepler University, Linz, Austria (2) Technische Hochschule Ingolstadt, Ingolstadt, Germany (3) University of Applied Sciences Upper Austria, Hagenberg, Austria ","Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",cerence; Helmholtz Instituut; here; Rijkswaterstaat - Ministry of Infrastructure and Water Management; Uber ARG; Utrecht University - Faculty of Social and Behavioral Sciences,"Association for Computing Machinery, Inc",,p 502-507,21-Sep-19,"Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",2019,,,,9.78145E+12,10.1145/3349263.3349595,,"11th ACM International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019","September 21, 2019 - September 25, 2019",,"Nowadays, HCI Research on automated driving is commonly carried out using either low-quality setups with 2D monitors or expensive driving simulators with motion platforms. Furthermore, software for user studies on automated driving is often expensive and hard to modify for different scenarios. We plan to fill this gap by proposing a low-cost, high-fidelity immersive prototyping simulator by making use of virtual reality (VR) technology: AutoWSD - Automated driving simulator for research on windshield displays. We showcase a hybrid software and hardware solution as well as demonstrate how to design and implement scenarios for user studies, and thereby encourage discussion about potential improvements and extensions for AutoWSD, as well as the topic of trust, acceptance, user experience and simulator sickness in automation. © 2019 Copyright is held by the owner/author(s).",17,User interfaces,Augmented reality - Automation - Automobile simulators - Computer software - Head-up displays - Software prototyping - Virtual reality - Windshields,Automated driving - Design and implements - Driving simulator - Motion platforms - Simulator sickness - Software and hardwares - User study - Windshield displays,"662 Automobiles and Smaller Vehicles - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.1 Computer Programming - 731 Automatic Control Principles and Applications",,,,This work is supported by the University of Applied Sciences PhD program of the government of Upper Austria.,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Egocentric Visitors Localization in Cultural Sites [arXiv],"Ragusa, F. (1); Furnari, A. (1); Battiato, S. (1); Signorello, G. (2); Farinella, G.M. (1) ","(1) IPLab, Univ. degli Studi di Catania, Catania, Italy (2) CUTGANA, Univ. degli Studi di Catania, Catania, Italy ",arXiv,,"arXiv, USA",,20 pp.,10-Apr-19,,,,,,,,,,,,"We consider the problem of localizing visitors in a cultural site from egocentric (first person) images. Localization information can be useful both to assist the user during his visit (e.g., by suggesting where to go and what to see next) and to provide behavioral information to the manager of the cultural site (e.g., how much time has been spent by visitors at a given location? What has been liked most?). To tackle the problem, we collected a large dataset of egocentric videos using two cameras: a head-mounted HoloLens device and a chest-mounted GoPro. Each frame has been labeled according to the location of the visitor and to what he was looking at. The dataset is freely available in order to encourage research in this domain. The dataset is complemented with baseline experiments performed considering a state-of-the-art method for location-based temporal segmentation of egocentric videos. Experiments show that compelling results can be achieved to extract useful information for both the visitor and the site-manager. [ACM Journal on Computing and Cultural Heritage (JOCCH), 2019].",30,,augmented reality - cameras - feature extraction - history - image segmentation - information retrieval - video signal processing,egocentric videos - visitor - site-manager - Cultural Heritage - egocentric visitors localization - Cultural sites - cultural site - egocentric images - localization information - behavioral information - how much time has been spent by visitors at a given location - head-mounted HoloLens device - chest-mounted GoPro - location-based temporal segmentation,"B6135 Optical, image and video signal processing - C5260B Computer vision and image processing techniques - C5260D Video signal processing - C6130V Virtual reality - C7250R Information retrieval techniques",G03B17/00 - G03B19/00 - G06F17/30 - G06T,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Bibliometric analysis of wearable devices and their applications to english education,"Cheng, Li (1); Yao, Jigang (2) ","(1) Beijing University of Posts and Telecommunications, 10-Xitucheng Rd. Haidian District, Beijing, China (2) Beijing University of Technology, 100-Pingleyuan Chaoyang District, China ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 35-38,28-Jun-19,"Proceedings of the 2019 International Conference on Modern Educational Technology, ICMET 2019",2019,,,,9.78145E+12,10.1145/3341042.3341059,,"2019 International Conference on Modern Educational Technology, ICMET 2019","June 28, 2019 - June 30, 2019",,"This paper reports a study investigating the trends and hotspots of wearable devices (WDs) research using the software of Citespace. A total of 4,423 research articles (2008-2017) were retrieved from Web of Science, an online service offering access to multiple databases of academic and scientific disciplines. The study employed CiteSpace III to do a systematic and quantitative analysis of the WDs literature. Results show that wearable devices have been used in many fields including materials, biomedicine and telecommunications. In the field of English education, two trends have been identified. One is the use of WDs in a virtual reality/augmented reality classroom or language lab and the other the integration of smart devices with mobile learning. With the innovations in materials, information technology and social networks, WDs will provide more effective services to learners of English. It is anticipated that due to their increased Internet connectivity, improved usability, reduced costs and increased reliability, there will be more and more pedagogical opportunities for educators to use wearable devices in English education in the future. © 2019 ACM.",15,Wearable technology,Educational technology - Electronic document exchange - Social sciences computing - Virtual reality,Bibliometric analysis - Citespace - English educations - Internet connectivity - Mobile Learning - Scientific discipline - Visual analysis - Wearable devices,"723 Computer Software, Data Handling and Applications - 901.2 Education",,,,The research work was supported by the Research Fund of Beijing University of Posts and Telecommunications (No. 500518526) and Beijing Research Fund (BJSZ2019ZC12).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Smartglasses in the sterile supply process,"Krauß, Veronika (1); Uzun, Yücel (1); Oppermann, Leif (1); Reiners, René (1) ","(1) Fraunhofer Institute for Applied Information Technology, FIT Sankt, Augustin, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 859-861,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,10.1145/3340764.3345367,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"The presented demonstrator showcases the potential of augmented reality (AR) smartglasses in the sterile supply chain. In the scope of the project 'Smartglasses in der Sterilgutver-sorgung', we developed an application to investigate on the applicability of smartglasses and AR paradigms during the reprocessing of used operational tools. The demonstrator was developed and tested using the iterative user-centered approach involving actual employees of the respective domain. © 2019 Association for Computing Machinery.",7,Augmented reality,Supply chains,Operational tools - Supply process - User-centered approach,"723 Computer Software, Data Handling and Applications - 912 Industrial Engineering and Management - 913 Production Planning and Control; Manufacturing",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",,,ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,,,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"The proceedings contain 58 papers. The topics discussed include: locating nearby physical objects in augmented reality; is it real? understanding interaction mechanics within the reality-virtuality continuum; DroneSAR: extending physical spaces in spatial augmented reality using projection on a drone; evaluation of attention inducing effects using ubiquitous humanlike face robots; and variable and situated user interfaces: assumptions, potentials and design issues.",,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented primitive data types of programming language,"Tariqur Rahman, Md. (1); Ashfiqur Rahman, A.T.M. (1); Chowdhury, Jahiruddin (1); Zaman, A.G.M. (1) ","(1) Computer Science Department, American International University-Bangladesh, Dhaka, Bangladesh ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,10-Jan-20,"International Conference on Computing Advancements: 'Age of Computing and Augmented Life', ICCA 2020",2020,,,,9.78145E+12,10.1145/3377049.3377127,,"2020 International Conference on Computing Advancements: Age of Computing and Augmented Life, ICCA 2020","January 10, 2020 - January 12, 2020",,"Learning programming language for the first timer is critical and difficult to understand the concepts properly. It will be easier for the beginners to understand the concepts if any visual representation is available for them. Augmented reality (AR) is one of the most preferable medium in recent time. In this paper, we present an experiment of augmentation of primitive data types of C programming language in mobile platform. The visualization includes the structure of the data types, such as, memory block and space taken by each data type. © 2020 Association for Computing Machinery.",12,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, MobiQuitous 2019",,,ACM International Conference Proceeding Series,,Association for Computing Machinery,,,12-Nov-19,"Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, MobiQuitous 2019",2019,,,,9.78145E+12,,,"16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, MobiQuitous 2019","November 12, 2019 - November 14, 2019",,The proceedings contain 58 papers. The topics discussed include: regression-based network monitoring in swarm robotic systems; efficient content caching for named data network nodes; a deep spatio-temporal attention-based neural network for passenger flow prediction; mobile augmented reality techniques for emergency response; the impression of virtual experience: mobile augmented reality cloud solution; jive: spatially-constrained encryption key sharing using visible light communication; an intelligent optimization method for information recommendation; and a scheme for anomalous RFID trajectory detection based on improved clustering algorithm under digital-twin-driven.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Proceedings of the 2019 International Conference on Mathematics, Science and Technology Teaching and Learning, ICMSTTL 2019",,,ACM International Conference Proceeding Series,Tsinghua University in Taiwan; University of Central Queensland; University of New South Wales; University of Technology Sydney,Association for Computing Machinery,,,28-Jun-19,"Proceedings of the 2019 International Conference on Mathematics, Science and Technology Teaching and Learning, ICMSTTL 2019",2019,,,,9.78145E+12,,,"2019 International Conference on Mathematics, Science and Technology Teaching and Learning, ICMSTTL 2019","June 28, 2019 - June 30, 2019",,"The proceedings contain 14 papers. The topics discussed include: improving learning through cloud-based mobile technologies and virtual and augmented reality for Australian higher education; the impact of educational microcontent on the student learning experience; exploring the relationship between OJT course performance and academic performance of computer science students in selected programming courses; the impact of big data on health care services in Australia: using big data analytics to categorize and deal with patients; comparison of effectiveness between contextual teaching and learning (CTL) and problem-based learning (PBL) approach on the interest of junior high school students; self-directed learning of student in mathematics education: is there any problem?; blended learning for bilingual math, science and technology teachers’ professional development in China; and periodic solutions of branched space from closed orbits under mixed perturbations.",,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Extending conavigator into a collaborative digital space,"Murnane, Mark (1); Engel, Don (1); Freeland, Stephen (1); Boot, Lee (1); Jarzynski, Mark (1); Lindvig, Katrine (2); Hillersdal, Line (2); Earle, David (2) ","(1) University of Maryland, Baltimore County, Baltimore; MD, United States (2) University of Copenhagen, Copenhagen, Denmark ",Proceedings of the International ACM SIGGROUP Conference on Supporting Group Work,ACM SIGCHI,Association for Computing Machinery,,p 127-130,6-Jan-20,GROUP 2020 - Companion of the 2020 ACM International Conference on Supporting Group Work,2020,,,,9.78145E+12,10.1145/3323994.3369890,,"21st ACM International Conference on Supporting Group Work, GROUP 2020","January 6, 2020 - January 8, 2020",,"We present an extension of the existing CoNavigator collaboration system that allows for persistence of in-person collaboration sessions through a digitally projected overlay and camera system. This augmented reality environment allows participants in a CoNavigator session to resume from a previous session without having to laboriously recreate the physical state of the space manually. By combining features traditionally found only in digital space with the intuitive nature of a tactile physical environment, we hope to produce a tool that builds on the work of CoNavigator and lowers barriers to adoption while increasing efficacy. © held by the owner/author(s).",6,Augmented reality,Computer vision,Barriers to adoption - Camera systems - Collaboration - Collaboration systems - Digital space - Group work - Physical environments - Physical state,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Proceedings of the 2019 International Conference on Modern Educational Technology, ICMET 2019",,,ACM International Conference Proceeding Series,,Association for Computing Machinery,,,28-Jun-19,"Proceedings of the 2019 International Conference on Modern Educational Technology, ICMET 2019",2019,,,,9.78145E+12,,,"2019 International Conference on Modern Educational Technology, ICMET 2019","June 28, 2019 - June 30, 2019",,The proceedings contain 24 papers. The topics discussed include: research on the teaching mode of information technology course based on education cloud platform; research on teaching reform of compiling principle based on the cultivation of practical ability; the conceptual framework for applying digital community marketing and marketing practices into educational relationship marketing model of private technical high school in Taiwan; analysis of the influence of education app on the independent learning mode of junior middle school students; an empirical study of incorporation of augmented reality into civic education; and novel education technology may derive from personal genome data: a language gene polymorphism site potentially associated with translation-writing errors in a bilingual classroom of Chinese students.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Research of virtual simulation experiment platform for marine auxiliary machinery based on VR/AR,"Shang, Lei (1); Chen, Jinsong (2); Gao, Qian (1); Hu, Fucai (2) ","(1) National Engineering Research Center for Water Transport Safety, Wuhan University of Technology, Wuhan, China (2) School of Energy and Power Engineering, Wuhan University of Technology, Wuhan, China ",ACM International Conference Proceeding Series,Association for Science and Engineering (ASciE),Association for Computing Machinery,,,22-Oct-19,"Proceedings of the 3rd International Conference on Computer Science and Application Engineering, CSAE 2019",2019,,,,9.78145E+12,10.1145/3331453.3361643,a64,"3rd International Conference on Computer Science and Application Engineering, CSAE 2019","October 22, 2019 - October 24, 2019",,"Aiming at the shortcomings in the current experimental teaching of marine auxiliary machinery, this paper develops a virtual experimental teaching platform for marine auxiliary machinery based on the emerging VR/AR (Virtual Reality/Augmented Reality) technology. This platform adopts 3Ds Max as modeling tool and Unity3D as virtual reality engine, realizes the multi-experiment-mode of marine auxiliary machinery cognition and virtual disassembly experiment under cloud service framework, this platform can be used on terminals such as PC, intelligent mobile devices, HTC Vive and HoloLens. In this paper, the framework design, developing environment construction, key technologies and operational effects of the marine auxiliary machinery virtual simulation experiment platform are elaborated. The practical application results show that the platform is easy to use and can motivate the enthusiasm of students. To a certain extent, it improves the effect of the marine auxiliary machinery experimental teaching and enhances practical ability of students. © 2019 Association for Computing Machinery.",12,Virtual reality,3D modeling - Human computer interaction - Machine design - Machinery - Marine engineering - Simulation platform,Environment constructions - Experiment platforms - Experimental teachings - Framework designs - Key technologies - Operational effects - Virtual reality engine - Virtual simulations,"601 Mechanical Design - 675 Marine Engineering - 723 Computer Software, Data Handling and Applications",,,"Number: 201809, Acronym: -, Sponsor: -; ",This research was supported by the National Water Transport Safety Engineering Technology Research Center Open Fund (201809),,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An adaptive rendering framework for efficient synthetic light field generation,Liang-Chi Tseng (1); Wei-Chung Hsu (1) ,"(1) Nat. Taiwan Univ., Taipei, Taiwan ",ACM SIGAPP Applied Computing Review,,"ACM, USA","v 19, n 3",33-44,Sept. 2019,,,,1559-6915,,,10.1145/3372001.3372004,,,,,"Real-time global illumination rendering is very desirable for emerging applications such as Virtual Reality (VR) and Augmented Reality (AR). However, client devices have difficulties to support photo-realistic rendering, such as Ray-Tracing, due to insufficient computing resources. Many modern frameworks adopted Light Field rendering to support device displaying. A Light Field can be precomputed and stored in the Cloud. During runtime, the display extracts the color information from the Light Field to generate arbitrary real time viewpoints or re-focusing within a predefined area. To efficiently compute the Light Field, We have combined DIBR (Depth-Image-Based-Rendering) and traditional ray-tracing in an adaptive fashion to synthesize images. By measuring the color errors during runtime, we adaptively determine a good balance between DIBR and Ray Tracing. We study several cases and problems when combing these algorithms and came up with a more efficient solution. To further optimize the computation efficiency, we also added a multi-level design to exploit the degree of shareable pixels among images to control the computation for error reduction. In addition, we also design an efficient multi-threading scheduling to show the scalability of our approach. Experiments show that when the number of threads is small, we achieved up to 3.24X speedup in Light Field generation for relative simple scenes like Cornell Box, and about 2X speed up for complex scenes like Conference Room or Sponza. In the multi-thread environment, our proposed scheme can offer about 2X speedup over traditional ones.",,,multi-threading - ray tracing - rendering (computer graphics),adaptive rendering framework - real-time global illumination rendering - photo-realistic rendering - ray-tracing - light field rendering - depth-image-based-rendering - synthetic light field generation - multithreading scheduling,C6190P Parallel software - C6130B Graphics techniques,G06F9/46,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
VR Tsunami!,"Jacoby, Derek (1); Coady, Yvonne (2); Dahl, Eric (2); Wynden, Andy (2); Richardson, Matt (2) ","(1) QVirt Labs, Inc, Victoria; BC, Canada (2) University of Victoria, Victoria; BC, Canada ","ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3305365.3329728,a9,"ACM SIGGRAPH 2019 Appy Hour - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"The Mod Squad lab at the University of Victoria is focused on research that combines geospatial analytics, cloud computing, and Virtual/Augmented Reality. VR Tsunami is an example of Serious Gaming that uses the interaction styles of video games to engage students in learning outcomes. The experience is based on real data from a real tsunami event, provided on a range of devices to teach middle school students about emergency preparedness in areas of British Columbia that are prone to tsunami activity. © 2019 Copyright Held by the Owner/Author(s).",3,Serious games,Interactive computer graphics - Tsunamis,British Columbia - Emergency preparedness - Geo-spatial - Geospatial applications - Interaction styles - Learning outcome - Middle school students - Serious gaming,"471.4 Seawater, Tides and Waves - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
IoT-Fog 2019 - Proceedings of the 2019 Workshop on Fog Computing and the IoT,,,IoT-Fog 2019 - Proceedings of the 2019 Workshop on Fog Computing and the IoT,ACM,"Association for Computing Machinery, Inc",,,15-Apr-19,IoT-Fog 2019 - Proceedings of the 2019 Workshop on Fog Computing and the IoT,2019,,,,9.78145E+12,,,"2019 Workshop on Fog Computing and the IoT, IoT-Fog 2019",15-Apr-19,,The proceedings contain 12 papers. The topics discussed include: towards quality-of-control-aware scheduling of industrial applications on fog computing platforms; safety of fog-based industrial automation systems; personalized augmented reality via fog-based imitation learning; adaptive industrial IOT/CPS messaging strategies for improved edge compute utility; implementing time-triggered communication over a standard Ethernet switch; the need for deterministic virtualization in fog computing and the industrial Internet of things; and multilayer distributed control over 5G networks: challenges and security threats.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
NHT 2019 - Proceedings of the 8th International Workshop on Narrative and Hypertext,,,NHT 2019 - Proceedings of the 8th International Workshop on Narrative and Hypertext,ACM SIGWEB,"Association for Computing Machinery, Inc",,,12-Sep-19,NHT 2019 - Proceedings of the 8th International Workshop on Narrative and Hypertext,2019,,,,9.78145E+12,,,"8th International Workshop on Narrative and Hypertext, NHT 2019, in conjunction with 30th ACM Conference on Hypertext and Social Media, HT 2019",17-Sep-19,,The proceedings contain 3 papers. The topics discussed include: no story without a backstory: the role and importance of the backstory in an augmented reality application for cultural heritage; the unethical future of mixed reality storytelling; and M22 - a modern visual novel framework.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",,,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,The proceedings contain 133 papers. The topics discussed include: Capttery: scalable battery-like room-level wireless power; BreathListener: fine-grained breathing monitoring in driving environments utilizing acoustic signals; FlyZone: a testbed for experimenting with aerial drone applications; InternalBlue - Bluetooth binary patching and experimentation framework; animal-borne anti-poaching system; SpecEye: towards pervasive and privacy-preserving screen exposure detection in daily life; when IoT met augmented reality: visualizing the source of the wireless signal in AR view; freedom: fast recovery enhanced VR delivery over mobile networks; GLEAM: an illumination estimation framework for real-time photorealistic augmented reality on mobile devices; and understanding and detecting overlay-based android malware at market scales.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Clench interaction: Novel biting input techniques,"Xu, Xuhai (1, 2); Yu, Chun (2); Dey, Anind K. (1); Mankoff, Jennifer (3) ","(1) Information School, University of Washington, Seattle, United States (2) Department of Computer Science and Technology, Tsinghua University, Beijing, China (3) Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300505,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"People eat every day and biting is one of the most fundamental and natural actions that they perform on a daily basis. Existing work has explored tooth click location and jaw movement as input techniques, however clenching has the potential to add control to this input channel. We propose clench interaction that leverages clenching as an actively controlled physiological signal that can facilitate interactions. We conducted a user study to investigate users’ ability to control their clench force. We found that users can easily discriminate three force levels, and that they can quickly confirm actions by unclenching (quick release). We developed a design space for clench interaction based on the results and investigated the usability of the clench interface. Participants preferred the clench over baselines and indicated a willingness to use clench-based interactions. This novel technique can provide an additional input method in cases where users’ eyes or hands are busy, augment immersive experiences such as virtual/augmented reality, and assist individuals with disabilities. © 2019 Association for Computing Machinery.",40,Human engineering,Software engineering,Design spaces - Force level - Input channels - Input methods - Input techniques - Jaw movements - Novel techniques - Physiological signals,461.4 Ergonomics and Human Factors Engineering - 723.1 Computer Programming,,,"Number: 20151080408, Acronym: THU, Sponsor: Tsinghua University; Number: 61572276, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; Number: 61672314, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; ","This work is supported by the National Key Research and Development Plan under Grant No. 2016YFB1001402, the Natural Science Foundation of China under Grant No. 61672314 and No. 61572276, Tsinghua University Research Funding No. 20151080408, and also by Beijing Key Lab of Networked Multimedia.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Analysis and Design of a Latency Control Protocol for Multi-Path Data Delivery With Pre-Defined QoS Guarantees,"Chiariotti, F. (1); Kucera, S. (2); Zanella, A. (1); Claussen, H. (2) ","(1) Dept. of Inf. Eng., Univ. of Padua, Padua, Italy (2) Nokia Bell Labs., Dublin, Ireland ",IEEE/ACM Transactions on Networking,,"IEEE, USA","v 27, n 3",1165-78,Jun-19,,,,1063-6692,,,10.1109/TNET.2019.2911122,,,,,"As the capacity and reliability of mobile networks increases, so does the demand for more responsive end-to-end services: applications such as augmented reality, live video conferencing, and smart or autonomous vehicles require reliable, throughput-intensive end-to-end communications with strict delay constraints. Only consistently reliable delivery of data flows well within human interactivity deadlines will enable a truly immersive user experience. To enable data delivery within pre-defined deadlines, controlled on demand by an application or its user, we propose and demonstrate a novel transport-layer protocol for explicit latency control called latency-controlled end-to-end aggregation protocol (LEAP). The LEAP splits a data flow with quality of service (QoS) constraints into multiple subflows that are delivered over multiple parallel links (e.g., Wi-Fi and LTE in a standard smartphone, WiGig, and 5G in the near future). The subflow data rates are set based on a novel proactive forecasting of the achievable channel capacity, subject to application-specific QoS constraints. Cross-path encoding and redundancy adaptation are then used to deliberately balance the trade-off between maximum throughput, required delay, and minimum reliability as function of application/user-specific input parameters. When compared to leading state-of-the-art transport protocols in live network experiments, LEAP exhibits a superior capacity to reliably provide a high and stable throughput with bounded latency, both in wired and wireless scenarios. The LEAP is also the first protocol to allow applications to explicitly set their priorities, giving them the freedom to set the operating point in the trade-off between throughput, latency, and reliability.",54,,channel capacity - channel coding - mobile radio - quality of service - telecommunication network reliability - transport protocols - wireless LAN,novel transport-layer protocol - explicit latency control - LEAP - data flow - service constraints - multiple subflows - multiple parallel links - subflow data rates - application-specific QoS constraints - redundancy adaptation - minimum reliability - live network experiments - multipath data delivery - pre-defined QoS guarantees - responsive end-to-end services - throughput-intensive end-to-end communications - strict delay constraints - channel capacity - application-user-specific input parameters - immersive user experience - latency-controlled end-to-end aggregation protocol - quality of service - proactive forecasting - cross-path encoding - bounded latency - mobile network reliability,B6250F Mobile radio systems - B0170N Reliability - B6120B Codes - B6150M Protocols,H03M - H04B1/74 - H04B7/00 - H04B7/26 - H04L1/00 - H04L29/06 - H04W - H04W84/12,Practical (PRA); Theoretical or Mathematical (THR),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Design of an autonomous sentry gun system for the detection of people in restricted zones,"Figueroa, Yordi (1); Arias, Luis (1); Sánchez, Ricardo (1); Hallo, Vicente (1); Velasco, Nancy (1); Mendoza, Darío (1) ","(1) Universidad de las Fuerzas Armadas ESPE, Sangolquí, Ecuador ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 57-61,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332320,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"A proper mechatronic system capable of neutralizing people entering unauthorized areas by firing a paintball gun is presented in this paper. The effective shooting range is 25 meters limited only by the range of the marker. The implemented system is capable of operating in different light conditions, at a maximum distance of 70 meters. The mechanism, together with the artificial vision system move in two axes: turn and tilt. Processing includes identification, tracking, pan tilt zooms control and automatic trigger activation control. The tests were carried out in a stadium, following the safety rules to avoid damage to the test subject. The shooting effectiveness is 80%. © 2019 Association for Computing Machinery.",10,Augmented reality,Computer applications - Computer programming - Error detection,Activation control - Artificial vision system - Automatic trigger - Different lights - Maximum distance - Mechatronic systems - Restricted zone - Shooting ranges,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A comparative evaluation of techniques for sharing AR experiences in museums,"Franz, Juliano (1); Alnusayri, Mohammed (1); Malloch, Joseph (1); Reilly, Derek (1) ","(1) Faculty of Computer Science, Dalhousie University, 6050 University Avenue, Halifax; NS; B3H 4R2, Canada ",Proceedings of the ACM on Human-Computer Interaction,,Association for Computing Machinery,"v 3, n CSCW",,Nov-19,,2019,,,25730142,,10.1145/3359226,124,,,,"Museums are constantly searching for new ways to increase engagement with their exhibits, from electronic guides to modern digital technologies such as special-purpose tablets, smartphones, and virtual and augmented reality (AR). For AR exhibits in particular, promoting shared experience and group cohesion is not straightforward. In this work, we investigate scenarios in which not everyone is using a head-worn display (HWD), either because there aren’t enough available or simply because someone might feel uncomfortable using it. We propose two sharing techniques for AR experiences and evaluate them in a long term in-the-wild study: Over-the-Shoulder AR, which renders a real-time virtual representation of the augmented reality content on a large secondary display; Semantic Linking, which displays contextual information about the virtual content on the same large display. We also introduce a complementary technique: Indicator Rings, which display the locations of the HWD user’s objects-of-focus. We observed that participants in the Over-the-Shoulder AR and Semantic Linking conditions stayed together and exhibited more verbal exchanges than participants in a Baseline condition, which could indicate that they were more engaged. Self-reported measures indicated an increase in pair communication and increased comprehension of the virtual content for participants without the HWD. Participants without the HWD also displayed a greater understanding of the location of virtual elements with support from the Indicator Rings, and used them as a tool to guide the HWD user through the virtual content. We discuss design implications for interactive augmented reality exhibits and possible applications outside the cultural heritage scenario. © 2019 Association for Computing Machinery.",46,,,,,,,"Number: DNDPJ 4907483-2015, Acronym: NSERC, Sponsor: Natural Sciences and Engineering Research Council of Canada; Number: -, Acronym: -, Sponsor: Lockheed Martin; ",This research was funded by by the Natural Sciences and Engineering Research Council of Canada (NSERC) (grant number DNDPJ 4907483-2015) in partnership with Defense Research and Development Canada (DRDC) and Lockheed Martin.,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Exploring tangible interaction and diegetic feedback in an ar math game for children,"Li, Jingya (1); Van Der Spek, Erik (1); Hu, Jun (1); Feijs, Loe (1) ","(1) Department of Industrial Design, Eindhoven University of Technology, Den Dolech 2, Eindhoven; 5612 AZ, Netherlands ","Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",Boise Osmo; Boise State University; et al.; Langan Barber Foundation; St. Luke's; STEM Action Center,"Association for Computing Machinery, Inc",,p 580-585,12-Jun-19,"Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",2019,,,,9.78145E+12,10.1145/3311927.3325333,,"18th ACM International Conference on Interaction Design and Children, IDC 2019","June 12, 2019 - June 15, 2019",,"Augmented Reality is becoming an emerging trend in the development of play-and-learn experience and increasingly accessible to children. However, there is a lack of understanding of how to design AR games to effectively improve children's learning experience, especially with respect to the novel interaction and representation paradigms that AR affords. In this study, we describe the design and the implementation process of different interaction types (screen-touch and tangible interaction) and feedback mechanisms (non-diegetic feedback and diegetic feedback) in an AR math game for children aged 7 and 8. We report the insights based on our current prototypes and discuss the design implications for our future work. © 2019 Association for Computing Machinery.",15,Augmented reality,Motivation - Touch screens,Children - Design implications - Feedback mechanisms - Game design - Implementation process - Interaction techniques - Learning experiences - Tangible interaction,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 912.4 Personnel",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Preface,"Kavakli-Thorne, Manolya (1) ","(1) Macquarie University, Faculty of Science, Department of Computing, Australia ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p V,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019",,,"Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019",ACM Special Interest Group on Computer-Human Interaction (SIGCHI),"Association for Computing Machinery, Inc",,,1-Oct-19,"Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019",2019,,,,9.78145E+12,,,"21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019","October 1, 2019 - October 4, 2019",,"The proceedings contain 80 papers. The topics discussed include: SeeingHaptics: visualizations for communicating haptic designs; investigating smartphone-based pan and zoom in 3D data spaces in augmented reality; understanding emoji interpretation through user personality and message context; the influence of hand size on touch accuracy; mapping perceptions of humanness in intelligent personal assistant interaction; tiger: wearable glasses for the 20-20-20 rule to alleviate computer vision syndrome; from design to development to evaluation of a pregnancy app for low-income women in a community-based setting; exploring cross-modal training via touch to learn a mid-air marking menu gesture set; and how do people type on mobile devices? observations from a study with 37,000 volunteers.",,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"SIGGRAPH Asia 2019 XR, SA 2019",,,"SIGGRAPH Asia 2019 XR, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,17-Nov-19,"SIGGRAPH Asia 2019 XR, SA 2019",2019,,,,9.78145E+12,,,"SIGGRAPH Asia 2019 XR - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,The proceedings contain 22 papers. The topics discussed include: encounters: a multiparticipant audiovisual art experience with XR; FaceDrive: facial expression driven operation to control virtual supernumerary robotic arms; FreeMo: extending hand tracking experiences through capture volume and user freedom; head gaze target selection for redirected interaction; HyperDrum: interactive synchronous drumming in virtual reality using everyday objects; JumpinVR: enhancing jump experience in a limited physical space; light me up: an augmented-reality projection system; PhantomTouch: creating an extended reality by the illusion of touch using a shape-memory alloy matrix; and pumping life: embodied virtual companion for enhancing immersive experience with multisensory feedback.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Building outline extraction from aerial images using convolutional neural networks,"Alidoost, F. (1); Arefi, H. (1); Tombari, F. (2) ","(1) School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Iran (2) Department of Computer Aided Medical Procedures and Augmented Reality, Faculty of Informatics, Technical University of Munich, Germany ","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,International Society for Photogrammetry and Remote Sensing,"v 42, n 4/W18",p 57-61,18-Oct-19,,2019,,16821750,,,10.5194/isprs-archives-XLII-4-W18-57-2019,,"ISPRS International GeoSpatial Conference 2019, Joint Conferences of 5th Sensors and Models in Photogrammetry and Remote Sensing, SMPR 2019 and 3rd Geospatial Information Research, GI Research 2019","October 12, 2019 - October 14, 2019",,"Automatic detection and extraction of buildings from aerial images are considerable challenges in many applications, including disaster management, navigation, urbanization monitoring, emergency responses, 3D city mapping and reconstruction. However, the most important problem is to precisely localize buildings from single aerial images where there is no additional information such as LiDAR point cloud data or high resolution Digital Surface Models (DSMs). In this paper, a Deep Learning (DL)-based approach is proposed to localize buildings, estimate the relative height information, and extract the buildings' boundaries using a single aerial image. In order to detect buildings and extract the bounding boxes, a Fully Connected Convolutional Neural Network (FC-CNN) is trained to classify building and non-building objects. We also introduced a novel Multi-Scale Convolutional-Deconvolutional Network (MS-CDN) including skip connection layers to predict normalized DSMs (nDSMs) from a single image. The extracted bounding boxes as well as predicted nDSMs are then employed by an Active Contour Model (ACM) to provide precise boundaries of buildings. The experiments show that, even having noises in the predicted nDSMs, the proposed method performs well on single aerial images with different building shapes. The quality rate for building detection is about 86% and the RMSE for nDSM prediction is about 4 m. Also, the accuracy of boundary extraction is about 68%. Since the proposed framework is based on a single image, it could be employed for real time applications. © 2019 F. Alidoost et al.",25,Image processing,Air navigation - Antennas - Buildings - Convolution - Convolutional neural networks - Deep learning - Disaster prevention - Disasters - Extraction - Maintenance - Remote sensing,Active contour model - Automatic Detection - Boundary extraction - Digital surface models - Disaster management - Lidar point cloud datum - Outline extractions - Real-time application,402 Buildings and Towers - 431.5 Air Navigation and Traffic Control - 716.1 Information Theory and Signal Processing - 802.3 Chemical Operations - 913.5 Maintenance - 914.1 Accidents and Accident Prevention,,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Depth from motion for smartphone Ar,"Valentin, Julien (1); Kowdle, Adarsh (1); Barron, Jonathan T. (1); Wadhwa, Neal (1); Dzitsiuk, Max (1); Schoenberg, Michael (1); Verma, Vivek (1); Csaszar, Ambrus (1); Turner, Eric (1); Dryanovski, Ivan (1); Afonso, Joao (1); Pascoal, Jose (1); Tsotsos, Konstantine (1); Leung, Mira (1); Schmidt, Mirko (1); Guleryuz, Onur (1); Khamis, Sameh (1); Tankovitch, Vladimir (1); Fanello, Sean (1); Izadi, Shahram (1); Rhemann, Christoph (1) ","(1) Google Inc, United States ","SIGGRAPH Asia 2018 Technical Papers, SIGGRAPH Asia 2018",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,4-Dec-18,"SIGGRAPH Asia 2018 Technical Papers, SIGGRAPH Asia 2018",2019,,,,9.78145E+12,10.1145/3272127.3275041,193,"SIGGRAPH Asia 2018 Technical Papers - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH Asia 2018","December 4, 2018 - December 7, 2018",,"Augmented reality (AR) for smartphones has matured from a technology for earlier adopters, available only on select high-end phones, to one that is truly available to the general public. One of the key breakthroughs has been in low-compute methods for six degree of freedom (6DoF) tracking on phones using only the existing hardware (camera and inertial sensors). 6DoF tracking is the cornerstone of smartphone AR allowing virtual content to be precisely locked on top of the real world. However, to really give users the impression of believable AR, one requires mobile depth. Without depth, even simple effects such as a virtual object being correctly occluded by the real-world is impossible. However, requiring a mobile depth sensor would severely restrict the access to such features. In this article, we provide a novel pipeline for mobile depth that supports a wide array of mobile phones, and uses only the existing monocular color sensor. Through several technical contributions, we provide the ability to compute low latency dense depth maps using only a single CPU core of a wide range of (medium-high) mobile phones. We demonstrate the capabilities of our approach on high-level AR applications including real-time navigation and shopping. © 2018 Copyright held by the owner/author(s).",75,Interactive computer graphics,Air navigation - Augmented reality - Degrees of freedom (mechanics) - Smartphones,Dense depth map - Depth from motion - Inertial sensor - Motion stereo - Real-time navigation - Six degree-of-freedom - Structure from motion - Technical contribution,"431.5 Air Navigation and Traffic Control - 718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 931.1 Mechanics",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Three-dimensional Interaction technique using an acoustically manipulated balloon,"Furumoto, Takuro (1); Kamigaki, Takaaki (1); Ito, Mitsuru (1); Fujiwara, Masahiro (1); Makino, Yasutoshi (1); Shinoda, Hiroyuki (1) ","(1) University of Tokyo, Kashiwa, Chiba, Japan ","SIGGRAPH Asia 2019 Emerging Technologies, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 51-52,17-Nov-19,"SIGGRAPH Asia 2019 Emerging Technologies, SA 2019",2019,,,,9.78145E+12,10.1145/3355049.3360536,,"SIGGRAPH Asia 2019 Emerging Technologies - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"We propose a system that uses an acoustically manipulated balloon as a visual and tangible interface for the representation of a mid-air virtual object in a full-body augmented reality environment. In this system, airborne ultrasound phased-array transducers on the ceiling actuate a spherical balloon inflated with a mixture of helium and air. This configuration permits (1) a full-body workspace with lateral scalability, (2) a long flight time, (3) good visibility, and (4) easily tangible access to the balloon. A projector-camera system projects a 2D or 3D perspective-correct image onto the balloon. The user can manipulate the corresponding virtual object by physically manipulating the balloon. © 2019 Copyright held by the owner/author(s).",10,Balloons,Augmented reality - Interactive computer graphics - Stereo image processing - Ultrasonic applications,3D interactions - Airborne ultrasound - Flight time - Projector-camera system - Stereoscopic image - Tangible interfaces - Three-dimensional interaction - Virtual objects,"652.5 Balloons and Gliders - 723 Computer Software, Data Handling and Applications - 753.3 Ultrasonic Applications",,,"Number: JP 18J13314, Acronym: JSPS, Sponsor: Japan Society for the Promotion of Science; ",This work was partly supported by JSPS KAKENHI Grant Numbers JP 18J13314 and JP 16H06303.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
What is mixed reality?,"Speicher, Maximilian (1); Hall, Brian D. (2); Nebeling, Michael (2) ","(1) University of Michigan C and A Europe, United States (2) University of Michigan, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300767,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"What is Mixed Reality (MR)? To revisit this question given the many recent developments, we conducted interviews with ten AR/VR experts from academia and industry, as well as a literature survey of 68 papers. We find that, while there are prominent examples, there is no universally agreed on, one-size-fits-all definition of MR. Rather, we identified six partially competing notions from the literature and experts’ responses. We then started to isolate the different aspects of reality relevant for MR experiences, going beyond the primarily visual notions and extending to audio, motion, haptics, taste, and smell. We distill our findings into a conceptual framework with seven dimensions to characterize MR applications in terms of the number of environments, number of users, level of immersion, level of virtuality, degree of interaction, input, and output. Our goal with this paper is to support classification and discussion of MR applications’ design and provide a better means to researchers to contextualize their work within the increasingly fragmented MR landscape. © 2019 Copyright held by the owner/author(s).",91,Mixed reality,Augmented reality - Human engineering - Tantalum compounds - Taxonomies - Virtual reality,Conceptual frameworks - Contextualize - Degree of interaction - Expert interviews - Haptics - Literature reviews - Literature survey,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 903 Information Science",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Design and evaluation of DIO construction toolkit for co-making shared constructions,"Arora, Jatin (1); Mathur, Kartik (1); Goel, Manvi (1); Kuma, Piyush (1); Mishra, Abhijeet (1); Parnami, Aman (1) ","(1) Weave Lab, IIIT-Delhi, New Delhi; Delhi, India ","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",,Association for Computing Machinery,"v 3, n 4",,Dec-19,,2019,,,24749567,,10.1145/3369833,127,,,,"We present the design and implementation of DIO, a novel digital-physical construction toolkit to enable constructionist learning for children from age group 8-12 years. The toolkit comprises of dome-shaped (D) tangible modules with various attachments that allow suspension on the body of multiple children and/or in the environment to support a variety of sensing/input (I), actuation/output (O) functionalities. The modules are enabled for wireless communication and can be linked together using an Augmented Reality based programming interface running on a smartphone. The smartphone recognizes our hemispherical modules omnidirectionally through novel computer vision based 3D patterns; custom made to provide logical as well as semantic encoding. In this paper, we show how, owing to its unique form-factor, the toolkit enables multi-user constructions for the children and offers a shared learning experience. We further reflect on our learning from a one-year long iterative design process and contribute a social scaffolding based procedure to engage children with such constructionist toolkits effectively. Copyright © 2019 held by the owner/author(s).",62,Design,Augmented reality - Scaffolds - Semantics - Smartphones,Constructionist learning - Design and evaluations - Design and implementations - Iterative design - Learning experiences - Programming interface - Tangible blocks - Wireless communications,"405.1 Construction Equipment - 718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Increasing driver awareness through translucency on windshield displays,"Van Amersfoorth, Emma (1); Schuermans, Laurent (1); Roefs, Lotte (1); Pfleging, Bastian (1); Bonekamp, Quinta (1) ","(1) Eindhoven University of Technology, Eindhoven, Netherlands ","Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",cerence; Helmholtz Instituut; here; Rijkswaterstaat - Ministry of Infrastructure and Water Management; Uber ARG; Utrecht University - Faculty of Social and Behavioral Sciences,"Association for Computing Machinery, Inc",,p 156-160,21-Sep-19,"Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",2019,,,,9.78145E+12,10.1145/3349263.3351911,,"11th ACM International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019","September 21, 2019 - September 25, 2019",,"When driving a car, important objects (e.g., pedestrians) are often hidden by surrounding vehicles which can delay drivers’ reaction times. In this paper we therefore explore how an augmented reality enabled windshield display can improve the drivers’ capabilities. By overlaying of what is behind nearby vehicles onto the own windshield, these vehicles can be rendered translucent. In a simulator experiment we evaluate the influence of three levels of opacity on driver and braking behavior. Results indicate a trend of translucency decreasing the required braking time. © 2019 Copyright held by the owner/author(s).",11,User interfaces,Augmented reality - Windshields,Braking behavior - Driving safety - Important object - Windshield displays,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
LPGL: Low-power graphics library for mobile AR headsets,"Choi, Jaewon (1); Park, HyeonJung (1); Paek, Jeongyeup (2); Balan, Rajesh Krishna (3); Ko, JeongGil (1) ","(1) Ajou Univerisity, Department of Computer Engineering, Korea, Republic of (2) Chung-Ang University, School of Computer Science and Engineering, Korea, Republic of (3) Singapore Management University, School of Information Systems, Singapore, Singapore ","MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 155-167,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,10.1145/3307334.3326097,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,"We present LpGL, an OpenGL API compatible Low-power Graphics Library for energy efficient AR headset applications. We first characterize the power consumption patterns of a state of the art AR headset, Magic Leap One, and empirically show that its internal GPU is the most impactful and controllable energy consumer. Based on the preliminary studies, we design LpGL so that it uses the device’s gaze/head orientation information and geometry data to infer user perception information, intercepts application-level graphics API calls, and employs frame rate control, mesh simplification, and culling techniques to enhance energy efficiency of AR headsets without detriment of user experience. Results from a comprehensive set of controlled in-lab experiments and an IRB-approved user study with 25 participants show that LpGL reduces up to ∼22% of total energy usage while adding only 46µsec of latency per object with close to no loss in subjective user experience. © 2019 Association for Computing Machinery.",50,Energy efficiency,Application programming interfaces (API) - Augmented reality - Image coding,Application level - Consumption patterns - Culling techniques - Mesh simplifications - Mobile Headsets - Orientation information - State of the art - User perceptions,"525.2 Energy Conservation - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Interaction design & prototyping for immersive analytics,"Bach, Benjamin (1); Engelke, Ulrich (2); Serrano, Marcos (3); Cordeil, Maxime (4); Ens, Barrett (4); Willett, Wesley (5) ","(1) University of Edinburgh, Edinburgh, United Kingdom (2) CSIRO Data61, Perth; WA, Australia (3) IRIT, University of Toulouse, Toulouse, France (4) Monash University, Melbourne; VIC, Australia (5) University of Calgary, Calgary; AB, Canada ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3299019,3299019,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Immersive Analytics is concerned with the design and evaluation of interactive next-generation interfaces that support human understanding, data analysis, and decision making. New immersive technologies present many opportunities for enhancing humans' experiences with data interaction, but also present many challenges, a subset of which are specific to the analytics domain. This workshop is centered around a set of group prototyping sessions, aimed at identifying new approaches to existing design challenges. In addition to giving perspective on opportunities and difficulties faced by future designers, these exercises will also explore new prototyping methods and tools for the design of interactive data-centric interfaces. This part-day workshop aims to build new ties between the existing immersive analytics community with researchers across many disciplines of the CHI community. © 2019 Copyright held by the owner/author(s).",13,Data visualization,Augmented reality - Decision making - Human engineering - Virtual reality,Design and evaluations - Design challenges - Human understanding - Immersive - Immersive technologies - Interaction design - Situated Analytics - Spatial interaction,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 912.2 Management",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Geollery: A mixed reality social media platform,"Du, Ruofei (1); Li, David (1); Varshney, Amitabh (1) ","(1) Department of Computer Science, University of Maryland, College Park, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300915,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"We present Geollery, an interactive mixed reality social media platform for creating, sharing, and exploring geotagged information. Geollery introduces a real-time pipeline to progressively render an interactive mirrored world with three-dimensional (3D) buildings, internal user-generated content, and external geotagged social media. This mirrored world allows users to see, chat, and collaborate with remote participants with the same spatial context in an immersive virtual environment. We describe the system architecture of Geollery, its key interactive capabilities, and our design decisions. Finally, we conduct a user study with 20 participants to qualitatively compare Geollery with another social media system, Social Street View. Based on the participants’ responses, we discuss the benefits and drawbacks of each system and derive key insights for designing an interactive mirrored world with geotagged social media. User feedback from our study reveals several use cases for Geollery including travel planning, virtual meetings, and family gathering. © 2019 Copyright held by the owner/author(s).",69,Mixed reality,Augmented reality - Flow visualization - Geographic information systems - Human engineering - Social networking (online) - Three dimensional computer graphics - User interfaces - Virtual reality,3D reconstruction - 3D user interface - Immersive virtual environments - Social media - Social media platforms - Social media systems - Three-dimensional (3D) buildings - User-generated content,"461.4 Ergonomics and Human Factors Engineering - 631.1 Fluid Flow, General - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 903.3 Information Retrieval and Use",,,"Number: 1429404, Acronym: NSF, Sponsor: National Science Foundation; Number: 1564212, Acronym: NSF, Sponsor: National Science Foundation; Number: 1823321, Acronym: NSF, Sponsor: National Science Foundation; ","This work has been supported in part by the NSF Grants 1823321, 1564212, 1429404, and the State of Maryland&rsquo;s MPower initiative. Any opinions, findings, conclusions, or recommendations expressed in this article are those of the authors and do not necessarily reflect the views of the research sponsors.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The missing interface: Micro-gestures on augmented objects,"Pucihar, Klen opi (1, 5); Huerst, Wolfgang (2); Kato, Hirokazu (3); Sandor, Christian (3); Plopski, Alexander (3); Leiva, Luis A. (4); Kljun, Matjaz (5); Taketomi, Takafumi (3) ","(1) University of Primorska, Slovenia (2) Utrecht University, Netherlands (3) NAIST, Japan (4) Aalto University, Finland (5) Faculty of information studies, Novo Mesto, Slovenia ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312986,3312986,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Augmenting arbitrary physical objects with digital content leads to the missing interface problem, because those objects were never designed to incorporate such digital content and so they lack a user interface. A review of related work reveals that current approaches fail due to limited detection fidelity and spatial resolution. Our proposal, based on Google Soli's radar sensing technology, is designed to detect micro-gestures on objects with sub-millimeter precision. Preliminary results with a custom gesture set show that Soli's core features and traditional machine learning models (Random Forest and Support Vector Machine) do not lead to robust recognition accuracy, and so more advanced techniques should be used instead, possibly incorporating additional sensor features. © 2019 Copyright held by the owner/author(s).",18,Gesture recognition,Augmented reality - Decision trees - Human engineering - Millimeter waves - Object detection - Radar - Support vector machines - User interfaces,Digital contents - Google Soli - Interface problems - Machine learning models - Millimeter wave radar - Physical objects - Robust recognition - Spatial resolution,"461.4 Ergonomics and Human Factors Engineering - 711 Electromagnetic Waves - 716.2 Radar Systems and Equipment - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 961 Systems Science",,,"Number: 739574, Acronym: EC, Sponsor: European Commission; ","The authors acknowledge the European Commission for funding the InnoRenew CoE project (Grant Agreement 739574) under the Horizon2020 Widespread -Teaming program and the Republic of Slovenia (Investment funding of the Republic of Slovenia and the European Union of the European regional Development Fund). This research was also supported by Slovenian research agency ARRS (program no. P1-0383 and J1-9186), Google ATAP, and the Academy of Finland (BAD project). We also acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Lookunlock: Using spatial-targets for user-authentication on HMDs,"Funk, Markus (1); Mizutani, Iori (2); Mayer, Simon (2); Marky, Karola (3); Kritzler, Mareike (4); Michahelles, Florian (4) ","(1) TU Darmstadt, Siemens CT, Darmstadt, Germany (2) University of St. Gallen, Siemens CT, St. Gallen, Switzerland (3) TU Darmstadt, Darmstadt, Germany (4) Siemens CT, Berkeley; CA, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312959,3312959,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"With head-mounted displays (HMDs), users can access and interact with a broad range of applications and data. Although some of this information is privacy-sensitive or even confidential, no intuitive, unobtrusive and secure authentication technique is available yet for HMDs. We present LookUnlock, an authentication technique for HMDs that uses passwords that are composed of spatial and virtual targets. Through a proof-of-concept implementation and security evaluation, we demonstrate that this technique can be efficiently used by people and is resistant to shoulder-surfing attacks. © 2019 Copyright held by the owner/author(s).",11,Authentication,Augmented reality - Helmet mounted displays - Human engineering,Authentication techniques - Head mounted displays - Proof of concept - Secure authentications - Security evaluation - Spatial Passwords - Usable security - User authentication,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Vibeye: Vibration-mediated object recognition for tangible interactive applications,"Oh, Seungjae (1); Yun, Gyeore (1); Park, Chaeyong (1); Kim, Jinsoo (1); Choi, Seungmoon (1) ","(1) POSTECH, Pohang, Gyeongbuk, Korea, Republic of ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300906,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"We present VibEye: a vibration-mediated recognition system of objects for tangible interaction. A user holds an object between two fingers wearing VibEye. VibEye triggers a vibration from one finger, and the vibration that has propagated through the object is sensed at the other finger. This vibration includes information about the object’s identity, and we represent it using a spectrogram. Collecting the spectrograms of many objects, we formulate the object recognition problem to a classical classification problem among the images. This simple method, when tested with 20 users, shows 92.5% accuracy for 16 objects of the same shape with various materials. This material-based classifier is also extended to the recognition of everyday objects. Lastly, we demonstrate several tangible applications where VibEye provides the needed functionality while enhancing user experiences. VibEye is particularly effective for recognizing objects made of different materials, which is difficult to distinguish by other means such as light and sound. © 2019 Copyright held by the owner/author(s).",43,Object recognition,Augmented reality - Human engineering - Spectrographs - Virtual reality,Haptics - Interactive applications - Material-based - Object recognition problem - Recognition systems - Tangible interaction - User experience - Vibration-based sensing,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 741.3 Optical Devices and Systems",,,"Number: IITP-2018-2011-1-00783, Acronym: NRF, Sponsor: National Research Foundation of Korea; Number: NRF-2017R1A2B4008144, Acronym: NRF, Sponsor: National Research Foundation of Korea; ",This work was supported by the National Research Foundation of Korea (NRF-2017R1A2B4008144) and the &lsquo;&lsquo;ICT Con-silience Creative program&rsquo;&rsquo; (IITP-2018-2011-1-00783) supervised by the IITP(Institute for Information & communications Technology Promotion).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Audio-visual AR to improve awareness of hazard zones around robots,"Martín, Ane San (1); Kildal, Johan (1) ","(1) IK4-TEKNIKER, Eibar; 20600, Spain ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312996,3312996,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Navigating a space populated by fenceless industrial robots while carrying out other tasks can be stressful, as the worker is unsure about when she is invading the area of influence of a robot, which is a hazard zone. Such areas are difficult to estimate and standing in one may have consequences for worker safety and for the productivity of the robot. We investigate the use of multimodal (auditory and/or visual) head-mounted AR displays to warn about entering hazard zones while performing an independent navigation task. As a first step in this research, we report a design-research study (including a user study), conducted to obtain a visual and an auditory AR display subjectively judged to approach equivalence. The goal is that these designs can serve as the basis for a future modality comparison study. © 2019 Copyright is held by the author/owner(s).",16,Hazards,Augmented reality - Display devices - Human engineering - Industrial robots,Audio-visual - Auditory - Collaborative robots - Head-mounted device - HoloLens - Multi-modal - Pedestrian navigation - Visual,"461.4 Ergonomics and Human Factors Engineering - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731.6 Robot Applications - 914.1 Accidents and Accident Prevention",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A design space for gaze interaction on head-mounted displays,"Hirzle, Teresa (1); Gugenheimer, Jan (1); Geiselhart, Florian (1); Bulling, Andreas (2); Rukzio, Enrico (1) ","(1) Institute of Media Informatics, Ulm University, Germany (2) Institute for Visualisation and Interactive Systems, University of Stuttgart, Germany ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300855,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Augmented and virtual reality (AR/VR) has entered the mass market and, with it, will soon eye tracking as a core technology for next generation head-mounted displays (HMDs). In contrast to existing gaze interfaces, the 3D nature of AR and VR requires estimating a user’s gaze in 3D. While first applications, such as foveated rendering, hint at the compelling potential of combining HMDs and gaze, a systematic analysis is missing. To fill this gap, we present the first design space for gaze interaction on HMDs. Our design space covers human depth perception and technical requirements in two dimensions aiming to identify challenges and opportunities for interaction design. As such, our design space provides a comprehensive overview and serves as an important guideline for researchers and practitioners working on gaze interaction on HMDs. We further demonstrate how our design space is used in practice by presenting two interactive applications: EyeHealth and XRay-Vision. © 2019 Copyright held by the owner/author(s).",64,Eye tracking,Augmented reality - Depth perception - Helmet mounted displays - Human engineering - Virtual reality,3D gaze - Design spaces - Gaze interaction - Head mounted displays - Interaction design,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Interactive fabrication of CSG models with assisted carving,"Hattab, Ammar (1); Taubin, Gabriel (2) ","(1) School of Engineering, Brown University, Providence; RI, United States (2) School of Engineering, Brown University Providence, RI, United States ","TEI 2019 - Proceedings of the 13th International Conference on Tangible, Embedded, and Embodied Interaction",ACM SIGCHI,"Association for Computing Machinery, Inc",,p 677-682,17-Mar-19,"TEI 2019 - Proceedings of the 13th International Conference on Tangible, Embedded, and Embodied Interaction",2019,,,,9.78145E+12,10.1145/3294109.3295644,,"13th International Conference on Tangible, Embedded, and Embodied Interaction, TEI 2019","March 17, 2019 - March 20, 2019",,"We propose a method that helps an unskilled user to carve a physical replica of a 3D CAD model while only using manual cutting tools. The method starts by analyzing the input CAD model and generates a set of carving instructions. Then using a projector, we project the instructions sequentially one at a time to a block of material to guide the user in performing each of them. After each cutting step, we use the projector-camera setup to 3D scan the object after cutting. And automatically align the scanned point cloud to the CAD model, to prepare the position for the next instruction. We demonstrate a complete system to support this operation and show several examples manually carved while using the system. © 2019 Association of Computing Machinery.",26,Computer aided design,3D modeling - Augmented reality - Cutting tools,3D CAD Modeling - Assisted Carving - CAD modeling - Complete system - Point cloud - Projector-camera - Solid model - Spatial augmented realities,"603.2 Machine Tool Accessories - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: IIP-1500249, Acronym: NSF, Sponsor: National Science Foundation; Number: IIS-1717355, Acronym: NSF, Sponsor: National Science Foundation; ",The work described herein was partially supported by a Brown Fellowship and by NSF grants IIS-1717355 and IIP-1500249.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
For a better (simulated) world: Considerations for VR in external communication research,"Colley, Mark (1); Walch, Marcel (1); Rukzio, Enrico (1) ","(1) Institute of Media Informatics, Ulm University, Germany ","Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",cerence; Helmholtz Instituut; here; Rijkswaterstaat - Ministry of Infrastructure and Water Management; Uber ARG; Utrecht University - Faculty of Social and Behavioral Sciences,"Association for Computing Machinery, Inc",,p 442-449,21-Sep-19,"Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",2019,,,,9.78145E+12,10.1145/3349263.3351523,,"11th ACM International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019","September 21, 2019 - September 25, 2019",,"In the emerging research field of external communication of autonomous vehicles with vulnerable road users (e.g. pedestrians), there is no agreed upon set of methods to design and evaluate concepts. The approaches vary from pure paper-based design studies over Virtual or Augmented Reality simulation to real-world testing of early prototypes. While there are benefits to each of these approaches, the most promising concept is considered to be the virtual reality (VR) approach since it allows for a quick, realistic and safe evaluation of new designs and concepts. A literature review of existing concepts for vehicle-pedestrian communication revealed that only 7 publications and preprints between 2014 and 2019 used VR in their research. We evaluated each based on criteria relevant for pedestrian crossing decisions and factors important for conducting experiments. Our results show relevant considerations when implementing a VR simulator for external communication research and conducting studies in this field. © 2019 Copyright is held by the owner/author(s).",50,Vehicle to vehicle communications,Augmented reality - Autonomous vehicles - Pedestrian safety - User interfaces - Virtual reality,External communications - Interface designs - Literature reviews - Pedestrians - Real-world testing - Research fields - Self drivings - Vehicle PEdestrian Communications,"406.2 Roads and Streets - 716 Telecommunication; Radar, Radio and Television - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,"This work was conducted within the project Intuitiver funded by the Ministry of Science, Research and the Arts of the State of Baden-W&uuml;rttemberg.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Challenges using head-mounted displays in shared and social spaces,"Gugenheimer, Jan (1); McGill, Mark (2); Steinicke, Frank (3); Mai, Christian (4); Williamson, Julie (2); Perlin, Ken (5) ","(1) Ulm University, Ulm, Germany (2) University of Glasgow, Glasgow, United Kingdom (3) University of Hamburg, Hamburg, Germany (4) LMU Munich, Munich, Germany (5) New York University, New York, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3299028,3299028,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Everyday mobile usage of AR and VR Head-Mounted Displays (HMDs) is becoming a feasible consumer reality. The current research agenda for HMDs has a strong focus on technological impediments (e.g. latency, field of view, locomotion, tracking, input) as well as perceptual aspect (e.g. distance compression, vergence-accomodation). However, this ignores significant challenges in the usage and acceptability of HMDs in shared, social and public spaces. This workshop will explore these key challenges of HMD usage in shared, social contexts; methods for tackling the virtual isolation of the VR/AR user and the exclusion of collocated others; the design of shared experiences in shared spaces; and the ethical implications of appropriating the environment and those within it. © 2019 Copyright held by the owner/author(s).",40,Helmet mounted displays,Augmented reality - Human engineering - Mixed reality - Virtual reality,Ethical implications - Field of views - Head mounted displays - Perceptual aspects - Research agenda - Shared experiences - Social acceptability - Social context,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Holodoc: Enabling mixed reality workspaces that harness physical and digital content,"Li, Zhen (1); Annett, Michelle (2); Hinckley, Ken (3); Singh, Karan (1); Wigdor, Daniel (1) ","(1) University of Toronto, Toronto; ON, Canada (2) MishMashMakers, Newmarket; ON, Canada (3) Microsoft Research, Redmond; WA, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300917,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Prior research identified that physical paper documents have many positive attributes, for example natural tangibility and inherent physical flexibility. When documents are presented on digital devices, however, they can provide unique functionality to users, such as the ability to search, view dynamic multimedia content, and make use of indexing. This work explores the fusion of physical and digital paper documents. It first presents the results of a study that probed how users perform document-intensive analytical tasks when both physical and digital versions of documents were available. The study findings then informed the design of HoloDoc, a mixed reality system that augments physical artifacts with rich interaction and dynamic virtual content. Finally, we present the interaction techniques that HoloDoc affords, and the results of a second study that assessed HoloDoc’s utility when working with digital and physical copies of academic articles. © 2019 Copyright held by the owner/author(s).",63,Mixed reality,Augmented reality - Digital devices - Dynamics - Human engineering,Digital contents - Digital pens - Digital versions - Interaction techniques - Mixed reality systems - Multimedia contents - Physical artifacts - Reading behavior,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
TrackCap: Enabling smartphones for 3D interaction on mobile head-mounted displays,"Mohr, Peter (1, 2); Tatzgern, Markus (3); Langlotz, Tobias (4); Lang, Andreas (1, 3); Schmalstieg, Dieter (1); Kalkofen, Denis (1) ","(1) Graz University of Technology, Austria (2) VRVis GmbH, Austria (3) Salzburg University of Applied Sciences, Austria (4) University of Otago, New Zealand ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300815,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"The latest generation of consumer market Head-mounted displays (HMD) now include self-contained inside-out tracking of head motions, which makes them suitable for mobile applications. However, 3D tracking of input devices is either not included at all or requires to keep the device in sight, so that it can be observed from a sensor mounted on the HMD. Both approaches make natural interactions cumbersome in mobile applications. TrackCap, a novel approach for 3D tracking of input devices, turns a conventional smartphone into a precise 6DOF input device for an HMD user. The device can be conveniently operated both inside and outside the HMD’s field of view, while it provides additional 2D input and output capabilities. © 2019 Copyright held by the owner/author(s).",34,Helmet mounted displays,Augmented reality - Human engineering - Knobs - Mixed reality - Mobile computing - Mobile computing - Smartphones - Three dimensional displays,3D interactions - Consumer market - Head mounted displays - Input and outputs - Input devices - Mobile applications - Natural interactions - Wearable computing,"461.4 Ergonomics and Human Factors Engineering - 716 Telecommunication; Radar, Radio and Television - 718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: 52421, Acronym: NPST, Sponsor: National Plan for Science, Technology and Innovation; Number: 859208, Acronym: FFG, Sponsor: &Ouml;sterreichische Forschungsf&ouml;rderungsgesellschaft; ","This work was enabled by the Competence Center VRVis, the FFG (grant 859208 - Matahari) and the EU FP7 project MAGELLAN (ICT-FP7-611526). VRVis is funded by BMVIT, BMWFW, Styria, SFG and Vienna Business Agency in the scope of COMET - Competence Centers for Excellent Technologies (854174) which is managed by FFG. Furthermore, Tobias is partially supported by Callaghan Innovation, host of the Science for Technological Innovation National Science Challenge, Seed Project 52421, and by the Marsden Fund Council from Government funding, administered by the Royal Society of NZ.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmentation not duplication: Considerations for the design of digitally-augmented comic books,"Kljun, Matja (1); Pucihar, Klen opi (1, 5); Alexander, Jason (2); Weerasinghe, Maheshya (1, 6); Campos, Cuauhtli (1); Ducasse, Julie (1); Kopain, Barbara (1); Grubert, Jens (3); Coulton, Paul (2); elar, Miha (4) ","(1) University of Primorska, Koper, Slovenia (2) Lancaster University, Lancaster, United Kingdom (3) Coburg University, Coburg, Germany (4) Astral Film, Ljubljana, Slovenia (5) Faculty of Information Studies, Novo Mesto, Slovenia (6) SACHI, School of CS, University of St. Andrews, United Kingdom ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300333,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Digital-augmentation of print-media can provide contextually relevant audio, visual, or haptic content to supplement the static text and images. The design of such augmentation—its medium, quantity, frequency, content, and access technique—can have a significant impact on the reading experience. In the worst case, such as where children are learning to read, the print medium can become a proxy for accessing digital content only, and the textual content is avoided. In this work, we examine how augmented content can change the reader’s behaviour with a comic book. We first report on the usage of a commercially available augmented comic for children, providing evidence that a third of all readers converted to simply viewing the digital media when printed content is duplicated. Second, we explore the design space for digital content augmentation in print media. Third, we report a user study with 136 children that examined the impact of both content length and presentation in a digitally-augmented comic book. From this, we report a series of design guidelines to assist designers and editors in the development of digitally-augmented print media. © 2019 Copyright held by the owner/author(s).",47,Human engineering,Augmented reality - Digital storage,Access techniques - Comic books - Design spaces - Digital augmentation - Digital contents - Print media - Textual content - User study,"461.4 Ergonomics and Human Factors Engineering - 722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications",,,"Number: J1–9186, Acronym: ARRS, Sponsor: Javna Agencija za Raziskovalno Dejavnost RS; Number: P1–0383, Acronym: ARRS, Sponsor: Javna Agencija za Raziskovalno Dejavnost RS; ",The authors acknowledge the European Commission for funding the InnoRenew CoE project (Grant Agreement 739574) under the Horizon2020 Widespread-Teaming program and the Republic of Slovenia (Investment funding of the Republic of Slovenia and the European Union of the European regional Development Fund). The research was also supported by Slovenian research agency ARRS (P1&ndash;0383 and J1&ndash;9186).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
'I simply watched where she was looking at': Coordination in short-term synchronous cooperative mixed reality,"Prilla, Michael (1) ","(1) Information and Technology Management, Institute for Informatics, Clausthal University of Technology, Julius-Albert-Str. 4, Clausthal-Zellerfeld; 38678, Germany ",Proceedings of the ACM on Human-Computer Interaction,,Association for Computing Machinery,"v 3, n GROUP",,Dec-19,,2019,,,25730142,,10.1145/3361127,246,,,,"Mixed reality (MR) cooperation scenarios are more and more interesting for business and research as powerful wearable devices like head mounted displays (HMD) become commercially available. A lot of work focuses on remote MR cooperation settings like remote maintenance support and co-located scenarios in which participants cooperate over a longer period time. Despite this, MR also has great potential for realtime co-located cooperation support with the need of short-term decisions and interactions. However, little is known on how this support can be provided. To bridge this gap, we conducted an experiment using a MR visual search task performed by dyads. Based on related work, visual search was chosen to represent typical challenges of short-term cooperative MR tasks. The aim of the experiment was to explore how the participants coordinate their searches and how this influences their performance in a task. We found that participants mainly used embodied and verbal cues to coordinate their searches (rather than virtual cues provided by the HMD) and that less communication worked significantly better, which is in (partial) contrast to existing findings. We discuss potential reasons for and impacts of these findings. Copyright is held by the owner/author(s). Publication rights licensed to ACM. © 2019 Association for Computing Machinery. All rights reserved.",54,Mixed reality,Augmented reality - Helmet mounted displays,Cooperation - Coordination - Head mounted displays - Related works - Remote maintenance support - Short term - Visual search - Wearable devices,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Digitally augmenting the physical ground space with timed visual cues for crutch-assisted walking,"Peres, Beatriz (1); Azadegan, Aida (2); Campos, Pedro F. (3) ","(1) M-ITI, University of Madeira, Funchal, Portugal (2) Intelligent Systems Research Laboratory, University of Reading, Reading, United Kingdom (3) ITI/Larsys, University of Madeira, Funchal, Portugal ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312891,3312891,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"This late-breaking work presents initial results regarding a novel mobile-projection system, aimed at helping people to learn how to walk with crutches. The existing projection-based solutions for gait training disorders are based on walking over a fixed surface (usually a treadmill). In contrast, our solution projects visual cues (footprints and crutch icons) directly into the floor, augmenting the physical space surrounding the crutches, in a portable way. Walking with crutches is a learning skill that requires continuous repetition and constant attention to detail to make sure they are being used correctly, avoiding negative consequences, such as falls or injuries. We conducted expert consultation sessions, and we identified the main issues that patients face when walking with crutches. This informed the design of Augmented Crutches. We performed a qualitative evaluation and conclude with design implications: the importance of timing, self-assurance and awareness. © 2019 Copyright held by the owner/author(s).",8,Walking aids,Augmented reality - Human engineering - Patient rehabilitation - User interfaces,Assisted Walking - Attention to details - Augmented Experiences - Design implications - Expert consultation - Interaction design - Qualitative evaluations - User experience,"461.4 Ergonomics and Human Factors Engineering - 461.5 Rehabilitation Engineering and Assistive Technology - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: UI, Sponsor: University of Ibadan; ","We thank all the volunteers and therapists for sparing their time and helping us design an effective solution. This project was partially funded by the FCT UID/EEA/50009/2019, by the Regional Project M1420-01-0145-FEDER-000002, and by the SUAVE project M1420-01-0247-FEDER-000019.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An evaluation of radar metaphors for providing directional stimuli using non-verbal sound,"Cassidy, Brendan (1); Read, Janet C. (1); MacKenzie, I. Scott (2) ","(1) Child Computer Interaction Group, University of Central Lancashire, Preston, Lancashire, United Kingdom (2) Department of Computing, York University, Toronto, Canada ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300289,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"We compared four audio-based radar metaphors for providing directional stimuli to users of AR headsets. The metaphors are clock face, compass, white noise, and scale. Each metaphor, or method, signals the movement of a virtual arm in a radar sweep. In a user study, statistically significant differences were observed for accuracy and response time. Beat-based methods (clock face, compass) elicited responses biased to the left of the stimulus location, and non-beat-based methods (white noise, scale) produced responses biased to the right of the stimulus location. The beat methods were more accurate than the non-beat methods. However, the non-beat methods elicited quicker responses. We also discuss how response accuracy varies along the radar sweep between methods. These observations contribute design insights for non-verbal, non-visual directional prompting. © 2019 Copyright is held by the owner/author(s).",22,White noise,Acoustic noise - Augmented reality - Clocks - Human engineering - Radar,Accessibility - Directional prompting - Headset - Spatial audio - Visual impairment,"461.4 Ergonomics and Human Factors Engineering - 716.2 Radar Systems and Equipment - 723 Computer Software, Data Handling and Applications - 751.4 Acoustic Noise - 943.3 Special Purpose Instruments",,,"Number: EP/L027658/1, Acronym: EPSRC, Sponsor: Engineering and Physical Sciences Research Council; ",This research was part funded by UK Research Council EPSRC Grant No. EP/L027658/1. &lsquo;WE ARe ABLE: Displays and Play&rsquo;.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Enhancing Indoor Inertial Odometry with WiFi,"Venkatnarayan, R.H. (1); Shahzad, M. (1) ","(1) North Carolina State Univ., Raleigh, NC, United States ","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",,"ACM, USA","v 3, n 2",47 (27 pp.),Jun-19,,,,2474-9567,,,10.1145/3328918,,,,,"Accurately measuring the distance traversed by a subject, commonly referred to as odometry, in indoor environments is of fundamental importance in many applications such as augmented and virtual reality tracking, indoor navigation, and robot route guidance. While theoretically, odometry can be performed using a simple accelerometer, practically, it is well-known that the distances measured using accelerometers suffer from large drift errors. In this paper, we propose WIO, a WiFi-assisted Inertial Odometry technique that uses WiFi signals as an auxiliary source of information to correct these drift errors. The key intuition behind WIO is that among multiple reflections of a transmitted WiFi signal arriving at the WiFi receiver, WIO first isolates one reflection and then measures the change in the length of the path of that reflection as the subject moves. By identifying the extent through which the length of the path of that reflection changes, along with the direction of motion of the subject relative to that path, WIO can estimate the distance traversed by the subject using WiFi signals. WIO then uses this distance estimate to correct the drift errors. While researchers have previously proposed to use WiFi signals to correct drift errors, prior schemes suffer from one or more of the following six limitations: they 1) do not work indoors, 2) require manual exhaustive fingerprinting, 3) are not resilient against changes in environment including human movements, 4) do not work on commodity WiFi devices, 5) require multiple access points, and/or 6) can measure distance traversed by humans but not by non-human subjects. WIO addresses all of these limitations. We implemented WIO using commodity devices, and extensively evaluated it in a wide variety of complex indoor scenarios on both human and robotic subjects. Our results demonstrate that WIO achieved an average error of just 6.28% in estimating the distances traversed by the subjects.",,,computerised instrumentation - distance measurement - radio receivers - wireless LAN,augmented reality tracking - virtual reality tracking - indoor navigation - robot route guidance - accelerometer - WiFi signal transmission - distance measurement - WIO addresses - complex indoor scenarios - robotic subjects - indoor inertial odometry - WiFi receiver devices - WiFi-assisted inertial odometry technique,A0630C Spatial variables measurement - B7320C Spatial variables measurement - B6210L Computer communications - B6250 Radio links and equipment - B7210B Computerised instrumentation - C7410H Computerised instrumentation - C5620L Local area networks,G01B - H04B1/06 - H04B7/00 - H04L12/28 - H04W - H04W84/18 - H04W84/12,Practical (PRA); Theoretical or Mathematical (THR),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Syn(es)thetic Reality: Simulating synesthesia for the non synesthetic,"Khare, Vinay (1); Fraguada, Luis Edgardo (2); Bigger, Elizabeth Esther (2) ","(1) Institute for Advanced Architecture of Catalunya, Barcelona, Spain (2) Datable Studio Barcelona, Spain ","Proceedings - International Symposium on Wearable Computers, ISWC",emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,Association for Computing Machinery,,p 290-295,9-Sep-19,ISWC 2019 - Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,15504816,,9.78145E+12,10.1145/3341163.3346940,,"23rd International Symposium on Wearable Computers, ISWC 2019","September 9, 2019 - September 13, 2019",,"Syn(es)thetic Reality explores a new way of sensing the world by understanding sounds through colors. It looks to simulate projective chromesthesia, an experience of seeing colors involuntarily as a result of sound input. The project achieves this through an augmented reality web application to be run on mobile phones via a wearable device. By displaying audio based color content composited over the camera feed, the user's visual perception is altered to reflect their auditory perception. The basic principle is to correlate sound frequencies to hue and amplitude to saturation and visualise these colors creatively. © 2019 Association for Computing Machinery.",5,Color,Augmented reality - Wearable computers,Chromesthesia - Google Cardboard - Mediated Reality - Mobile Technology - Synesthesia,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 741.1 Light/Optics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ASV: Accelerated Stereo Vision System [arXiv],"Yu Feng; Whatmough, P.; Yuhao Zhu ",,arXiv,,"arXiv, USA",,14 pp.,15 Nov. 2019,,,,,,,,,,,,"Estimating depth from stereo vision cameras, i.e., 'depth from stereo', is critical to emerging intelligent applications deployed in energy- and performance-constrained devices, such as augmented reality headsets and mobile autonomous robots. While existing stereo vision systems make trade-offs between accuracy, performance and energy-efficiency, we describe ASV, an accelerated stereo vision system that simultaneously improves both performance and energy-efficiency while achieving high accuracy. The key to ASV is to exploit unique characteristics inherent to stereo vision, and apply stereo-specific optimizations, both algorithmically and computationally. We make two contributions. Firstly, we propose a new stereo algorithm, invariant-based stereo matching (ISM), that achieves significant speedup while retaining high accuracy. The algorithm combines classic 'hand-crafted' stereo algorithms with recent developments in Deep Neural Networks (DNNs), by leveraging the correspondence invariant unique to stereo vision systems. Secondly, we observe that the bottleneck of the ISM algorithm is the DNN inference, and in particular the deconvolution operations that introduce massive compute-inefficiencies. We propose a set of software optimizations that mitigate these inefficiencies. We show that with less than 0.5% hardware area overhead, these algorithmic and computational optimizations can be effectively integrated within a conventional DNN accelerator. Overall, ASV achieves 5x speedup and 85% energy saving with 0.02% accuracy loss compared to today DNN-based stereo vision systems. [In Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO '52). ACM, New York, NY, USA, 643-656 (2019) doi:10.1145/3352460.3358253].",78,,cameras - image matching - neural nets - optimisation - stereo image processing,software optimizations - deconvolution operations - deep neural networks - mobile autonomous robots - augmented reality headsets - energy-constrained devices - stereo-specific optimizations - energy-efficiency - performance-constrained devices - stereo vision cameras - accelerated stereo vision system - DNN-based stereo vision systems - ASV - computational optimizations - algorithmic optimizations - ISM algorithm - stereo algorithm - invariant-based stereo matching,B6135E Image recognition - B0260 Optimisation techniques - C5260B Computer vision and image processing techniques - C5290 Neural computing techniques - C1180 Optimisation techniques,G03B17/00 - G03B19/00 - G06T - H04N13/00,Practical (PRA); Theoretical or Mathematical (THR),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Lost in style: Gaze-driven Adaptive Aid for VR Navigation,"Alghofaili, Rawan (1); Sawahata, Yasuhito (2); Huang, Haikun (3); Wang, Hsueh-Cheng (4); Shiratori, Takaaki (5); Yu, Lap-Fai (1) ","(1) George Mason University, Fairfax; VA, United States (2) Japan Broadcasting Corporation, Setagaya, Tokyo, Japan (3) University of Massachusetts-Boston, Boston; MA, United States (4) National Chiao Tung University, Hsinchu, Taiwan (5) Facebook Reality Labs, Pittsburgh; PA, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300578,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"A key challenge for virtual reality level designers is striking a balance between maintaining the immersiveness of VR and providing users with on-screen aids after designing a virtual experience. These aids are often necessary for wayfnding in virtual environments with complex paths. We introduce a novel adaptive aid that maintains the effectiveness of traditional aids, while equipping designers and users with the controls of how often help is displayed. Our adaptive aid uses gaze patterns in predicting user’s need for navigation aid in VR and displays mini-maps or arrows accordingly. Using a dataset of gaze angle sequences of users navigating a VR environment and markers of when users requested aid, we trained an LSTM to classify user’s gaze sequences as needing navigation help and display an aid. We validated the efcacy of the adaptive aid for wayfnding compared to other commonly-used wayfnding aids. © 2019 Association for Computing Machinery.",32,Eye tracking,Classification (of information) - Human engineering - Navigation - Virtual reality,Games/Play - Gaze angles - Immersiveness - Navigation aids - Virtual/Augmented reality,"461.4 Ergonomics and Human Factors Engineering - 716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications",,,"Number: 1565978, Acronym: NSF, Sponsor: National Science Foundation; ",We thank Kristen Laird for her help in conducting the user studies. This research is supported by the National Science Foundation under award number 1565978. We are grateful to the anonymous reviewers for their constructive comments.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Direct finger manipulation of 3D object image with ultrasound haptic feedback,"Matsubayashi, Atsushi (1); Makino, Yasutoshi (2); Shinoda, Hiroyuki (2) ","(1) University of Tokyo, Bunkyo-ku, Tokyo, Japan (2) University of Tokyo, Kashiwa-shi, Chiba, Japan ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300317,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"In this study, we prototype and examine a system that allows a user to manipulate a 3D virtual object with multiple fingers without wearing any device. An autostereoscopic display produces a 3D image and a depth sensor measures the movement of the fingers. When a user touches a virtual object, haptic feedback is provided by ultrasound phased arrays. By estimating the cross section of the finger in contact with the virtual object and by creating a force pattern around it, it is possible for the user to recognize the position of the surface relative to the finger. To evaluate our system, we conducted two experiments to show that the proposed feedback method is effective in recognizing the object surface and thereby enables the user to grasp the object quickly without seeing it. © 2019 Copyright held by the owner/author(s).",33,Feedback,Display devices - Human engineering - Stereo image processing - Ultrasonics,Auto-stereoscopic display - Finger manipulation - Haptic feedbacks - Object surface - Touch/Haptic/Pointing/Gesture - Ultrasound phased arrays - Virtual objects - Virtual/Augmented reality,461.4 Ergonomics and Human Factors Engineering - 722.2 Computer Peripheral Equipment - 723.2 Data Processing and Image Processing - 731.1 Control Systems - 753.1 Ultrasonic Waves,,,"Number: 16H06303, Acronym: JSPS, Sponsor: Japan Society for the Promotion of Science; Number: JPMJCR18A2, Acronym: JST, Sponsor: Japan Science and Technology Corporation; ",The authors would like to thank Professor Hideki Kakeya for lending the stereoscopic display used in the research. This work is supported in part by the JSPS Grant-in-Aid for Scientific Research 16H06303 and JST CREST JPMJCR18A2.,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Multi-tier caching analysis in CDN-based over-the-top video streaming systems,"Al-Abbasi, A.O. (1); Aggarwal, V. (1); Moo-Ryong Ra (2) ","(1) Sch. of IE, Purdue Univ., West Lafayette, IN, United States (2) Cloud Platform Software Res. Dept., AT&T Labs. Res., Bedminster, NJ, United States ",IEEE/ACM Transactions on Networking,,"IEEE, USA","v 27, n 2",835-47,Apr-19,,,,1063-6692,,,10.1109/TNET.2019.2900434,,,,,"Internet video traffic has been rapidly increasing and is further expected to increase with the emerging 5G applications, such as higher definition videos, the IoT, and augmented/virtual reality applications. As end users consume video in massive amounts and in an increasing number of ways, the content distribution network (CDN) should be efficiently managed to improve the system efficiency. The streaming service can include multiple caching tiers, at the distributed servers and the edge routers, and efficient content management at these locations affects the quality of experience (QoE) of the end users. In this paper, we propose a model for video streaming systems, typically composed of a centralized origin server, several CDN sites, and edge-caches located closer to the end user. We comprehensively consider different systems design factors, including the limited caching space at the CDN sites, allocation of CDN for a video request, choice of different ports (or paths) from the CDN and the central storage, bandwidth allocation, the edge-cache capacity, and the caching policy. We focus on minimizing a performance metric, stall duration tail probability (SDTP), and present a novel and efficient algorithm accounting for the multiple design flexibilities. The theoretical bounds with respect to the SDTP metric are also analyzed and presented. The implementation of a virtualized cloud system managed by Openstack demonstrates that the proposed algorithms can significantly improve the SDTP metric compared with the baseline strategies.",46,,5G mobile communication - augmented reality - bandwidth allocation - cache storage - client-server systems - cloud computing - content management - telecommunication network routing - video streaming,augmented-virtual reality applications - systems design factors - CDN-based over-the-top video streaming systems - novel accounting - caching policy - edge-cache capacity - video request - caching space - CDN sites - centralized origin server - efficient content management - edge routers - distributed servers - multiple caching tiers - streaming service - system efficiency - content distribution network - end user - higher definition videos - emerging 5G applications - internet video traffic - multitier caching analysis - virtualized cloud system - efficient algorithm accounting,"B6430G Video on demand and video servers - B6150P Communication network design, planning and routing - B6210R Multimedia communications - B6250F Mobile radio systems - B6210L Computer communications - C5620W Other computer networks - C6190J Internet software - C6120 File organisation - C6130V Virtual reality",H04N - G06F9/44 - G06F12/00 - H04B7/00 - H04B7/26 - H04L12/28 - H04W - H04W16/00 - H04W40/00,Practical (PRA); Theoretical or Mathematical (THR),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Building and enhancing stereographic digital collections: working across departments to augment reality,"Ricupero, B. (1); Dalton, G. (2); Lehman, A. (1) ","(1) Digital Collections/Coe Libr., Univ. of Wyoming, Laramie, WY, United States (2) Special Collections/Coe Libr., Univ. of Wyoming, Laramie, WY, United States ",2019 ACM/IEEE Joint Conference on Digital Libraries (JCDL). Proceedings,,"IEEE Computer Society, Los Alamitos, CA, USA",,343-4,2019,,,,,,978-1-7281-1547-4,10.1109/JCDL.2019.00062,,2019 ACM/IEEE Joint Conference on Digital Libraries (JCDL),2-6 June 2019,"Champaign, IL, USA","On July 18, 2018, the University of Wyoming Librarieswere awarded a grant through the State Historical RecordsAdvisory Board (SHRAB). Funded through the National Historical Publications and Records Commission (NHPRC), the project scope includes the digitization of a multi-institutional stereoview collection, and its eventual augmentation to near 3D representation. Collection contributions will come from the William Robertson Coe Library and the Laramie Plains Museum.",6,,augmented reality - digital libraries - history - image representation - information retrieval systems - stereo image processing - Web sites,digitization - multiinstitutional stereoview collection - eventual augmentation - collection contributions - SHRAB - State Historical Records Advisory Board - Wyoming Libraries - stereographic digital collections - 3D representation - William Robertson Coe Library - Laramie Plains Museum - National Historical Publications and Records Commission,"B6135 Optical, image and video signal processing - C7820 Humanities computing - C7250R Information retrieval techniques - C5260B Computer vision and image processing techniques - C6130B Graphics techniques - C6130V Virtual reality - C7210N Information networks",G06F17/30 - G06T - H04N13/00,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
'The printer is telling me about itself': Supporting the appropriation of hardware by using projection mapping,"Ludwig, Thomas (1); Döll, Michael (1); Kotthaus, Christoph (1) ","(1) University of Siegen, Siegen, Germany ",DIS 2019 - Proceedings of the 2019 ACM Designing Interactive Systems Conference,Adobe; Google; Sketch; UC San Diego's Design Lab; Virginia Tech,"Association for Computing Machinery, Inc",,p 331-344,18-Jun-19,DIS 2019 - Proceedings of the 2019 ACM Designing Interactive Systems Conference,2019,,,,9.78145E+12,10.1145/3322276.3322342,,"2019 ACM Conference on Designing Interactive Systems, DIS 2019","June 23, 2019 - June 28, 2019",,"The increasing complexity of cyber-physical systems poses special challenges for users to be able to appropriate and apply such technologies within their practice. Classic tools to support appropriation have usually taken the form of written manuals or video tutorials. However, recent research regarding appropriation infrastructures and sociable technologies has suggested that appropriation support functionality can be integrated directly into software and cyber-physical systems. The problem confronting this kind of support is the adequate visualization of the information and the provision of user interfaces that could offer the necessary basis for appropriation. Based on the example of 3D printing, we examine how projection mapping-as an innovative form of visualization-can be used as a user interface for hardware-related appropriation support. Through reflections upon the design and evaluation of a projection-based 3D-printer system, we provide insights that extend the notion of appropriation support to encompass projection mapping and that can contribute to the future development of projection-based human-machine interfaces. © 2019 Copyright is held by the owner/author(s).",74,User interfaces,3D printers - Augmented reality - Cyber Physical System - Embedded systems - Mapping - Printing presses - Three dimensional computer graphics - Visualization,3-D printing - Appropriation - Design and evaluations - Human Machine Interface - Printer systems - Recent researches - Video tutorials,"405.3 Surveying - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 745.1.1 Printing Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Building and enhancing stereographic digitial collections: Working across departments to augment reality,"Ricupero, Bryan (1); Dalton, Glory (2); Lehman, Amanda (1) ","(1) Digital Collections/Coe Library, University of Wyoming, Laramie; WY, United States (2) Special Collections/Coe Library, University of Wyoming, Laramie; WY, United States ",Proceedings of the ACM/IEEE Joint Conference on Digital Libraries,Association for Computing Machinery (ACM); IEEE; IEEE Computer Society,Institute of Electrical and Electronics Engineers Inc.,v 2019-June,p 343-344,Jun-19,"Proceedings - 2019 ACM/IEEE Joint Conference on Digital Libraries, JCDL 2019",2019,,15525996,,9.78173E+12,10.1109/JCDL.2019.00062,8791202,"19th ACM/IEEE Joint Conference on Digital Libraries, JCDL 2019","June 2, 2019 - June 6, 2019",,"On July 18, 2018, the University of Wyoming Librarieswere awarded a grant through the State Historical RecordsAdvisory Board (SHRAB). Funded through the National Historical Publications and Records Commission (NHPRC), the project scope includes the digitization of a multi-institutional stereoview collection, and its eventual augmentation to near 3D representation. Collection contributions will come from the William Robertson Coe Library and the Laramie Plains Museum. © 2019 IEEE.",6,Digital libraries,Information systems - Open source software - Open systems,3d representations - Human-centered computing - Mixed/augmented reality - Project scope - Wyoming,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 903.2 Information Dissemination",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Asthma-nauts: Apps using gameplay to collect health metrics and educate kids about asthma,"Jayaprakash, Karthika Priya (1); Stephens, Cal (2); Lesnick, Burton (3); Arriaga, Rosa I. (1) ","(1) Interactive Computing, Georgia Institute of Technology, GA, United States (2) College of Computer Science, Georgia Institute of Technology, GA, United States (3) Children's Healthcare of Atlanta, Atlanta; GA, United States ","Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW",ACM SIGCHI; Adobe; Amazon; et al.; Facebook; IBM Research,Association for Computing Machinery,,p 226-230,9-Nov-19,CSCW 2019 Companion - Conference Companion Publication of the 2019 Computer Supported Cooperative Work and Social Computing,2019,,,,9.78145E+12,10.1145/3311957.3359483,,"22nd ACM Conference on Computer-Supported Cooperative Work and Social Computing, CSCW 2019","November 9, 2019 - November 13, 2019",,"In this paper, we propose the designs for two apps that benefit pediatric patients with asthma. We discuss the current state of the field through results from a survey of published apps related to asthma. We observed several key gaps that were unfilled and use these results to derive a set of requirements for a new app design. We describe two different designs that meet these requirements. The apps are designed to collect health metrics and educate kids aged 7-12 years through gameplay in two different contexts, at home and in hospital Emergency Departments (ED). © 2019 Copyright is held by the owner/author(s).",8,Diseases,Application programs - Augmented reality - Design - Education - Groupware - Hospitals - Interactive computer systems - mHealth - Pediatrics - Ubiquitous computing,Asthma - Gameplay - Hospital emergency departments - Lung function - Mobile applications - Pediatric patients,"461.4 Ergonomics and Human Factors Engineering - 461.6 Medicine and Pharmacology - 462.2 Hospitals, Equipment and Supplies - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Shadower: Applying shadows to children’s outdoor interaction,"Chen, Yang (1); Lin, Yuyu (1); Liu, Lijuan (1); Yao, Cheng (1); Ying, Fangtian (2) ","(1) Zhejiang University, Hangzhou, China (2) China Academy of Art, Hangzhou, China ",UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,p 33-36,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,10.1145/3341162.3343843,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,"Shadows are ubiquitous in our life. Through the phenomena of shadows, we focus on a novel way which can change people’s perspectives of observing surroundings and create a natural outdoor interaction for children in their daily life. In this paper, we present a mobile AR game which uses shadows as clues with a treasure hunting game mechanism to make a connection with children and outdoor surroundings. In the field experiment at a kindergarten, children(n=6) participated in the outdoor interaction experience with Shadower. We conducted a preliminary user study with Smileyometer strategy to evaluate children’s reaction to our prototype. Qualitative results indicate that the use of shadows from the outdoor environment as AR makers have the potential to expand the approach to facilitate children’s engagement through their outdoor interaction. © 2019 Copyright held by the owner/author(s).",11,Ubiquitous computing,Augmented reality - Wearable computers,Children - Daily lives - Field experiment - Interaction experiences - Outdoor environment - Outdoor interaction - Shadow - User study,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: 61332017, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; ",We appreciate all user study participants. The project is supported by the National Natural Science Foundation of China (Grant No. 61332017).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The hitchhiker’s guide to the eyewear applications,"Kim, Suzi (1); Choi, Sunghee (1) ","(1) School of Computing, KAIST, Korea, Republic of ",UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,p 633-636,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,10.1145/3341162.3348386,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,"We have become accustomed to flat screens over a long time because flat screens are both easy to manufacture and easy to carry around. However, there is a huge gap between the 3D experiences that we see directly with our eyes and those viewed through a flat screen. This paper compares 3D experiences through eyewear and other media. Based on the strengths of eyewear, we suggest several design considerations for eyewear applications vividly conveying the actual 3D experience. © 2019 Association for Computing Machinery.",5,Ubiquitous computing,Augmented reality - Mixed reality - Screens (sizing) - Virtual reality - Wearable computers,3d experiences - Design considerations - Eyewear - Flat screens - Smart glass,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: -, Acronym: MSIT, Sponsor: Ministry of Science and ICT, South Korea; Number: -, Acronym: IITP, Sponsor: Institute for Information and Communications Technology Promotion; ","This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2019-0-01158, Development of a Framework for 3D Geometric Model Processing)",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Human-centered, ergonomic wearable device with computer vision augmented intelligence for VR multimodal human-smart home object interaction","Ker-Jiun Wang (1); Zheng, C.Y. (2); Zhi-Hong Mao (3) ","(1) Dept. of Bioeng., Univ. of Pittsburgh, Pittsburgh, PA, United States (2) Inf. Experience Design & Fashion, R. Coll. of Art, London, United Kingdom (3) Electr. & Comput. Eng. & Bioeng., Univ. of Pittsburgh, Pittsburgh, PA, United States ",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE Robot. & Autom. Soc.,"IEEE, Piscataway, NJ, USA",,767-8,2019,,,,,,978-1-5386-8555-6,10.1109/HRI.2019.8673156,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,"Daegu, South Korea","In the future, Human-Robot Interaction should be enabled by a compact, human-centered and ergonomic wearable device that can merge human and machine altogether seamlessly by constantly identifying each other's intentions. In this paper, we will showcase the use of an ergonomic and lightweight wearable device that can identify human's eye/facial gestures with physiological signal measurements. Since human's intentions are usually coupled with eye movements and facial expressions, through proper design of interactions using these gestures, we can let people interact with the robots or smart home objects naturally. Combined with Computer Vision object recognition algorithms, we can allow people use very simple and straightforward communication strategies to operate telepresence robot and control smart home objects remotely, totally “Hands-Free”. People can wear a VR head-mounted display and see through the robot's eyes (the remote camera attached on the robot) and interact with the smart home devices intuitively by simple facial gestures or blink of the eyes. It is tremendous beneficial for the people with motor impairment as an assistive tool. For the normal people without disabilities, they can also free their hands to do other tasks and operate the smart home devices at the same time as multimodal control strategies.",1,,augmented reality - educational robots - ergonomics - face recognition - gesture recognition - handicapped aids - helmet mounted displays - home automation - human computer interaction - human-robot interaction - interactive devices - object recognition - robot vision - telerobotics - virtual reality - wearable computers,ergonomic wearable device - computer vision augmented intelligence - human-smart home object interaction - physiological signal measurements - facial expressions - robots - telepresence robot - VR head-mounted display - smart home devices - multimodal control strategies - human-robot interaction - object recognition algorithms - smart home objects - facial gestures - computer vision,B6135E Image recognition - C3365 Automated buildings - C3390T Telerobotics - C5260B Computer vision and image processing techniques - C5540B Interactive-input devices - C6130B Graphics techniques - C6130V Virtual reality - C6180 User interfaces - C7420 Control engineering computing,G05B15/00 - G06F3/00 - G06K9/00 - G06T,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Immersive media design and children,"Sobel, Kiley (1); Takeuchi, Lori (1); Castaneda, Lisa M. (2); Bindman, Samantha W. (2) ","(1) Joan Ganz Cooney Center at Sesame Workshop, New York; NY, United States (2) Foundry10, Seatle; WA, United States ","Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",Boise Osmo; Boise State University; et al.; Langan Barber Foundation; St. Luke's; STEM Action Center,"Association for Computing Machinery, Inc",,p 689-696,12-Jun-19,"Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",2019,,,,9.78145E+12,10.1145/3311927.3325163,,"18th ACM International Conference on Interaction Design and Children, IDC 2019","June 12, 2019 - June 15, 2019",,"This one-day workshop will bring together a community of researchers, designers, practitioners, and other experts who are interested in the responsible design of immersive media-or augmented, virtual, mixed, and cross reality-for children, while taking into account children's developmental needs, equity, and inclusivity. At the workshop, participants will discuss and make connections across their work, participate in hands-on activities, and begin producing a set of design guidelines for immersive media for children, grounded in their collective experiences, research, and knowledge of the field. Afer the workshop, we will publish these guidelines as a white paper to distribute to interested industries, developers, designers, and more to positively influence the thoughtful design of immersive media for children. © 2019 Association for Computing Machinery.",18,Mixed reality,Augmented reality - Design - Virtual reality,Child development - Children - Cross reality - Hands-on activities - Immersive media - White papers,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Visualizing the 'hidden' variables in robot programs,"Shepherd, David C. (1); Kraft, Nicholas A. (1); Francis, Patrick (1) ","(1) ABB Corporate Research, Raleigh; NC; 27606, United States ","Proceedings - 2019 IEEE/ACM 2nd International Workshop on Robotics Software Engineering, RoSE 2019",,Institute of Electrical and Electronics Engineers Inc.,,p 13-16,May-19,"Proceedings - 2019 IEEE/ACM 2nd International Workshop on Robotics Software Engineering, RoSE 2019",2019,,,,9.78173E+12,10.1109/RoSE.2019.00007,8823708,"2nd IEEE/ACM International Workshop on Robotics Software Engineering, RoSE 2019",27-May-19,,"Programs for one-armed industrial robots include many location-centric statements, such as 'Move to location AboveSurface.' Unfortunately, developers struggle to understand static location variables like AboveSurface, as mapping their seven-coordinate definitions to the real world is time consuming and error prone. Thus these locations, as well as the paths between them, which constitute the heart of any robot program, are effectively invisible to the programmer. To enable effective robot programming this core data must be visualized. There are two potential approaches to visualizing it. The first, visualizing locations one by one (e.g., by automatically moving the robot to each location) is limited to one location at a time. The second, visualizing locations in virtual reality can show all locations and paths at one time, but eliminates the ability to interact with the real world, which has many drawbacks. To avoid these drawbacks we propose using high-precision mixed reality to visualize program locations and paths, all while preserving the natural interaction with the real world workspace and robot. In this paper we demonstrate the feasibility of such an approach, sketch a solution, and discuss the advantages and disadvantages. © 2019 IEEE.",13,Robot programming,Augmented reality - Industrial robots - Location - Mixed reality - Robotics - Software engineering - Virtual reality,Core datum - Error prones - High-precision - Liveness - Natural interactions - Program comprehension - Real-world - Robot programs,"723 Computer Software, Data Handling and Applications - 723.1 Computer Programming - 731.5 Robotics - 731.6 Robot Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Engaging IT students through the NASA SUITS design challenge: An experience report,"Vincenti, Giovanni (1) ","(1) University of Baltimore, Baltimore; MD, United States ",SIGITE 2019 - Proceedings of the 20th Annual Conference on Information Technology Education,ACM Special Interest Group on Information Technology Education (SIGITE),"Association for Computing Machinery, Inc",,p 22-27,26-Sep-19,SIGITE 2019 - Proceedings of the 20th Annual Conference on Information Technology Education,2019,,,,9.78145E+12,10.1145/3349266.3351400,,"20th Annual Conference on Information Technology Education, SIGITE 2019","October 3, 2019 - October 5, 2019",,"Engaging students is at times challenging, especially when course assignments revolve around tedious work that is not applicable beyond the classroom. One way to prepare students who are about to graduate is the Capstone course, which often focuses on real-life problems that require a solution. This paper is an experience report of the integration of the NASA SUITS (Spacesuit User Interface Technologies for Students) Design Challenge into an IT Capstone course. The success of this initiative prompted a revision of projects associated with several prerequisite courses, with the aim of engaging students early on into a complex project that spans over multiple technological domains. © 2019 Association for Computing Machinery.",17,Students,Augmented reality - Curricula - NASA - User interfaces,Capstone - Design challenges - Engaging students - Experience report - Experiential learning - Interface technology - Project based learning - Real-life problems,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 901.2 Education",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Integrating humans in decentralized control systems with autonomous transport vehicles under the premise and use of proxemic distances,"Kirks, Thomas (1); Jost, Jana (1); Uhlott, Tim (1) ","(1) Fraunhofer IML, Joseph-v.-Fraunhofer Str. 2-4, Dortmund; 44227, Germany ",CEUR Workshop Proceedings,,CEUR-WS,v 2503,p 106-112,2019,"HCI Engineering 2019 - Joint Proceedings HCI Engineering 2019 - Methods and Tools for Advanced Interactive Systems and Integration of Multiple Stakeholder Viewpoints, co-located with 11th ACM SIGCHI Symposium on Engineering Interactive Computing Systems, EICS 2019",2019,,16130073,,,,,"2019 Joint HCI Engineering - Methods and Tools for Advanced Interactive Systems and Integration of Multiple Stakeholder Viewpoints, HCI Engineering 2019",18-Jun-19,,"In future warehouses and production facilities co-working areas of humans and autonomous vehicles will play a more important role. There, humans are confronted with no longer centralized controlled transport systems but decentralized ones, where humans can not easily comprehend the autonomous behavior of robots. Additionally, safety issues may interfere with process optimization. Therefore, new ways of interaction between humans and robots have to be investigated. This paper describes an experimental setup where humans and autonomous transport vehicles share the same working environment. Using methods from communication sciences and psychology an interactive way is introduced to mitigate the problem of process interference. The integration of humans in multi-agent systems of autonomous transport robots and the increased awareness using augmented reality for the human are central purpose of the experiment. Copyright © 2019 for this paper by its authors.",14,Human robot interaction,Augmented reality - Autonomous vehicles - Integration - Man machine systems - Multi agent systems - Optimization - User interfaces,Autonomous behaviors - Autonomous transport vehicles - Controlled transport - Human awareness - Interactive way - Production facility - Working areas - Working environment,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731.5 Robotics - 921.2 Calculus - 921.5 Optimization Techniques",,,"Number: 688117, Acronym: -, Sponsor: -; ",&#9733; This project has received funding from the European Union&rsquo;s Horizon 2020 research and in-novation programme under grant agreement No 688117 (SafeLog).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Imapper: Interaction-guided Scene Mapping from Monocular Videos,"Monszpart, Aron (2); Guerrero, Paul (2); Ceylan, Duygu (2); Yumer, Ersin (1); Mitra, Niloy J. (2) ","(1) Uber ATG, United States (2) Department of Computer Science, University College London, Adobe Research, United Kingdom ",ACM Transactions on Graphics,,Association for Computing Machinery,"v 38, n 4",,Jul-19,,2019,,7300301,15577368,,10.1145/3306346.3322961,92,,,,"Next generation smart and augmented reality systems demand a computational understanding of monocular footage that captures humans in physical spaces to reveal plausible object arrangements and human-object interactions. Despite recent advances, both in scene layout and human motion analysis, the above setting remains challenging to analyze due to regular occlusions that occur between objects and human motions. We observe that the interaction between object arrangements and human actions is often strongly correlated, and hence can be used to help recover from these occlusions. We present iMapper, a data-driven method to identify such human-object interactions and utilize them to infer layouts of occluded objects. Starting from a monocular video with detected 2D human joint positions that are potentially noisy and occluded, we first introduce the notion of interaction-saliency as space-time snapshots where informative human-object interactions happen. Then, we propose a global optimization to retrieve and fit interactions from a database to the detected salient interactions in order to best explain the input video. We extensively evaluate the approach, both quantitatively against manually annotated ground truth and through a user study, and demonstrate that iMapper produces plausible scene layouts for scenes with medium to heavy occlusion. Code and data are available on the project page. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",71,Augmented reality,Global optimization,Interaction - Monocular video - Occlusion - Scene layout - Shape analysis,"723 Computer Software, Data Handling and Applications - 921.5 Optimization Techniques",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Virtual reality passenger experiences,"McGill, Mark (1); Brewster, Stephen (1) ","(1) University of Glasgow, United Kingdom ","Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",cerence; Helmholtz Instituut; here; Rijkswaterstaat - Ministry of Infrastructure and Water Management; Uber ARG; Utrecht University - Faculty of Social and Behavioral Sciences,"Association for Computing Machinery, Inc",,p 434-441,21-Sep-19,"Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",2019,,,,9.78145E+12,10.1145/3349263.3351330,,"11th ACM International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019","September 21, 2019 - September 25, 2019",,"Our research aims to improve passenger journeys across both public and private transport, in cars, buses, planes and trains by utilizing Mixed Reality head-mounted (e.g. visual/auditory augmented and virtual reality) displays. This paper discusses our initial motivations and formative work in this area, both for in-car VR [33, 32] and for in-flight VR [42], and outlines some of the key challenges we anticipate in enabling mixed reality passenger experiences. © 2019 Copyright is held by the owner/author(s).",43,Mixed reality,Augmented reality - Transportation - User interfaces - Virtual reality,Augmented and virtual realities - Passenger journey - Passengers - Private transport,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: 835197, Acronym: -, Sponsor: -; ",This work is funded by the European Union&rsquo;s H2020 research and innovation programme (#835197: ViAjeRo).,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
SceneCam: Using AR to improve multi-camera remote collaboration,"Rasmussen, Troels A. (1); Huang, Weidong (2) ","(1) Department of Computer Science, Aarhus University, Aarhus, Denmark (2) University of Technology Sydney, Sydney, Australia ","SIGGRAPH Asia 2019 XR, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 36-37,17-Nov-19,"SIGGRAPH Asia 2019 XR, SA 2019",2019,,,,9.78145E+12,10.1145/3355355.3361892,,"SIGGRAPH Asia 2019 XR - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"During multi-camera remote collaboration on physical tasks, as the name implies, multiple cameras capture different areas and perspectives of a task space. It can be challenging for the remote user to obtain the right view of the local user and to understand the spatial relationship between the disjointed views of task space areas. We present SceneCam, a prototype with which we use AR to explore different techniques for improving multi-camera remote collaboration by making optimal camera selection easier and faster for the remote user and by making the spatial relationship between task space areas explicit. To this end, SceneCam implements two camera selection techniques: nudging the remote user to select an optimal camera view of the local user, and automatic selection of an optimal camera view of the local user. Furthermore, SceneCam provides the remote user with two focus-in-context views - exocentric and egocentric views - that visualize the spatial relationship between the multiple task space areas and the local user. © 2019 Association for Computing Machinery.",2,Cameras,Augmented reality - Interactive computer graphics,Automatic selection - Camera selection - Focus-in-context - Multi-cameras - Multiple cameras - Multiple tasks - Remote collaboration - Spatial relationships,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 742.2 Photographic Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
3D printing for mixed reality hands-on museum exhibit interaction,"Mann, Laura (1); Fryazinov, Oleg (1) ","(1) NCCA, Bournemouth University, United Kingdom ","ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3306214.3338609,,"ACM SIGGRAPH 2019 Posters - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"This work presents a combination of 3D printing with mixed reality to use the results in the context of museum exhibitions or for cultural heritage. While now priceless artefacts are encased in glass, kept safe and out of reach of the visitors, we present a new pipeline which would allow visitors hands-on interaction with realistic 3D printed replicas of the artefacts which are then digitally augmented to have the genuine artefacts' appearances. © 2019 Copyright held by the owner/author(s).",3,3D printers,Augmented reality - Exhibitions - Interactive computer graphics - Mixed reality - Museums,3-D printing - Cultural heritages - Museum exhibits,"402.2 Public Buildings - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 745.1.1 Printing Equipment",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The future of shared experiences - XR is a lonely world,"Sisto, Aaron (1); Detweiler, Jameson (2); Luo, Victor (3); Mani, Varun (4); Agarawala, Anand (5); Mine, Mark (6); Grossman, Ben (7) ","(1) VentureX, Washington; D.C., United States (2) Fantasmo, Venice; CA, United States (3) NASA Jet Propulsion Laboratory, Los Angeles; CA, United States (4) PTC, Somerville; MA, United States (5) Spatial, New York; NY, United States (6) Disney Imagineering, Los Angeles; CA, United States (7) Magnopus, Los Angeles; CA, United States ","ACM SIGGRAPH 2019 Panels, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Panels, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3306212.3328125,,"ACM SIGGRAPH 2019 Panels - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"XR has never been more immersive, entertaining and dynamic than it is now. So why is the virtual world still such a lonely place? Shared virtual experiences are slowing becoming a reality, with the potential to transform the way XR is used in education, entertainment, telepresence and the enterprise. But will technology and content finally coevolve to support realistic, real-time, multi-person interactions? This panel explores the expanding dimensions of shared experiences in XR, offering views on emerging trends, applications and breakthrough technologies. © Copyright is held by the owner/author(s).",,Interactive computer graphics,Augmented reality - Virtual reality - Visual communication,Breakthrough technology - Emerging trends - Extended reality - Immersive - Real time - Shared experiences - Telepresence - Virtual worlds,"717.1 Optical Communication Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Cake cam: Take your photo and be in it too,"Lusk, Candice (1); Jones, Michael D. (1) ","(1) Brigham Young University, Provo; UT, United States ","Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019",ACM Special Interest Group on Computer-Human Interaction (SIGCHI),"Association for Computing Machinery, Inc",,,1-Oct-19,"Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019",2019,,,,9.78145E+12,10.1145/3338286.3340123,a12,"21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019","October 1, 2019 - October 4, 2019",,"Tourists often turn to strangers when they need a photographer while traveling; however, they do so at a cost. Strangers are not typically trained photographers, nor are they telepathically intuiting what composition the tourist wants. Existing smartphone camera interfaces do not communicate the desired framing to the stranger, and prior work in mobile photography guidance does not manage the 3D movement required when composing the tourist's ideal photo. We ofer a new kind of mobile interaction for communicating the intended photo to a stranger without instructions. In our methodology, the tourist frst composes a photo with the desired framing. The app, Cake Cam, then stores the camera position and orientation. Finally, 3D augmented reality markers guide the stranger to retake the photo with the tourist now standing in the frame. Our study resulted in more accurate camera placements and required fewer additional instructions than the traditional tourist photography method (n=40). © 2019 Association for Computing Machinery.",21,Human computer interaction,Augmented reality - Cameras - Cams - Photography,Camera placement - Camera positions - Mobile interaction - Mobile photographies - Smart-phone cameras,"601.3 Mechanisms - 723 Computer Software, Data Handling and Applications - 742.2 Photographic Equipment - 746 Imaging Techniques",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Smart discovery of cultural and natural tourist routes,"Stathopoulos, Evangelos A. (1); Paliokas, Ioannis (1); Meditskos, Georgios (1); Diplaris, Sotiris (1); Pehlivanides, George (2); Tsafaras, Spyridon (2); Valkouma, Efthalia (3); Riggas, Christodoulos (4); Vrochidis, Stefanos (1); Votis, Konstantinos (1); Tzovaras, Dimitrios (1); Kompatsiaris, Ioannis (1) ","(1) Information Technologies Institute, Centre for Research and Technology, Hellas, Thessaloniki, Greece (2) Tetragon S.A., Thessaloniki, Greece (3) Egnatia Odos S.A., Thessaloniki, Greece (4) Piraeus Bank Group, Cultural Foundation, Athens, Greece ","Proceedings - 2019 IEEE/WIC/ACM International Conference on Web Intelligence Workshops, WI 2019 Companion",,"Association for Computing Machinery, Inc",,p 208-214,14-Oct-19,"Proceedings - 2019 IEEE/WIC/ACM International Conference on Web Intelligence Workshops, WI 2019 Companion",2019,,,,9.78145E+12,10.1145/3358695.3361105,,"19th IEEE/WIC/ACM International Conference on Web Intelligence Workshop, WI 2019","October 14, 2019 - October 17, 2019",,"This paper presents a system designed to utilize innovative spatial interconnection technologies for sites and events of environmental, cultural and tourist interests. The system will discover and consolidate semantic information from multiple sources, providing the end-user the ability to organize and implement integrated and enhanced tours. The system, called e-xnilatis, will extend existing innovative techniques, including: semi-automated searching and extraction of real-time knowledge from online resources, automated discovery of points of interest as well as events, semantic integration, classification and hierarchy of information from various sources, spatial representation of content, personalized user experience and Augmented Reality (AR) for the interconnection of the digital with the natural environment. A complex online platform and applications for smart devices will be developed so that the user manages and receives the optimum route information along with AR experience when applicable. The platform will be an open architecture tool that, with the appropriate time-space constraints, will be able to create adaptive content. A typical example of this is the Egnatia motorway, where the e-xnilatis platform will be evaluated. © 2019 Association for Computing Machinery.",35,Search engines,Augmented reality - Automation - Classification (of information) - Environmental technology - Semantics,Innovative techniques - Interconnection technology - Reasoning - Semantic information - Semantic integration - Spatial representations - Time space constraints - User experience,"454 Environmental Engineering - 716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications - 731 Automatic Control Principles and Applications",,,"Number: T1EDK-00410, Acronym: -, Sponsor: -; ","This research has been co-financed by the European Union and Greek national funds through the Operational Program Competitiveness, Entrepreneurship and Innovation, under the call RESEARCH-CREATE-INNOVATE (project code: T1EDK-00410).",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Innervate immersion: Case study of dynamic simulations in AR/VR environments for learning muscular innervation,"Cook, Margaret (1); Payne, Austin (1); Ackley, Amber (1); Seo, Jinsil Hwaryoung (1); Gonzalez, Karla Chang (1); Kicklighter, Caleb (1); Pine, Michelle (1); McLaughlin, Timothy (1) ","(1) Texas A and M University College Station, TX, United States ","ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3306214.3338580,,"ACM SIGGRAPH 2019 Posters - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"We present a collaborative immersive technology effort, InNervate AR and InNervate VR. These applications meet the need to expand on existing anatomy education platforms by implementing a more dynamic and interactive user interface. This user interface allows for exploration of the complex relationship between motor nerve deficits and their effects upon the canine anatomy's ability to produce movement. Preliminary AR user studies provided us with positive feedback in the quality of learning. The studies show that the dynamic touch interactions in AR definitely benefit students' critical reasoning and spatial visualization in learning motor nerve and muscle relationships. However, users seek a more immersive VR-based learning environment, without the distractions that an AR experience may offer. Based on this feedback, a VR version of this learning experience was created. Preliminary responses show that users are satisfied with this VR environment which allows them to manipulate and control the anatomical content with full-body interactions. © 2019 Copyright held by the owner/author(s).",5,User interfaces,Augmented reality - Computer aided instruction - Feedback - Interactive computer graphics - Virtual reality,Anatomy educations - Complex relationships - Full-body interaction - Immersive technologies - Interactive user interfaces - Learning environments - Learning experiences - Spatial visualization,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 731.1 Control Systems",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
AuraRing: Precise Electromagnetic Finger Tracking,"Parizi, F.S. (1); Whitmire, E. (1); Patel, S. (1) ","(1) Univ. of Washington, Seattle, WA, United States ","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",,"ACM, USA","v 3, n 4",150 (28 pp.),Dec. 2019,,,,2474-9567,,,10.1145/3369831,,,,,"Wearable computing platforms, such as smartwatches and head-mounted mixed reality displays, demand new input devices for high-fidelity interaction. We present AuraRing, a wearable magnetic tracking system designed for tracking fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the ring. We develop two different approaches to pose reconstruction - a first-principles iterative approach and a closed-form neural network approach. Notably, AuraRing requires no runtime supervised training, ensuring user and session independence. AuraRing has a resolution of 0.1 mm and a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power.",,,augmented reality - coils - image reconstruction - magnetic sensors - neural nets - object tracking - pose estimation - wearable computers,AuraRing - wearable computing platforms - head-mounted mixed reality displays - high-fidelity interaction - wearable magnetic tracking system - fine-grained finger movement - embedded electromagnetic transmitter coil - multiple sensor coils - closed-form neural network approach - electromagnetic finger tracking - pose reconstruction,"B6135 Optical, image and video signal processing - B7230 Sensing devices and transducers - C5260B Computer vision and image processing techniques - C5290 Neural computing techniques - C5430 Microcomputers - C6130V Virtual reality",G06F15/00 - G06K9/00 - G06T - H01F5/00 - H01F27/28,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Interacting in mixed reality,"Chrysanthou, Antrea (1); Kleanthous, Styliani (1); Matsi, Elena (1) ","(1) University of Cyprus, Aglantzia, Nicosia, Cyprus ","International Conference on Intelligent User Interfaces, Proceedings IUI",ACM Special Interest Group on Artificial Intelligence (SIGAI); ACM Special Interest Group on Computer-Human Interaction (SIGCHI),Association for Computing Machinery,,p 347-351,17-Mar-20,"Proceedings of the 25th International Conference on Intelligent User Interfaces, IUI 2020",2020,,,,9.78145E+12,10.1145/3377325.3377532,,"25th ACM International Conference on Intelligent User Interfaces, IUI 2020","March 17, 2020 - March 20, 2020",,"With the development of intelligent interfaces and emerging technologies children and adult users are provided with exciting interaction approaches in several applications. Holographic applications were until recently only available to few people, mostly experts and researchers. In this work, we are investigating the differences between children and adult users towards their interaction behavior in mixed reality, when they were asked to perform a task. Analysis of the results demonstrates that children can be more efficient during their interaction in these environments while adults are more confident and their experience and knowledge is an advantage in achieving a task. © ACM.",11,Mixed reality,Augmented reality - User interfaces,Computer interaction - Emerging technologies - Gesture interaction - Holographic applications - Intelligent interface - Interaction behavior,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Enhancing Augmented VR Interaction via Egocentric Scene Analysis,Yang Tian (1); Chi-Wing Fu (2); Shengdong Zhao (3); Ruihui Li (1); Xiao Tang (1); Xiaowei Hu (1); Pheng-Ann Heng (1) ,"(1) Chinese Univ. of Hong Kong, Hong Kong, China (2) Key Lab. of Comput. Vision & Virtual Reality Technol., Chinese Univ. of Hong Kong, Shenzhen, China (3) Nat. Univ. of Singapore, Singapore, Singapore ","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",,"ACM, USA","v 3, n 3",105 (24 pp.),Sept. 2019,,,,2474-9567,,,10.1145/3351263,,,,,"Augmented virtual reality (AVR) takes portions of the physical world into the VR world to enable VR users to access physical objects. State-of-the-art solutions mainly focus on extracting and showing physical objects in the VR world. In this work, we go beyond previous solutions and propose a novel approach to realize AVR. We first analyze the physical environment in the user's egocentric view through depth sensing and deep learning, then acquire the layout and geometry of the surrounding objects, and further explore their affordances. Based on the above information, we create visual guidance (hollowed guiding path) and hybrid user interfaces (augmented physical notepad, LR finger slider, and LRRL finger slider) to augment the AVR interaction. Empirical evaluations showed that the participants responded positively to our AVR techniques.",,,augmented reality - user interfaces - virtual reality,AVR techniques - augmented VR interaction - egocentric scene - augmented virtual reality - physical world - VR world - VR users - physical objects - physical environment - user - surrounding objects - physical notepad - AVR interaction,C6130V Virtual reality - C6180 User interfaces - C5260B Computer vision and image processing techniques - C6130B Graphics techniques,G06T,General or Review (GEN); Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
IRIS: Inter-reality intera,"Pietroszek, Krzysztof (1) ","(1) IDEAS Lab, American University, Washington; DC, United States ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364731,3364731,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"While many metaphors were developed for interactions from a specific point at the reality-virtuality continuum, much less attention has been paid to designing metaphors that allow the users to cross the boundaries between the virtual, the augmented, and the real. We propose a use of an Inter-Reality Interactive Surface (IRIS) that enables users to collaborate across the reality-virtuality continuum within the same application. While we examine IRIS in the context of an immersive educational platform, UniVResity, the metaphor can be generalized to many other application domains. © 2019 Copyright held by the owner/author(s).",9,Virtual reality,Augmented reality,Collaboration - Educational platforms - Immersive - Immersive learning - Interaction metaphors - Interactive surfaces - Virtuality continuum,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Extended reality for chronic pain relief,"Wang, Jiaheng (1); Anslow, Craig (1); Robinson, Brian (1); McCallum, Simon (1) ","(1) Victoria University of Wellington, Wellington, New Zealand ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3365030,3365030,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Chronic pain is ongoing pain lasting for long periods of time after the initial injury or disease has healed. Chronic pain is difficult to treat and can affect the daily lives of patients. Distraction therapy is a proven way of relieving pain by redirecting the focus of patients' attention. Virtual reality is an effective platform for distraction therapy as it immerses the user visually, aurally, and even somewhat physically in a virtual world detached from reality. There is little research done on the effects that physical interactions have on pain management. This project aims to evaluate different types of extended reality (XR) interactions, including full body movement, for chronic pain patients to determine which is the best for pain relief. We are building a prototype for participants to interact both mentally and physically and measuring the reduction in subjective pain ratings at various points of the XR experience. © 2019 Copyright held by the owner/author(s).",11,Mixed reality,Augmented reality - Interactive computer graphics - Patient treatment - Virtual reality,Building a prototypes - Chronic pain - Full-body movement - Pain management - Pain relief - Physical interactions - User study - Virtual worlds,"461.6 Medicine and Pharmacology - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Approaches for immersive media curriculum implementation,"Jushchyshyn, Nick (1); McLaughlin, Timothy (2); Woolverton, Morgan (3) ","(1) Digital Media Department, Drexel University, Philadelphia; PA, United States (2) Department of Visualization, Texas A and M University, College Station; TX, United States (3) Game Art and Virtual Reality Development Department, Ringling College of Art + Design, Sarasota; FL, United States ","ACM SIGGRAPH 2019 Educators Forum, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Educators Forum, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3326542.3328014,a1,"ACM SIGGRAPH 2019 Educators Forum - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"The rapid expansion of availability, affordability and implementation of immersive media technologies in the market place has spawned an increasing interest in integration of design and production methodologies into higher education curriculum, while simultaneously posing a host of challenges to educators when with regards to resources, pedagogy and identifying industry needs. This panel brings together the directors of programs from Texas A&M University, The Ringling College of Art + Design, and Drexel University who have integrated immersive media into existing programs in visualization, as well as launched entire degree programs focused on the teaching design and production methodologies, to share and discuss their experiences with fellow. © 2019 Copyright held by the owner/author(s).",,Curricula,Augmented reality - Education - Interactive computer graphics - Virtual reality,Curricular development - Curriculum implementation - Degree program - Drexel University - Higher education - Immersive media - Rapid expansion - Teaching designs,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 901.2 Education",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Fostering positive affects in software development environments using extended reality,"Mehra, Rohit (1); Sharma, Vibhu Saujanya (1); Kaulgud, Vikrant (1); Podder, Sanjay (1) ","(1) Accenture Labs, Bangalore, India ","Proceedings - 2019 IEEE/ACM 4th International Workshop on Emotion Awareness in Software Engineering, SEmotion 2019",,Institute of Electrical and Electronics Engineers Inc.,,p 42-45,May-19,"Proceedings - 2019 IEEE/ACM 4th International Workshop on Emotion Awareness in Software Engineering, SEmotion 2019",2019,,,,9.78173E+12,10.1109/SEmotion.2019.00016,8825018,"4th IEEE/ACM International Workshop on Emotion Awareness in Software Engineering, SEmotion 2019","May 28, 2019 - May 28, 2019",,"Recent research on affects has been aimed at establishing the hypothesis that a developer's productivity has a positive correlation with positive affects, such as her happiness quotient. Moreover, studies show that the software development environment itself has a huge role to play in how a developer feels during her day to day activities. In this paper, we present our early work regarding an Extended Reality based approach that virtually transforms a developer's extended and immediate environment, in a way that induces positive affects. This is aimed at assisting the developer to perform better during development related activities like programming and debugging. Salient features of the approach include a sense of presence, uniqueness and easy customization. As part of our initial research efforts, we showcase a couple of Augmented Reality based prototype implementations capable of augmenting virtual and interactive pets and scenic content in the real world. Finally, we also discuss some important future directions that we are investigating further as part of our research. © 2019 IEEE.",26,Software design,Augmented reality,Affect - Extended Reality - Happiness - Immediate environment - Positive correlations - Prototype implementations - Sense of presences - Software development environment,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Multi-modal high-end visualization system,"Bourke, Conan (1); Bednarz, Tomasz (1) ","(1) EPICentre, UNSW Art and Design, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365731,a65,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"This paper describes a production-grade software toolkit used for shared multi-model visualization systems developed by the Expanded Perception and Interaction Centre. Our High-End Visualization System (HEVS) can be used as a framework to enable content to be run transparently on a wider range of platforms (Figure 2) with fewer compatibility issues and dependencies on commercial software. Content can be transferred more easily from large screens (including cluster-driven systems) such as CAVE-like platforms, hemispherical domes, and projected cylindrical displays through to multi-wall displays and HMDs such as VRR or AR. This common framework is able to provide a unifying approach to visual analytics and visualizations. In addition to supporting multi-modal displays, multiple platforms can be connected to create multi-user collaborative experiences across remotely located labs. We aim to demonstrate multiple projects developed with HEVS that have been deployed to various multi-modal display devices. © 2019 Association for Computing Machinery.",5,Visualization,Augmented reality - Display devices - Flow visualization - Interactive computer graphics - Virtual reality,Commercial software - Cylindrical displays - High-Performance Graphics - Multi-Modal Displays - Multiple platforms - Software toolkits - Visual analytics - Visualization system,"631.1 Fluid Flow, General - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Exploring the concept of the (future) mobile office,"Janssen, Christian P. (1); Boyle, Linda Ng (2); Kun, Andrew L. (3); Brumby, Duncan P. (4); Brewster, Stephen (5); Chuang, Lewis L. (6) ","(1) Utrecht University, Heidelberglaan 1, Utrecht; 3584 CS, Netherlands (2) University of Washington, Seattle; WA; 98195-2650, United States (3) University of New Hampshire, Electrical and Computer Eng., Durham; NH; 03824, United States (4) UCL Interaction Centre, University College London, London, United Kingdom (5) University of Glasgow, Glasgow; G12 8RZ, United Kingdom (6) Ludwig-Maximilians-Universität München, Frauenlobstr. 7a, München; 80337, Germany ","Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",cerence; Helmholtz Instituut; here; Rijkswaterstaat - Ministry of Infrastructure and Water Management; Uber ARG; Utrecht University - Faculty of Social and Behavioral Sciences,"Association for Computing Machinery, Inc",,p 465-467,21-Sep-19,"Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",2019,,,,9.78145E+12,10.1145/3349263.3349600,,"11th ACM International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019","September 21, 2019 - September 25, 2019",,"This video shows a concept of a future mobile office in a semi-automated vehicle that uses augmented reality. People perform non-driving tasks in current, non-automated vehicles even though that is unsafe. Moreover, even for passengers there is limited space, it is not social, and there can be motion sickness. In future cars, technology such as augmented reality might alleviate some of these issues. Our concept shows how augmented reality can project a remote conversant onto the dashboard. Thereby, the driver can keep an occasional eye on the road while the automated vehicle drives, and might experience less motion sickness. Potentially, this concept might even be used for group calls or for group activities such as karaoke, thereby creating a social setting. We also demonstrate how integration with an intelligent assistant (through speech and gesture analysis) might save the driver from having to grab a calendar to write things down, again allowing them to focus on the road. © 2019 Copyright is held by the owner/author(s).",10,User interfaces,Augmented reality - Automation - Diseases - Roads and streets - Vehicles,Automated driving - Automated vehicles - Driver distractions - Gesture analysis - Group activities - Intelligent assistants - Mobile offices - Social settings,"406.2 Roads and Streets - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731 Automatic Control Principles and Applications",,,"Number: CMMI-1840085, Acronym: NSF, Sponsor: National Science Foundation; ","This video idea was generated at the seminar on Users and automated driving systems: How will we interact with tomorrow&rsquo;s vehicles?"" held at Schloss Dagstuhl", Germany (Seminar number 19132). We gratefully acknowledge the organizers of the seminar, Schloss Daghstul," and the other members of the seminar for their contributions. Andrew Kun and Linda Boyle were in part supported by NSF grant CMMI-1840085. The funding organization had no involvement in the nature or design of this research.""",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village
Obstacle detection and alert system for smartphone AR users,"Kang, Hyeongyeop (1); Lee, Geonsun (2); Han, Junghyun (2) ","(1) Kangwon National University, Samcheok, Korea, Republic of (2) Korea University, Seoul, Korea, Republic of ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364256,3364256,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"This paper presents an obstacle detection and alert system for the pedestrians who use smartphone AR applications. The system analyzes the input camera image to extract feature points and determines whether the feature points come from obstacles ahead in the path. With the obstacle detector, two experiments were made. The first investigated the obstacle alert interfaces, and the second investigated the orientation guide interfaces that instruct users to hold their smartphones with some angles/orientations appropriate to capture the environment. Then, the best interfaces identified from the experiments were integrated and tested to examine their usability and user experiences. © 2019 Association for Computing Machinery.",39,Obstacle detectors,Augmented reality - Pedestrian safety - Smartphones - Virtual reality,Alert systems - AR application - Camera images - Obstacle detection - User experience,"406.2 Roads and Streets - 718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: MSIT, Sponsor: Ministry of Science and ICT, South Korea; Number: -, Acronym: NRF, Sponsor: National Research Foundation of Korea; ",This work was supported by the National Research Foundation of Korea (NRF) Grant funded by the Korea government (MSIT) (No. NRF-2017M3C4A7066316 and No. NRF-2016R1A2B3014319). The preliminary version of this work was presented as a poster at ISMAR 2019.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A design for optical cloaking display,"Aoto, Takahito (1); Itoh, Yuta (2); Otao, Kazuki (2); Takazawa, Kazuki (2); Ochiai, Yoichi (2) ","(1) University of Tsukuba, Japan (2) University of Tsukuba, Pixie Dust Technologies, Inc., Japan ","ACM SIGGRAPH 2019 Emerging Technologies, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Emerging Technologies, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3305367.3327979,,"ACM SIGGRAPH 2019 Emerging Technologies - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"In the graphics research context, optical cloaking that hides an object by relaying a light field is also a kind of display. Despite much interest in the cloaking, large-size cloaking has not been achieved without some limitations and/or assumptions. To solve this problem, a computational design method is proposed for an optical cloaking display via viewpoint transformation. The method uses two kinds of passive optical elements that transfer a point into a plane symmetric point. In the experiments, a novel passive display was developed that optically cloaks the object and transmits a light field properly. Experimental results for the multiviewpoint scene that was captured are presented. © 2019 Copyright held by the owner/author(s).",6,Field emission displays,Augmented reality - Design - Interactive computer graphics - Metamaterials,A-plane - Computational design methods - Invisible cloaks - Light field displays - Light fields - Optical cloaking - Passive displays - Viewpoint transformation,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 951 Materials Science",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"LUI: A multimodal, intelligent interface for large displays","Parthiban, Vik (1); Lee, Ashley Jieun (1) ","(1) Massachusetts Institute of Technology, United States ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365743,a48,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"On large screen displays, using conventional keyboard and mouse input is difficult because small mouse movements often do not scale well with the size of the display and individual elements on screen. We propose LUI, or Large User Interface, which increases the range of dynamic surface area of interactions possible on such a display. Our model leverages real-time continuous feedback of freehanded gestures and voice to control extensible applications such as photos, videos, and 3D models. Utilizing a single stereo-camera and voice assistant, LUI does not require exhaustive calibration or a multitude of sensors to operate, and it can be easily installed and deployed on any large screen surfaces. In a user study, participants found LUI efficient and easily learnable with minimal instruction, and preferred it to more conventional interfaces. This multimodal interface can also be deployed in augmented or virtual reality spaces and autonomous vehicle displays. © 2019 Association for Computing Machinery.",4,Mammals,Augmented reality - Human computer interaction - Interactive computer graphics - Speech - Stereo image processing - Three dimensional computer graphics - User interfaces - Virtual reality,Dynamic surface - Gestures - Intelligent interface - Large displays - Large screen displays - Mouse movements - Multi-modal interfaces - Stereo cameras,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 751.5 Speech",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
AHMED: Toolset for Ad-Hoc mixed-reality exhibition design,"Pietroszek, Krzysztof (1); Moore, Carl (1) ","(1) IDEAS Lab, American University, Washington; DC, United States ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364729,3364729,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"We present 'AHMED', a mixed-reality toolset that allows visitors to experience mixed-reality museum or art exhibitions created ad-hoc at locations such as event venues, private parties,or a living room. The system democratizes access to exhibitions for populations that cannot visit these exhibitions in person for reasons of disability, time-constraints, travel restrictions, or socio-economic status. © 2019 Copyright held by the owner/author(s).",9,Mixed reality,Augmented reality - Economics - Exhibitions - Museums - Photogrammetry,Heritage - Living room - Socio-economic status - Time constraints - Toolsets - Travel restrictions - Volumetric capture,"402.2 Public Buildings - 405.3 Surveying - 723 Computer Software, Data Handling and Applications - 971 Social Sciences",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Occlusion-aware hand posture based interaction on tabletop projector,"Fujinawa, Eisuke (1); Goto, Kenji (1); Irie, Atsushi (1); Wu, Songtao (2); Xu, Kuanhong (2) ","(1) Sony Corporation, Tokyo, Japan (2) Sony China Research Laboratory, Beijing, China ",UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 113-115,14-Oct-19,UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332167.3356890,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Conventional camera-based hand interaction technique suffered from self-occlusion among fingers, which lowers the detection accuracy of fingertip positions, leading to uncomfortable UI controls. Based on observations, self-occlusion depends on hand postures. We design an interaction framework in which interaction is decided in response to a recognized hand posture. Using a tabletop projection system that has a projector and a depth sensor, we implement the framework by integrating five touch and in-air interactions that balance its stability and usability. © 2019 Copyright held by the owner/author(s).",10,User interfaces,Augmented reality,Depth sensing - Hand gesture - Interaction techniques - Occlusion - Touch,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Aire - Visualize air quality,"Torres, Natalia Garcia (1); Campbell, Paulina Escalante (1) ","(1) Quimera Verde A.C., Monterrey, Mexico ","ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3305365.3329869,a1,"ACM SIGGRAPH 2019 Appy Hour - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"An interactive and immersive AR experience, Aire enables anyone to learn about air pollution and the contaminants present in their environment. We leverage the use of information and technologies already available and provide a way to visualize complex scientific concepts concerning air pollution. Learning is the first step to making the world a better place. © 2019 Copyright Held by the Owner/Author(s).",1,Interactive computer graphics,Air pollution - Air quality - Augmented reality - Education - Information use,Better places - Environmental awareness - Environmental issues - Immersive - Information and technologies,"451 Air Pollution - 451.2 Air Pollution Control - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 903.3 Information Retrieval and Use",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ActiTouch: Robust touch detection for on-skin AR/VR interfaces,"Zhang, Yang (1); Kienzle, Wolf (2); Ma, Yanjun (2); Ng, Shiu S. (2); Benko, Hrvoje (2); Harrison, Chris (1) ","(1) Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh; PA; 15213, United States (2) Facebook Reality Labs, 9845 Willows Rd., Redmond; WA; 98052, United States ",UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 1151-1159,17-Oct-19,UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332165.3347869,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Contemporary AR/VR systems use in-air gestures or handheld controllers for interactivity. This overlooks the skin as a convenient surface for tactile, touch-driven interactions, which are generally more accurate and comfortable than free space interactions. In response, we developed ActiTouch, a new electrical method that enables precise on-skin touch segmentation by using the body as an RF waveguide. We combine this method with computer vision, enabling a system with both high tracking precision and robust touch detection. Our system requires no cumbersome instrumentation of the fingers or hands, requiring only a single wristband (e.g., smartwatch) and sensors integrated into an AR/VR headset. We quantify the accuracy of our approach through a user study and demonstrate how it can enable touchscreen-like interactions on the skin. © 2019 Association for Computing Machinery.",40,User interfaces,Augmented reality - Virtual reality,Electrical methods - Free-space interactions - Handheld controllers - Interactivity - Touch interaction - Tracking precision - User study,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
MeCap: Whole-body digitization for low-cost VR/AR headsets,"Ahuja, Karan (1); Harrison, Chris (1); Goel, Mayank (1); Xiao, Robert (1) ","(1) Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh; PA; 15213, United States ",UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 453-462,17-Oct-19,UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332165.3347889,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Low-cost, smartphone-powered VR/AR headsets are becoming more popular. These basic devices - little more than plastic or cardboard shells - lack advanced features, such as controllers for the hands, limiting their interactive capability. Moreover, even high-end consumer headsets lack the ability to track the body and face. For this reason, interactive experiences like social VR are underdeveloped. We introduce MeCap, which enables commodity VR headsets to be augmented with powerful motion capture ('MoCap') and user-sensing capabilities at very low cost (under $5). Using only a pair of hemi-spherical mirrors and the existing rear-facing camera of a smartphone, MeCap provides real-time estimates of a wearer's 3D body pose, hand pose, facial expression, physical appearance and surrounding environment - capabilities which are either absent in contemporary VR/AR systems or which require specialized hardware and controllers. We evaluate the accuracy of each of our tracking features, the results of which show imminent feasibility. © 2019 Association of Computing Machinery.",57,User interfaces,Augmented reality - Costs - Real time systems - Smartphones - Virtual reality,Facial Expressions - Hand gesture - Headset - Motion capture - On-body - Specialized hardware - Surrounding environment - Tracking feature,"718.1 Telephone Systems and Equipment - 722.2 Computer Peripheral Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 911 Cost and Value Engineering; Industrial Economics",,,,We are grateful to the Packard and Sloan Foundations for supporting research in the Future Interfaces Group.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Mixed-reality exhibition for museum of peace corps experiences using AHMED toolset,"Pietroszek, Krzysztof (1) ","(1) Immersive Designs, Experiences, Applications and Stories (IDEAS) Lab, American University ",Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,10.1145/3357251.3358754,a24,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,"We present a mixed-reality exhibition for the Museum of Peace Corps Experiences designed using the Ad-Hoc Mixed-reality Exhibition Designer (AHMED) toolset. AHMED enables visitors to experience mixed-reality museum or art exhibitions created ad-hoc at any location. The system democratizes access to exhibitions for populations that cannot visit these exhibitions in person for reasons of disability, time-constraints, travel restrictions, or socio-economic status. © 2019 Association for Computing Machinery.",7,Exhibitions,Augmented reality - Economics - Mixed reality - Museums - Photogrammetry,Heritage - Socio-economic status - Time constraints - Toolsets - Travel restrictions - Volumetric capture,"402.2 Public Buildings - 405.3 Surveying - 723 Computer Software, Data Handling and Applications - 971 Social Sciences",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
LabelAR: A spatial guidance interface for fast computer vision image collection,"Laielli, Michael (1); Smith, James (1); Biamby, Giscard (1); Darrell, Trevor (1); Hartmann, Bjoern (1) ","(1) UC Berkeley, EECS, Berkeley; CA, United States ",UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 987-998,17-Oct-19,UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332165.3347927,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Computer vision is applied in an ever expanding range of applications, many of which require custom training data to perform well. We present a novel interface for rapid collection of labeled training images to improve CV-based object detectors. LabelAR leverages the spatial tracking capabilities of an AR-enabled camera, allowing users to place persistent bounding volumes that stay centered on real-world objects. The interface then guides the user to move the camera to cover a wide variety of viewpoints. We eliminate the need for post hoc labeling of images by automatically projecting 2D bounding boxes around objects in the images as they are captured from AR-marked viewpoints. In a user study with 12 participants, LabelAR significantly outperforms existing approaches in terms of the trade-off between detection performance and collection time. © 2019 Copyright is held by the owner/author(s).",39,Computer vision,Augmented reality - Cameras - Economic and social effects - Image enhancement - Object detection - User interfaces,Bounding volume - Collection time - Detection performance - Guidance interfaces - Image collections - Object detectors - Real-world objects - Spatial tracking,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 742.2 Photographic Equipment - 971 Social Sciences",,,"Number: -, Acronym: NGA, Sponsor: National Geospatial-Intelligence Agency; ",This research was supported through the National Geospatial-Intelligence Agency (NGA) Vector program. Approved for public release under case 19-678.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Towards dynamic positioning of text content on a windshield display for automated driving,"Riegler, Andreas (1); Riener, Andreas (2); Holzmann, Clemens (1) ","(1) University of Applied Sciences Upper Austria, Hagenberg, Austria (2) Technische Hochschule Ingolstadt, Ingolstadt, Germany ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364757,3364757,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Windshield displays (WSDs) are a promising new technology to augment the entire windscreen with additional information about vehicle state, highlight critical objects in the surrounding, or serve as replacement for conventional displays. Typically, augmentation is provided in a screen-fixed manner as overlay on the windscreen. However, it is unclear to date if this is optimal in terms of usability/ UX. In this work, we propose 'StickyWSD' - a world-fixed positioning strategy - and evaluate its impact on quantitative measures compared to screen-fixed positioning. Results from a user study conducted in a virtual reality driving simulator (N = 23) suggest that the dynamic world-fixed positioning technique shows increased performance and lowered error rates as well as take-over times. We conclude that the 'StickyWSD' approach offers lot of potential for WSDs that should be researched further. © 2019 Copyright held by the owner/author(s).",8,Windshields,Augmented reality - Virtual reality,Automated driving - Driving simulator - Dynamic world - Focus distance - Positioning techniques - Quantitative measures - User study - Windshield displays,"723 Computer Software, Data Handling and Applications",,,,This work was supported by the University of Applied Sciences PhD program of the government of Upper Austria.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Virtual window manipulation method for head-mounted display using smart device,"Sorimachi, Shu (1); Kita, Kota (1); Matsushita, Mitsunori (1) ","(1) Kansai University, Osaka, Japan ",Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,10.1145/3357251.3358753,a19,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,"In this paper, we propose a virtual window manipulation method used for information search while utilizing a head-mounted display (HMD). Existing HMD operation methods have several issues like causing user fatigue and processing input tasks inefficiently. Such problems are difficult to solve simultaneously. Therefore, we propose using head tracking cursors and smart devices. The suggested method aims to operate a head tracking cursor by swiping input on the smart device. In this paper, we compared the operability of this new method and the classic hand tracking one based on the results of user experiments. As a result, it was confirmed that operability of the proposed method is deemed to be high. © 2019 Association for Computing Machinery.",2,Helmet mounted displays,Augmented reality,Head mounted displays - Information search - Manipulation methods - Operation methods - User experiments - User fatigues - Visual information - Window manipulations,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
3D human avatar digitization from a single image,"Li, Zhong (1); Chen, Lele (2); Liu, Celong (1); Gao, Yu (1); Ha, Yuanzhou (1); Xu, Chenliang (1); Quan, Shuxue (1); Xu, Yi (1) ","(1) OPPO US Research Center, Palo Alto; CA, United States (2) University of Rochester, Rochester; NY, United States ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365707,a12,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"With the development of AR/VR technologies, a reliable and straightforward way to digitize three-dimensional human body is in high demand. Most existing methods use complex equipment and sophisticated algorithms. This is impractical for everyday users. In this paper, we propose a pipeline that reconstructs 3D human shape avatar at a glance. Our approach simultaneously reconstructs the three-dimensional human geometry and whole body texture map with only a single RGB image as input.We first segment the human body part from the image and then obtain an initial body geometry by fitting the segment to a parametric model. Next, we warp the initial geometry to the final shape by applying a silhouette-based dense correspondence. Finally, to infer invisible backside texture from a frontal image, we propose a network we call InferGAN. Comprehensive experiments demonstrate that our solution is robust and effective on both public and our own captured data. Our human avatars can be easily rigged and animated using MoCap data. We developed a mobile application that demonstrates this capability in AR/VR settings. © 2019 Association for Computing Machinery.",34,Three dimensional computer graphics,Augmented reality - Deep learning - Geometry - Image reconstruction - Image segmentation - Image texture - Interactive computer graphics - Virtual reality,3D reconstruction - Complex equipment - Dense correspondences - Frontal images - Human body modeling - Mobile applications - Parametric modeling - Single images,"723 Computer Software, Data Handling and Applications - 921 Mathematics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
MRV 2019: 3rd workshop on mixed reality for intelligent vehicles,"Riegler, Andreas (1); Riener, Andreas (2); Kun, Andrew L. (3); Gabbard, Joe (4); Brewster, Stephen (5); Wienrich, Carolin (6) ","(1) University of Applied Sciences Upper Austria, Johannes Kepler University, Linz, Austria (2) Technische Hochschule Ingolstadt, Ingolstadt, Germany (3) University of New Hampshire, Durham; NH; 03824, United States (4) Virginia Tech., Blacksburg; VA; 24061, United States (5) University of Glasgow, Glasgow; G12 8QQ, United Kingdom (6) University of Wuerzburg, Wuerzburg, Germany ","Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",cerence; Helmholtz Instituut; here; Rijkswaterstaat - Ministry of Infrastructure and Water Management; Uber ARG; Utrecht University - Faculty of Social and Behavioral Sciences,"Association for Computing Machinery, Inc",,p 38-44,21-Sep-19,"Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",2019,,,,9.78145E+12,10.1145/3349263.3350758,,"11th ACM International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019","September 21, 2019 - September 25, 2019",,"With the increasing development of Augmented reality (AR), the number of its purposes and applications in vehicles rises. Augmented reality may help to increase road safety, support more immersive (non-) driving related activities, and finally enhance driving and passenger experience. AR may also be the enabling technology to increase trust and acceptance in automated vehicles and therefore help on the transition towards automated driving. Further, automated driving extends use cases of augmented and other immersive technologies. However, there are still a number of challenges with the use of augmented reality when applied in vehicles, and also several human factors issues need to be solved. Additionally, Virtual reality (VR) has the potential to simulate AR applications for HCI research. In this workshop, we will discuss potentials and constraints as well as impact, role, and adequacy of AR and VR (mixed reality, MR) in driving applications and simulations. The primary goal of this workshop is to define a research agenda for the use of MR in intelligent vehicles within the next 3 to 5 years and beyond. © 2019 Copyright is held by the owner/author(s).",23,Mixed reality,Augmented reality - Automation - Intelligent vehicle highway systems - Motor transportation - User interfaces - Vehicles,AR application - Automated driving - Automated vehicles - Enabling technologies - Hci researches - Immersive technologies - Research agenda - WSDs/HUDs,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 731 Automatic Control Principles and Applications",,,"Number: CMMI-1840085, Acronym: NSF, Sponsor: National Sleep Foundation; Number: 03IHS109A, Acronym: BMBF, Sponsor: Bundesministerium f&uuml;r Bildung und Frauen; Number: -, Acronym: -, Sponsor: Laurea University of Applied Sciences; ","This work is supported by the Innovative Hochschule"" program of the German Federal Ministry of Education and Research (BMBF) under Grant No. 03IHS109A (MenschINBe-wegung)", the University of Applied Sciences PhD program of the government of Upper Austria," and the NSF grant CMMI-1840085.""",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,
A transparent display with per-pixel color and opacity control,"Rhodes, T.J. (1); Miller, Gavin (1); Sun, Qi (1); Ito, Daichi (1); Wei, Li-Yi (1) ","(1) Adobe Research, United States ","ACM SIGGRAPH 2019 Emerging Technologies, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Emerging Technologies, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3305367.3327984,,"ACM SIGGRAPH 2019 Emerging Technologies - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"We propose a new display system that composites matted foreground animated graphics and video, with per-pixel controllable emitted color and transparency, over real-world dynamic objects seen through a transparent display. Multiple users can participate simultaneously without any glasses, trackers, or additional devices. The current prototype is deployed as a desktop-monitor-sized transparent display box assembled from commodity hardware components with the addition of a high-frame-rate controllable diffuser. © 2019 Copyright held by the owner/author(s).",2,Interactive computer graphics,Augmented reality - Display devices - Pixels - Sensory perception,Animated graphic - Commodity hardware - Display system - High frame rate - Multiple user - Pixel color - Real-world - Transparent displays,"461.4 Ergonomics and Human Factors Engineering - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Extended reality for midwifery learning: MR VR demonstration Roads to Birth,"Jones, Donovan (1); See, Zi Siang (2); Billinghurst, Mark (3); Goodman, Lizbeth (3); Fealy, Shanna (1) ","(1) School of Nursing and Midwifery, University of Newcastle, Newcastle; NSW, Australia (2) School of Creative Industries, University of Newcastle, Newcastle; NSW, Australia (3) Emphatic Computing Lab, University of South Australia, Adelaide; SA, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365739,a61,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"This demonstration presents a development of a Mixed Reality (MR) and Virtual Reality (VR) research project for midwifery student learning, and a novel approach for showing extended reality content in an educational setting. The Road to Birth (RTB) visualises the changes that occur in the female body during pregnancy, and the five days immediately after birth (postpartum) in a detailed 3D setting. In the Base Anatomy studio, users can observe the base anatomical layers of an adult female. In Pregnancy Timeline, they can scroll through the weeks of gestation to see the development of the baby and the anatomical changes of the mother throughout the pregnancy and postpartum. Finally, users can learn about the different possible birthing positions that may present in Birth Considerations. During the demo, users can experience the system in either MR or VR. © 2019 Association for Computing Machinery.",2,Mixed reality,Augmented reality - Interactive computer graphics - Obstetrics - Virtual reality,Adult females - Anatomical changes - Educational settings - Student learning - User experience,"461.6 Medicine and Pharmacology - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Can a phone hear the shape of a room?,"Shih, O. (1); Rowe, A. (1) ","(1) Carnegie Mellon Univ., Pittsburgh, PA, United States ",2019 18th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN). Proceedings,,"IEEE, Piscataway, NJ, USA",,277-88,2019,,,,,,978-1-7281-0727-1,10.1145/3302506.3310407,,2019 18th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN),15-18 April 2019,"Montreal, QC, Canada","Understanding the location of acoustically reflective surfaces in a room is a critical component in advanced sound processing. For example, intelligent speakers can use a room's acoustic geometry to improve playback quality, source separation accuracy, and speech recognition. In this paper, we present Synesthesia, a system for capturing the acoustic properties of a room using a single fixed speaker and a mobile phone that records audio at multiple locations. Using the arrival time of echoes, the system is able to reconstruct the position of reflective surfaces like walls and then estimate properties like surface absorption. Previous work has shown how the acoustic room impulse response (RIR) of an environment can be used to analyze echoes within a space to reconstruct room geometry. The best current RIR-based approaches rely on high-end equipment and capturing an acoustic signal broadcast into space from a known fixed constellation of microphones. They also require the precise calibration and measurement of microphone positions. In addition, most approaches pose constraints on room geometries and limit the order of RIR to achieve accurate and consistent results. In this paper, we introduce a new approach that performs RIR imaging using a mobile phone that tracks its location with visual inertial odometry (VIO) to record a dense set of samples albeit with noise in their locations. We present a new approach that is able to relax several key assumptions on RIR and show through both experimentation and simulation that even with 20cm of uncertainty in the microphone locations provided by VIO, we are still able to reconstruct the room geometry with accurate shape and dimensions. We demonstrate this capability by prototyping a tool for acoustic engineers, that allows a user to view a room's estimated geometry and absorption overlaid on the actual sensed space with augmented reality.",39,,acoustic noise - acoustic signal processing - architectural acoustics - augmented reality - calibration - distance measurement - feature extraction - geometry - microphones - speech processing - speech recognition - transient response,source separation accuracy - acoustic properties - single fixed speaker - mobile phone - multiple locations - echoes - surface absorption - acoustic room impulse response - room geometry - current RIR-based approaches - acoustic signal broadcast - microphone positions - RIR imaging - microphone locations - acoustic engineers - estimated geometry - acoustically reflective surfaces - advanced sound processing - intelligent speakers - acoustic geometry,B6130E Speech recognition and synthesis - B0240Z Other topics in statistics - B0250 Combinatorial mathematics - C6130V Virtual reality - C1140Z Other topics in statistics - C1160 Combinatorial mathematics,E04B1/99 - G10L - G10L15/00 - H04R,Practical (PRA); Theoretical or Mathematical (THR),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Tabletop AR with HMD and tablet: A comparative study for 3D selection,"Plasson, Carole (1); Cunin, Dominique (2); Laurillau, Yann (1); Nigay, Laurence (1) ","(1) Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, Grenoble; F-38000, France (2) Ecole Superieure d'Art et Design Grenoble-Valence, Unite de Recherche, Valence; F-26000, France ",ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 409-414,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,10.1145/3343055.3360760,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"We experimentally compare the performance and usability of tablet-based and see-through head-mounted display (HMD)-based interaction techniques for selecting 3D virtual objects projected on a table. This study is a first step toward a better understanding of the advantages and limitations of these interaction techniques, with the perspective of improving interaction with augmented maps. To this end, we evaluate the performance of 3 interaction techniques in selecting 3D virtual objects in sparse and dense environments: (1) the direct touch interaction with a HMD; (2) the ray-casting interaction with a HMD; and (3) the touch interaction on a tablet. Our results show that the two techniques using a HMD are faster, less physically tiring and preferred by the participants over the tablet. The HMD-based interaction techniques perform equally well but the direct touch technique seems to be less impacted by small targets and occlusion. © Copyright is held by the author/owner(s).",9,Helmet mounted displays,Augmented reality - Human computer interaction - Rendering (computer graphics),Comparative studies - Direct-touch interactions - Head mounted displays - Interaction techniques - Tablet - Tabletop - Touch - Touch interaction,"723 Computer Software, Data Handling and Applications",,,,We gratefully acknowledge the support of the AP2 project ANR-15-CE23-0001.,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Immersive mixed reality object interaction for collaborative context-aware mobile training and exploration,"Botev, Jean (1); Mayer, Joe (1); Rothkugel, Steffen (1) ","(1) University of Luxembourg, Esch-sur-Alzette; 4364, Luxembourg ","Proceedings of the 11th ACM Workshop on Immersive Mixed and Virtual Environment Systems, MMVE 2019",ACM SIGMM,"Association for Computing Machinery, Inc",,p 4-9,18-Jun-19,"Proceedings of the 11th ACM Workshop on Immersive Mixed and Virtual Environment Systems, MMVE 2019",2019,,,,9.78145E+12,10.1145/3304113.3326117,,"11th ACM SIGMM Workshop on Immersive Mixed and Virtual Environment Systems, MMVE 2019",18-Jun-19,,"The recent generation of mobile devices brings the necessary processing power, sensorics and other hardware to enable highly interactive and immersive mixed reality experiences. However, existing applications mainly focus on single users or the individual experience, and are often limited to basic user-device interaction. This paper discusses an immersive object interaction approach based on gestural input generated solely from the integrated camera of a mobile device. Developed within the context of the CollaTrEx framework for collaborative context-aware mobile training and exploration, it particularly allows for in-situ multiuser interaction with virtual objects. The presented approach is demonstrated in a set of prototypical game implementations for smartphones. © 2019 Association for Computing Machinery.",20,Mixed reality,Augmented reality,Experimental prototype - Immersion - Integrated cameras - Interaction - Multi-user interaction - Object interactions - Processing power - Virtual objects,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Effects of dark mode on visual fatigue and acuity in optical see-Through head-mounted displays,"Kim, Kangsoo (1); Erickson, Austin (1); Lambert, Alexis (1); Bruder, Gerd (1); Welch, Gregory F. (1) ","(1) University of Central Florida, Orlando; FL, United States ",Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,10.1145/3357251.3357584,a9,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,"Light-on-dark color schemes, so-called 'Dark Mode,' are becoming more and more popular over a wide range of display technologies and application fields. Many people who have to look at computer screens for hours at a time, such as computer programmers and computer graphics artists, indicate a preference for switching colors on a computer screen from dark text on a light background to light text on a dark background due to perceived advantages related to visual comfort and acuity, specifically when working in low-light environments. In this paper, we investigate the effects of dark mode color schemes in the field of optical see-Through head-mounted displays (OST-HMDs), where the characteristic 'additive' light model implies that bright graphics are visible but dark graphics are transparent. We describe a human-subject study in which we evaluated a normal and inverted color mode in front of different physical backgrounds and among different lighting conditions. Our results show that dark mode graphics on OST-HMDs have significant benefits for visual acuity, fatigue, and usability, while user preferences depend largely on the lighting in the physical environment. We discuss the implications of these effects on user interfaces and applications. © 2019 Association for Computing Machinery.",37,Field emission displays,Augmented reality - Color - Color computer graphics - Helmet mounted displays - Lighting - User interfaces - Vision,Dark modes - Eye fatigue - Optical see-through display - User experience - Visual acuity,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 741.1 Light/Optics",,,"Number: 1564065, Acronym: NSF, Sponsor: National Science Foundation; Number: -, Acronym: UCF, Sponsor: University of Central Florida; Number: -, Acronym: UF, Sponsor: University of Florida; Number: 1800947, Acronym: IIS, Sponsor: Division of Information and Intelligent Systems; Number: Welch, Acronym: ONR, Sponsor: Office of Naval Research; Number: -, Acronym: SU, Sponsor: Stanford University; ","This material includes work supported in part by the National Science Foundation under Award Number 1564065 (Dr. Ephraim P. Glinert, IIS) and Collaborative Award Numbers 1800961, 1800947, and 1800922 (Dr. Tonya Smith-Jackson, IIS) to the University of Central Florida, University of Florida, and Stanford University respectively; the Office of Naval Research under Award Number N00014-17-1-2927 (Dr. Peter Squire, Code 34); and the AdventHealth Endowed Chair in Healthcare Simulation (Prof. Welch). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the supporting institutions.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Digital assistance for quality assurance: Augmenting workspaces using deep learning for tracking near-symmetrical objects,"Belo, João (1); Fender, Andreas (1); Feuchtner, Tiare (1); Grønbæk, Kaj (1) ","(1) Aarhus University, Denmark ",ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 275-287,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,10.1145/3343055.3359699,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"We present a digital assistance approach for applied metrology on near-symmetrical objects. In manufacturing, systematically measuring products for quality assurance is often a manual task, where a main challenge for the workers lies in accurately identifying positions to measure and correctly documenting these measurements. This paper focuses on a use-case, which involves metrology of small near-symmetrical objects, such as LEGO bricks. We aim to support this task through situated visual measurement guides. Aligning these guides poses a major challenge, since fine grained details, such as embossed logos, serve as the only feature by which to retrieve an object's unique orientation. We present a two-step approach, which consists of (1) locating and orienting the object based on its shape, and then (2) disambiguating the object's rotational symmetry based on small visual features. We apply and compare different deep learning approaches and discuss our guidance system in the context of our use case. Copyright © 2019 Association of Computing Machinery.",48,Deep learning,Augmented reality - Computer vision - E-learning - Industry 4.0 - Measurement - Quality assurance,Feature recognition - Focus + context - Guidance system - Learning approach - Rotational symmetries - Situated Instructions - Two-step approach - Visual measurements,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 913.3 Quality Assurance and Control",,,"Number: 6151-00006B, Acronym: -, Sponsor: Innovationsfonden; ","Thanks are due to Marius Hogr&auml;fer and A&iuml;na Georges for their constructive feedback. This work was supported by the Innovation Fund Denmark (IFD) grant no. 6151-00006B, as part of the Manufacturing Academy of Denmark MADE Digital project.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Groovy assignment: The VR ride a cross disciplinary assignment in computer graphics and interactivity,"Jushchyshyn, Nick (1); Lloyd, Robert (1); Sundquist, Erik (2) ","(1) Digital Media Department, Drexel University, Philadelphia; PA, United States (2) Product Design Department, Drexel University, Philadelphia; PA, United States ","ACM SIGGRAPH 2019 Educators Forum, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Educators Forum, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3326542.3328018,a3,"ACM SIGGRAPH 2019 Educators Forum - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"In this Groovy Assignment submission, we present a VR Ride assignment that challenges students to create a fully interactive VR computer graphics experience integrated with a themed ride. For this assignment, a ride is an apparatus that fully supports the users weight, utilizes the user’s body motions as a primary input for computer interactivity, and provides haptic feedback relevant to the VR experience. The assignment is designed to inspire and motivate creative thinking and cross disciplinary collaboration with faculty and students from outside the scope of those traditionally involved in programs focused on computer graphics and interaction. The assignment can be easily scaled, by utilizing wholly existing, found or purchased platforms (exercise equipment such as a rowing Machine or stationary bike) for use with small groups of students if desired. In the example presented, students from Electrical Engineering, Mechanical Engineering, Industrial Design, Game Design, VR & Immersive Media, Animation & VFX and Game Design programs collaborated using primarily found or recycled components to build a bespoke, human powered 'VR Cycle' ride, integrated with original VR experiences developed for the ride. © 2019 Copyright held by the owner/author(s).",,Interactive computer graphics,Animation - Augmented reality - Education - Product design - Students - Virtual reality,Curricular development - Home entertainment - Immersive media - Location based - Motion-based attractions - Themed entertainment,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 913.1 Production Engineering",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Effects of depth layer switching between an optical see-Through head-mounted display and a body-proximate display,"Eiberger, Anna (1); Kristensson, Per Ola (2); Mayr, Susanne (1); Kranz, Matthias (1); Grubert, Jens (3) ","(1) University of Passau, Germany (2) Cambridge University, United Kingdom (3) Coburg University of Applied, Sciences and Arts, Germany ",Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,10.1145/3357251.3357588,a15,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,"Optical see-Through head-mounted displays (OST HMDs) typically display virtual content at a fixed focal distance while users need to integrate this information with real-world information at different depth layers. This problem is pronounced in body-proximate multidisplay systems, such as when an OST HMD is combined with a smartphone or smartwatch. While such joint systems open up a new design space, they also reduce users' ability to integrate visual information. We quantify this cost by presenting the results of an experiment (n=24) that evaluates human performance in a visual search task across an OST HMD and a body-proximate display at 30 cm. The results reveal that task completion time increases significantly by approximately 50% and the error rate increases significantly by approximately 100% compared to visual search on a single depth layer. These results highlight a design trade-off when designing joint OST HMD-body proximate display systems. © 2019 Association for Computing Machinery.",51,Helmet mounted displays,Augmented reality - Economic and social effects - Sensory perception - Street traffic control,Accomodations - Human performance - Multi display environments - Optical see-through head-mounted displays - Real-world information - Task completion time - Vergences - Visual information,"406.2 Roads and Streets - 461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 971 Social Sciences",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A web-based remote assistance system with gravity-aware 3D hand gesture visualization,"Kim, Chelhwon (1); Chiu, Patrick (1); Y., Tjahjadi ","(1) FX Palo Alto Laboratory, United States ",ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 319-322,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,10.1145/3343055.3360742,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"We present a remote assistance system that enables a remotely located expert to provide guidance using hand gestures to a customer who performs a physical task in a different location. The system is built on top of a web-based real-time media communication framework which allows the customer to use a commodity smartphone to send a live video feed to the expert, from which the expert can see the view of the customer's workspace and can show his/her hand gestures over the video in real-time. The expert's hand gesture is captured with a hand tracking device and visualized with a rigged 3D hand model on the live video feed. The system can be accessed via a web browser, and it does not require any app software to be installed on the customer's device. Our system supports various types of devices including smartphone, tablet, desktop PC, and smart glass. To improve the collaboration experience, the system provides a novel gravity-aware hand visualization technique. © 2019 Copyright is held by the owner/author(s).",7,Three dimensional computer graphics,3D modeling - Application programs - Augmented reality - Sales - Smartphones - Visualization - Websites,Hand gesture - Hand tracking - Provide guidances - Real-time communication - Real-time media communication - Remote assistance - System supports - Visualization technique,"718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Intermediated reality,"Casas, Llogari (1) ","(1) Edinburgh Napier University, United Kingdom ","SIGGRAPH Asia 2019 Doctoral Consortium, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,17-Nov-19,"SIGGRAPH Asia 2019 Doctoral Consortium, SA 2019",2019,,,,9.78145E+12,10.1145/3366344.3366629,A8,"SIGGRAPH Asia 2019 Doctoral Consortium - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"This thesis explores technical solutions to address the gap between the virtual and physical worlds towards photo-realistic interactive Augmented Reality (AR). As mobile network bandwidth increases, latencies reduce and graphics processing power becomes more efficient, this work tackles the challenge of convincingly re-animating physical objects remotely through digital displays. A framework for distributed Intermediated Reality (IR) communication is introduced, and forms the structure of the constituent methods developed for seamless collaboration through the remote possession of entities that come to life in mobile AR. To perform such augmentation in an unnoticeable way, a method of deforming surface camera samples for seamless animations of physical objects with background inpainting is first introduced. This technique, in combination with a method to retarget the proximate appearance of real shadows to deformed virtual shadows and a method to perform environment illumination estimation using inconspicuous flat Fresnel lenses, brings real-world props to life in a compelling and practical way. Each method is integrated together to perform in real-time with analysis and evaluations using metric comparisons to expected ground truth renderings are provided. Intermediated Reality can be applied to a variety of industries and scenarios beyond communication. This thesis presents applications in the movie industry and computer games sectors. For example, an approach to reduce the number of physical configurations needed for a stop-motion animation movie by generating the in-between frames digitally in AR is demonstrated. AR-generated frames preserve its natural appearance and achieve smooth transitions between real-world key-frames and digitally generated in-betweens. Further, the techniques extend across the entire Reality-Virtuality Continuum to target new game experiences called Multi-Reality games. This gaming experience makes an evolutionary step toward the convergence of real and virtual game characters for visceral digital experiences. © 2019 Copyright held by the owner/author(s).",11,Mixed reality,Animation - Augmented reality - Computer games - Human computer interaction - Interactive computer graphics - Virtual addresses,Analysis and evaluation - Background inpainting - Environment illumination - Intermediated Reality - Real time graphics - Stop-motion animations - Technical solutions - Virtuality continuum,"722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Qadmetric optimized thumb-to-finger interaction for force assisted one-handed text entry on mobile headsets,"Lee, Lik Hang (1, 2); Lam, Kit Yung (1); Li, Tong (1); Braud, Tristan (1); Su, Xiang (3); Hui, Pan (1, 3) ","(1) Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong (2) Center for Ubiquitous Computing, University of Oulu, Finland (3) Department of Computer Science, University of Helsinki, Finland ","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",,Association for Computing Machinery,"v 3, n 3",,Sep-19,,2019,,,24749567,,10.1145/3351252,94,,,,"Augmented reality head-worn computers often feature small-sized touch interfaces that complicate interaction with content, provide insuficient space for comfortable text input, and can be awkward to use in social situations. This paper presents a novel one-handed thumb-to-finger text entry solution for augmented reality head-worn computers. We design a glove composed of 12 force-sensitive nodes featuring an ambiguous keyboard layout. We first explore the viability of force disambiguation to evaluate the force division within the force spectrum. We select a 3-level force division as it allows to considerably reduce the number of keys while featuring a high (83.9%) accuracy. Following this pilot study, we map the 26 English characters onto the 9 nodes located on the index, middle and ring fingers in a 3-3-3 configuration, and attribute the space, enter and backspace keys to the remaining three nodes. We consider text entry performance as a quadmetric optimization problem considering the following criteria: goodness of character pairs, layout similarity to the QWERTY keyboard, easiness of force interaction, and comfort level of thumb reach. The resulting layout strikes a balance between performance and usability. We finally evaluate the quadmetric optimized layout over 6 sessions with 12 participants. The participants achieve an average text entry rate of 6.47 WPM with 6.85% error rate in the final session, which is significantly faster than existing thumb-to-finger solutions. In addition, our one-handed text entry system enhances the user mobility compared to other state-of-the-art solutions by freeing one hand, while allowing the user to direct his visual attention to other activities. © 2019 Association for Computing Machinery.",44,Human computer interaction,Augmented reality - Behavioral research,Finger interactions - Force interaction - Optimization problems - Smart wearables - Text entry systems - Text input - Touch interfaces - Visual Attention,"723 Computer Software, Data Handling and Applications - 971 Social Sciences",,,"Number: 5GEAR, Acronym: -, Sponsor: Academy of Finland; ",This research has been supported in part by project 16214817 from the Research Grants Council of Hong Kong and the 5GEAR project from the Academy of Finland.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Smart glasses in galleries libraries archives museums,"Singh, Hemant (1); Blustein, James (2) ","(1) Faculty of Computer Science, Dalhousie University, Halifax; NS, Canada (2) Faculty of Computer Science, School of Information Management, Dalhousie Univeristy, Halifax; NS, Canada ",HUMAN 2019 - Proceedings of the 2nd International Workshop on Human Factors in Hypertext,ACM SIGWEB,"Association for Computing Machinery, Inc",,p 35-38,12-Sep-19,HUMAN 2019 - Proceedings of the 2nd International Workshop on Human Factors in Hypertext,2019,,,,9.78145E+12,10.1145/3345509.3350528,,"2nd International Workshop on Human Factors in Hypertext, HUMAN 2019, in conjunction with the 30th ACM International Conference on Hypertext and Social, HT 2019",17-Sep-19,,"We examine ways to engage visitors to art galleries in Canada and review less successful methods. We specifically explore the possibility of smart wearable devices, such as smart glasses, to improve visitors' experience and increase the number of visitors in galleries, archives and museums. © 2019 Copyright is held by the owner/author(s).",25,Museums,Augmented reality - Glass - Human engineering - Hypertext systems - Wearable technology,Art gallery - Smart glass - Smart wearables - Wearable devices,"402.2 Public Buildings - 461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 812.3 Glass",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
IlluminatedFocus: Vision augmentation using spatial defocusing,"Ueda, Tatsuyuki (1); Iwai, Daisuke (1); Sato, Kosuke (1) ","(1) Osaka University, Japan ","SIGGRAPH Asia 2019 Emerging Technologies, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 21-22,17-Nov-19,"SIGGRAPH Asia 2019 Emerging Technologies, SA 2019",2019,,,,9.78145E+12,10.1145/3355049.3360530,,"SIGGRAPH Asia 2019 Emerging Technologies - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"We propose IlluminatedFocus, augmented reality (AR) glasses enabling depth-independent spatial defocusing of a human vision. Our technique spatially manipulates the depth-of-field by synchronizing a periodic focal sweep of head-worn electrically tunable lenses and fast illumination control with a high-speed projector. In this demonstration, we show a system that switches focused and defocused views independently at each area of a 3D real scene. We realize various vision augmentation applications based on our method to show its potential to expand the application field of optical see-through AR. © 2019 Copyright held by the owner/author(s).",3,Interactive computer graphics,Augmented reality,Application fields - Defocusing - Depth of field - Electrically tunable - Focus control - High Speed - Human vision - Optical see-through,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: 18K19817, Acronym: JSPS, Sponsor: Japan Society for the Promotion of Science; ",This work was supported by JSPS KAKENHI Grant Number 17H04691 and 18K19817.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The unethical future of mixed reality storytelling,"Millard, David E. (1); Hewitt, Sarah (1); O'Hara, Kieron (1); Packer, Heather (1); Rogers, Neil (1) ","(1) University of Southampton, Southampton, United Kingdom ",NHT 2019 - Proceedings of the 8th International Workshop on Narrative and Hypertext,ACM SIGWEB,"Association for Computing Machinery, Inc",,p 5-8,12-Sep-19,NHT 2019 - Proceedings of the 8th International Workshop on Narrative and Hypertext,2019,,,,9.78145E+12,10.1145/3345511.3349283,,"8th International Workshop on Narrative and Hypertext, NHT 2019, in conjunction with 30th ACM Conference on Hypertext and Social Media, HT 2019",17-Sep-19,,"As mixed reality storytelling becomes more popular we are beginning to see examples of where it can go wrong, by causing harm to those that directly participate, or offence to those indirectly affected. Without an ethical framework to inform design, mixed reality storytelling could have the same sorts of unintended consequences as other digital technologies (for example, social media that has led to mass surveillance and problems with anti-social behaviour). But what might these be? In this paper we explore a range of ethical issues that could affect mixed reality storytelling technologies in order to illustrate the complexity that awaits as they become more popular. We describe ethical responsibilities under two broad themes. The first is a responsibility to the place, in terms of avoiding physical trespass, respecting cultural norms of behaviour, control over virtual graffiti, consideration of names, and awareness of the values embedded in narratives. The second is a responsibility to the person, in terms of safe passage, expectations of accuracy, respect for social and psychological norms, and obtaining wide consent. In both cases there are unresolved legal questions about the duty of care that designers have for their participants, and cultural questions around balancing the competing claim rights of stakeholders with the liberty rights of artists, writers, and designers. © 2019 Association for Computing Machinery.",26,Mixed reality,Augmented reality - Economic and social effects - Hypertext systems - Philosophical aspects - Social networking (online),Ambient literature - Ethics - Hybrid reality - Locative games - Locative literatures,"723 Computer Software, Data Handling and Applications - 971 Social Sciences",,,,The authors would like to thank the Web Science Institute for providing seed funding to undertake this research.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Social interaction in spatial augmented exertion interfaces,"Mast, Dania (1, 2) ","(1) Faculty for IT and Design Research Group, Healthy Lifestyle in a Supporting Environment, Hague University of Applied Sciences, Netherlands (2) Creative Intelligence Lab, Leiden Institute for Advanced Computer Science, Leiden University, Netherlands ",CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 29-34,17-Oct-19,CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play,2019,,,,9.78145E+12,10.1145/3341215.3356331,,"6th ACM SIGCHI Annual Symposium on Computer-Human Interaction in Play, CHI PLAY 2019","October 22, 2019 - October 25, 2019",,"Exertion interfaces have the potential to facilitate social interaction and physical activity, supporting people as part of a healthy lifestyle. Knowledge about the interrelationship between social interaction and physical activity in exertion interfaces and knowledge of the best technology and interactive features they are facilitated by is currently lacking. Based on existing exertion interfaces and previous research, pilot prototypes, followed by high fidelity prototypes will be designed and developed following a research through design approach. © 2019 Copyright is held by the owner/author(s).",20,Human computer interaction,Augmented reality - Interactive computer systems,Exertion Interface - Physical activity - Social interactions - Social Play - Spatial augmented realities,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications",,,"Number: 023.013.062, Acronym: NWO, Sponsor: Nederlandse Organisatie voor Wetenschappelijk Onderzoek; ","This PhD research is funded by the Dutch Research Council (NWO) doctoral grant for teachers (023.013.062) and supervised by Prof. dr. ir. Fons Verbeek (Leiden Institute for Advanced Computer Science, Leiden University, the Netherlands), dr. Sanne de Vries (Research Group Healthy Lifestyle in a Supporting Environment, the Hague University of Applied Sciences, the Netherlands) and dr.ir. Joost Broekens (Leiden Institute for Advanced Computer Science, Leiden University, the Netherlands).Since 2014, I have been a member of the research group Healthy Lifestyle in a Supporting Environment at the Hague University of Applied Sciences (focusing on technology for Physical Education), besides being a lecturer at the Faculty for IT & Design. Here, I learned that research motivates and suits me very well and decided to start working towards obtaining a PhD. I conducted a pilot experiment into passive and active play of cooperative Tetris and its influence on social interaction [10]. Following a preliminary PhD trajectory in 2017, I wrote a proposal for an NWO (The Dutch Research Council) Doctoral Grant for Teachers in, that was granted to me in 2019, allowing me to officially continue my PhD research.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Towards visual augmentation of the television watching experience: Manifesto and agenda,"Popovici, Irina (1); Vatavu, Radu-Daniel (1) ","(1) MintViz Lab, MANSiD Research Center, University Stefan cel Mare, Suceava Suceava; 720229, Romania ",TVX 2019 - Proceedings of the 2019 ACM International Conference on Interactive Experiences for TV and Online Video,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 199-204,4-Jun-19,TVX 2019 - Proceedings of the 2019 ACM International Conference on Interactive Experiences for TV and Online Video,2019,,,,9.78145E+12,10.1145/3317697.3325121,,"6th ACM International Conference on Interactive Experiences for TV and Online Video, TVX 2019","June 5, 2019 - June 7, 2019",,"We present an agenda for the visual augmentation of television watching based on recently booming technology, such as smart wearables and Augmented/Mixed Reality technology. Our agenda goes beyond second-screen viewing trends to explore the opportunities delivered by wearable devices and gadgets, such as smartglasses and head-mounted displays, to deliver rich visual experiences to users. While still a work-in-progress, we hope that our contribution will be inspiring to the TVX community and, consequently, foster critical and constructive discussions towards new devices, application opportunities, and tools to augment visually the television watching experience. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",29,Interactive television,Augmented reality - Helmet mounted displays - Wearable technology,Head mounted displays - Low vision - Smartglasses - Visual augmentation - Visual experiences - Wearable devices - Wearables - Work in progress,"716.4 Television Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,,"This work was supported by a grant of the Ministry of Research and Innovation, CNCS-UEFISCDI, project no. PN-III-P1-1.1-TE-2016-2173 (TE141/2018), within PNCDI III. Icons (black & white) were made by Freepik (http://www.freepik.com, Miscellaneous Elements"") from Flaticon (http://www.flaticon. com)"," which we adapted under Creative Commons BY 3.0 (http://creativecommons.org/licenses/by/3.0).""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,
Augmenting public reading experience to promote care home residents' social interaction,"Kang, Kai (1); Hu, Jun (1); Hummels, Caroline (1); Hengeveld, Bart (1) ","(1) Industrial Design Department, Eindhoven University of Technology, Eindhoven, Netherlands ",TVX 2019 - Proceedings of the 2019 ACM International Conference on Interactive Experiences for TV and Online Video,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 184-192,4-Jun-19,TVX 2019 - Proceedings of the 2019 ACM International Conference on Interactive Experiences for TV and Online Video,2019,,,,9.78145E+12,10.1145/3317697.3325128,,"6th ACM International Conference on Interactive Experiences for TV and Online Video, TVX 2019","June 5, 2019 - June 7, 2019",,"Institutional care settings are often described as places where residents suffer from social isolation. Although sharing media preferences, consumption patterns and practices is believed to be effective to trigger communications and develop friendships between older adults, it rarely happens in care homes. Our research explores the potential to promote residents' social interaction by augmenting public print media. In this work-in-progress, we started with newspapers as an example to understand residents' information sources, media habits and preferences. We were also interested in their perceptions of the attractiveness and sociability of augmented print media. The findings showed that the participants held positive attitudes on such technologies. Preliminary design requirements were summarized to inform the future development of related social technologies in public caring environments. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",10,Interactive television,Augmented reality,Consumption patterns - Information sources - Media sharing - Nursing homes - Positive attitude - Preliminary design - Social interactions - Social technologies,"716.4 Television Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
On reproducing semi-dense depth map reconstruction using deep convolutional neural networks with perceptual loss,"Makarov, Ilya (1); Maslov, Dmitrii (1); Gerasimova, Olga (1); Aliev, Vladimir (1); Korinevskaya, Alisa (1); Sharma, Ujjwal (2); Wang, Haoliang (3) ","(1) National Research University Higher School of Economics, Moscow, Russia (2) University of Amsterdam, Amsterdam, Netherlands (3) Adobe Research, United States ",MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,ACM SIGMM,"Association for Computing Machinery, Inc",,p 1080-1084,15-Oct-19,MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,2019,,,,9.78145E+12,10.1145/3343031.3351167,,"27th ACM International Conference on Multimedia, MM 2019","October 21, 2019 - October 25, 2019",,"In our recent papers, we proposed a new family of residual convolutional neural networks trained for semi-dense and sparse depth reconstruction without use of RGB channel. The proposed models can be used in low-resolution depth sensors or SLAM methods estimating partial depth with certain distributions. We proposed using perceptual loss for training depth reconstruction in order to better preserve edge structure and reduce over-smoothness of models trained on MSE loss alone. This paper contains reproducibility companion guide on training, running and evaluating suggested methods, while also presenting links on further studies in view of reviewers comments and related problems of depth reconstruction. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",10,Deep neural networks,Augmented reality - Autonomous vehicles - Computer vision - Convolution - Mixed reality - Neural networks,Convolutional neural network - Dense depth map - Depth Map - Depth reconstruction - Depth sensors - Edge structures - Low resolution - Reproducibilities,"716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: 17-11-01294, Acronym: RSF, Sponsor: Russian Science Foundation; Number: -, Acronym: HES, Sponsor: National Research University Higher School of Economics; ","Sections 1, 2, 4.2 and 5.2 on depth super-resolution were supported by the Russian Science Foundation under grant 17-11-01294 and performed at National Research University Higher School of Economics, Russia. Sections 3, 4.1 and 5.1 on depth reconstruction were prepared within the framework of the HSE University Basic Research Program and funded by the Russian Academic Excellence Project'5-100'.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Extended reality practice in art & design creative education,"Kim, June ; Bennett, Gregory ; Fu, Snow ; Kruse, Jan ",,"SIGGRAPH Asia 2019 Courses, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,17-Nov-19,"SIGGRAPH Asia 2019 Courses, SA 2019",2019,,,,9.78145E+12,10.1145/3355047.3359407,3359407,"SIGGRAPH Asia 2019 Courses - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"As Jerald (2018) states, though virtual reality (VR) has existed for over 50 years, its use as a creative medium is relatively new. In the last four years, as part of the 'second wave of VR', new affordability and accessibility of hardware and software for experiencing and creating VR has incited a surge of interest for the technology from creative industries. Meanwhile, interest and attempts to create VR projects has expanded into other forms of Extended Reality (XR) technologies, like Augmented Reality (AR) and Mixed Reality (MR). As a group of educators and practitioners from creative disciplines, our focus is to create a fundamentals of XR Education curriculum for undergraduates and/or postgraduates in schools of art and design who have no/less coding and software background. We believe in guiding students to approach VR as a creative medium is increasingly important. Furthermore, we also introduce the XR-ED Group (sponsored by ACM SIGGRAPH Educators Forum). This group is a collective of educators and practitioners interested in creating an XR curriculum, and to share the work of students. The group was first run as part of a co-located event with VRCAI 2018 / SIGGRAPH Asia 2018 in Tokyo, and this year will be running in Brisbane as a co-located event with VRCAI 2019 / SIGGRAPH Asia 2019. © 2019 Copyright held by the owner/author(s).",,Mixed reality,Augmented reality - Curricula - Interactive computer graphics - Students,Art and designs - Brisbane - Co-located - Creative education - Creative industries - Education curriculums - Hardware and software - Running-in,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 901.2 Education",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Mimicry attacks on smartphone keystroke authentication,"Khan, Hassan (1); Hengartner, Urs (2); Vogel, Daniel (2) ","(1) Reynolds Building, School of Computer Science, University of Guelph, Guelph; ON, Canada (2) Davis Center, Cheriton School of Computer Science, University of Waterloo, Waterloo; ON, Canada ",ACM Transactions on Privacy and Security,,Association for Computing Machinery,"v 23, n 1",,Feb-20,,2020,,24712566,24712574,,10.1145/3372420,2,,,,"Keystroke behaviour-based authentication employs the unique typing behaviour of users to authenticate them. Recent such proposals for virtual keyboards on smartphones employ diverse temporal, contact, and spatial features to achieve over 95% accuracy. Consequently, they have been suggested as a second line of defense with text-based password authentication. We show that a state-of-the-art keystroke behaviour-based authentication scheme is highly vulnerable against mimicry attacks. While previous research used training interfaces to attack physical keyboards, we show that this approach has limited effectiveness against virtual keyboards. This is mainly due to the large number of diverse features that the attacker needs to mimic for virtual keyboards. We address this challenge by developing an augmented reality-based app that resides on the attacker's smartphone and leverages computer vision and keystroke data to provide real-time guidance during password entry on the victim's phone. In addition, we propose an audiovisual attack in which the attacker overlays transparent film printed with spatial pointers on the victim's device and uses audio cues to match the temporal behaviour of the victim. Both attacks require neither tampering or installing software on the victim's device nor specialized hardware. We conduct experiments with 30 users to mount over 400 mimicry attacks. We show that our methods enable an attacker to mimic keystroke behaviour on virtual keyboards with little effort. We also demonstrate the extensibility of our augmented reality-based technique by successfully mounting mimicry attacks on a swiping behaviour-based continuous authentication system. © 2020 Association for Computing Machinery.",51,Authentication,Augmented reality - Computer keyboards - Smartphones,Authentication scheme - Continuous authentications - Mimicry attacks - Password authentication - Specialized hardware - Spoofing attacks - Temporal behaviour - Transparent films,"718.1 Telephone Systems and Equipment - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: RGPIN-2014-05499, Acronym: NSERC, Sponsor: Natural Sciences and Engineering Research Council of Canada; ","Part of this work appeared in ACM MobiSys&rsquo;18 [Khan et al. 2018]. We gratefully acknowledge the support of NSERC for Grants No. RGPIN-2019-05120, No. RGPIN-2014-05499, and No. RGPIN-2018-05187. Authors&rsquo; addresses: H. Khan, Reynolds Building, School of Computer Science, University of Guelph, Guelph, Ontario, Canada; email: hassan.khan@uoguelph.ca; U. Hengartner and D. Vogel, Davis Center, Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada; emails: {urs.hengartner, dvogel}@uwaterloo.ca. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. &copy; 2020 Association for Computing Machinery. 2471-2566/2020/02-ART2 $15.00 https://doi.org/10.1145/3372420",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
From manual driving to automated driving: A review of 10 years of AutoUI,"Ayoub, Jackie (1); Zhou, Feng (3); Bao, Shan (2); Yang, X. Jessie (3) ","(1) University of Michigan, Dearborn, Dearborn; MI, United States (2) University of Michigan Transportation Research Institute, Ann Arbor; MI, United States (3) University of Michigan, Ann Arbor, Ann Arbor; MI, United States ","Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",cerence; Helmholtz Instituut; here; Rijkswaterstaat - Ministry of Infrastructure and Water Management; Uber ARG; Utrecht University - Faculty of Social and Behavioral Sciences,"Association for Computing Machinery, Inc",,p 70-90,21-Sep-19,"Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",2019,,,,9.78145E+12,10.1145/3342197.3344529,,"11th ACM International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019","September 21, 2019 - September 25, 2019",,"This paper gives an overview of the ten-year development of the papers presented at the International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutoUI) from 2009 to 2018. We categorize the topics into two main groups, namely, manual driving-related research and automated driving-related research. Within manual driving, we mainly focus on studies on user interfaces (UIs), driver states, augmented reality and head-up displays, and methodology; Within automated driving, we discuss topics, such as takeover, acceptance and trust, interacting with road users, UIs, and methodology. We also discuss the main challenges and future directions for AutoUI and offer a roadmap for the research in this area. © 2019 Association for Computing Machinery.",166,User interfaces,Augmented reality - Automation - Head-up displays - Interface states,Automated driving - Main group - Manual driving - Road users - Roadmap - Vehicular applications,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731 Automatic Control Principles and Applications - 931 Classical Physics; Quantum Theory; Relativity - 932 High Energy Physics; Nuclear Physics; Plasma Physics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Hypercept: Speculating the visual world intervened by digital media,"Li, Jiabao (1); Michalatos, Panagiotis (1); Deng, Honghao (1) ","(1) Harvard Graduate School of Design, 48 Quincy St, Cambridge; MA; 02138, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3313282,3313282,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Human perception has long been influenced by technological breakthroughs. An intimate mediation of technology lies in between our direct perceptions and the environment we perceive. Through three extreme ideal types of perceptual machines, this project defamiliarizes and questions the habitual ways in which we interpret, operate, and understand the visual world intervened by digital media. The three machines create: Hyper-sensitive vision - a speculation on social media's amplification effect and our filtered communication landscape. Hyper-focused vision - an analogue version of the searching behavior on the Internet. Hyper-commoditized vision - monetized vision that meditates on the omnipresent advertisement targeted all over our visual field. © 2019 Copyright is held by the owner/author(s). ACM",2,Digital storage,Augmented reality - Human engineering - Vision,Amplification effects - Human perception - Searching behavior - Soft robotics - Tactile Art - Technological breakthroughs - Visual fields - Wearables,"461.4 Ergonomics and Human Factors Engineering - 722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Building Jarvis - A learner-aware conversational trainer,"Mohan, Shiwali (1); Ramea, Kalai (1); Price, Bob (1); Shreve, Matthew (1); Eldardiry, Hoda (1); Nelson, Les (1) ","(1) Palo Alto Research Center, Palo Alto; CA, United States ",CEUR Workshop Proceedings,,CEUR-WS,v 2327,,2019,"ACMIUI-WS 2019 - Joint Proceedings of the ACM IUI 2019 Workshops, co-located with the 24th ACM Conference on Intelligent User Interfaces, ACM IUI 2019",2019,,16130073,,,,,"2019 Joint ACM IUI Workshops, ACMIUI-WS 2019",20-Mar-19,,"Our long-term research goal is to develop intelligent systems that can support human learning. We are particularly interested in developing an approach to apprenticeship learning which occurs during the physical context of task execution and is known to be very effective in learning procedural tasks such as equipment maintenance or artifact assembly. We describe our initial progress in building such system - Jarvis - that leverages real-time computer vision, high-level inference, and augmented reality technology to monitor and support human task learning through apprenticeship. © 2019 Copyright for the individual papers by the papers’ authors. Copying permitted for academic purposes. This volume is published and copyrighted by its editors.",21,Learning systems,Apprentices - Augmented reality - Computer vision - Human computer interaction - Intelligent systems - User interfaces,Cognitive agents - Embodied dialog - Human agent interactions - Interactive instructions - Real-time reasoning - Soar - Task learning - Task-oriented,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 912.4 Personnel",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"A video-based automated driving simulator for automotive UI prototyping, UX and behaviour research","Gerber, Michael A. (1); Schroeter, Ronald (1); Vehns, Julia (2) ","(1) CARRS-Q, QUT Brisbane, Australia (2) CARRS-Q, LMU Brisbane, Munich, Germany ","Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",cerence; Helmholtz Instituut; here; Rijkswaterstaat - Ministry of Infrastructure and Water Management; Uber ARG; Utrecht University - Faculty of Social and Behavioral Sciences,"Association for Computing Machinery, Inc",,p 14-23,21-Sep-19,"Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",2019,,,,9.78145E+12,10.1145/3342197.3344533,,"11th ACM International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019","September 21, 2019 - September 25, 2019",,"The lack of automated cars above SAE level 3 raises challenges for conducting User Experience Design (UXD) and behaviour research for automated driving. User-centred methods are critical to ensuring a human-friendly progress of vehicle automation. This work introduces the Immersive Video-based Automated Driving (IVAD) Simulator. It uses carefully recorded 180/360° videos that are played back in a driving simulator. This provides immersive driving experiences in visually realistic and familiar environments. This paper reports learnings from an iterative development of IVAD, and findings of two user studies: One simulator study (N=15) focused on the immersive experience; and one VR study (N=16) focused on rapid prototyping and the evaluation of Augmented Reality (AR) concepts. Overall, we found the method to be a useful, versatile and low budget UXD tool with a high level of immersion that is uniquely aided by the familiarity of the environment. IVAD’s limitations and future improvements are discussed in relation to research applications within AutoUI. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",29,Behavioral research,Augmented reality - Automation - Automobile simulators - Budget control - Iterative methods - User interfaces - Virtual reality,Automated driving - Driving experiences - Iterative development - Prototyping tools - Research applications - User experience - User experience design - Vehicle automations,"662 Automobiles and Smaller Vehicles - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731 Automatic Control Principles and Applications - 921.6 Numerical Methods - 971 Social Sciences",,,"Number: LP150100979, Acronym: ARC, Sponsor: Australian Research Council; ","This work is funded by the Australian Research Council (ARC) as a Linkage Project (LP150100979) in cooperation with industry partner SeeingMachines Pty Ltd. We further acknowledge the help of our colleagues Yujia Wang, Dr Leo Razayan, Mohammad H. Faramarzian B., Jorge Prado and Dr Sebastien Demmel.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Design daydreams: Juxtaposing digital and physical inspiration,"Mothersill, Philippa (1); Bove, V. Michael (1) ","(1) MIT Media Lab., Cambridge; MA; 02139, United States ",DIS 2019 Companion - Companion Publication of the 2019 ACM Designing Interactive Systems Conference,Adobe; Google; Sketch; UC San Diego's Design Lab; Virginia Tech,"Association for Computing Machinery, Inc",,p 265-269,18-Jun-19,DIS 2019 Companion - Companion Publication of the 2019 ACM Designing Interactive Systems Conference,2019,,,,9.78145E+12,10.1145/3301019.3323909,,"2019 ACM Conference on Designing Interactive Systems, DIS 2019","June 23, 2019 - June 28, 2019",,"Design Daydreams is part of a suite of new computational design tools that integrate ambiguity and juxtaposition into the systems that we use to discover new ideas. Using a low-tech augmented reality system to visually overlay digital images on top of objects, the Design Daydreams augmented ‘post-it note’ fluidly extends the inspiration designers find online into the physically-interactive and collaborative brainstorming environment. Feedback suggested that the low-fidelity of the tool provided a natural ambiguity that left room for interpretation as designers juxtaposed digital and physical concepts together to create new ideas. © 2019 Copyright is held by the owner/author(s).",17,Augmented reality,,Augmented reality systems - Computational creativities - Computational design tools - Digital image - Low fidelities,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Demo: Tangible urban models,"Narazani, Marla (1); Eghtebas, Chloe (2); Jenney, Sarah L. (3); Mühlhaus, Michael (3) ","(1) Faculty of Computer Science, Technical University of Munich, Munich, Germany (2) Research Group Augmented Reality, Technical University of Munich, Munich, Germany (3) Architectural Informatics, Technical, University of Munich, Munich, Germany ",UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,p 320-323,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,10.1145/3341162.3343810,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,"Physical models are a key component in the architectural process and play an important role in understanding material and space relationships. We present Tangible Urban Models, an approach for leveraging the use of conductive material for 3D printed architectural prototypes. This enables non-interactive objects, such as buildings, to become tangible without the need to attach additional components. We combine this capability with an augmented reality (AR) app and explore the use of gestures for interacting with digital and physical content. The multi-material 3D printed buildings consist of thin layers of white plastic filament and a conductive wireframe to enable touch gestures. In this way, we enable a two-way interaction either with the physical model or with the mobile AR interface. © 2019 Copyright is held by the owner/author(s).",12,3D printers,Augmented reality - Conductive materials - Plastic filaments - Ubiquitous computing - User interfaces - Wearable computers,3-D printing - Architectural process - Interactive objects - Multi materials - Physical model - Tangible user interfaces - Thin layers - Two-way interaction,"708.2 Conducting Materials - 722.2 Computer Peripheral Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 745.1.1 Printing Equipment - 817.1 Polymer Products",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Satisfying a conversation with materials for dynamic fabrics,"Mackey, Angella (1, 3); Wakkary, Ron (1, 2); Wensveen, Stephan (1); Hupfeld, Annika (1); Tomico, Oscar (1, 4) ","(1) Eindhoven University of Technology, Eindhoven, Netherlands (2) Simon Fraser University, Surrey, Canada (3) Amsterdam University of Applied Sciences, Amsterdam, Netherlands (4) ELISAVA, Barcelona, Spain ",DIS 2019 - Proceedings of the 2019 ACM Designing Interactive Systems Conference,Adobe; Google; Sketch; UC San Diego's Design Lab; Virginia Tech,"Association for Computing Machinery, Inc",,p 1047-1058,18-Jun-19,DIS 2019 - Proceedings of the 2019 ACM Designing Interactive Systems Conference,2019,,,,9.78145E+12,10.1145/3322276.3322371,,"2019 ACM Conference on Designing Interactive Systems, DIS 2019","June 23, 2019 - June 28, 2019",,"Schön describes the way a designer engages with their materials as a 'conversation'. In clothing design this typically involves tangible and situated actions such as draping, ripping, and cutting-actions that evoke responses from the fabric at hand. Dynamic fabrics-surface-changing fabrics that combine digital and physical states-are still novel fashion-design materials. When working with the digital, intangible qualities of these fabrics, how does a dialogue unfold for designers accustomed to working physically with fabrics? In this paper we examine the design process of Phem, a collection of garments that use dynamic fabrics that function similarly to augmented reality. We reflect upon the improvisations required to satisfy a productive dialogue with the digital forms of these materials. We conclude with a discussion that proposes revisiting Schön's notion of a conversation in the context of digital forms, and use Ingold's perspectives on making to inform this inquiry. © 2019 Copyright is held by the owner/author(s).",42,Hosiery manufacture,Augmented reality - Wearable technology,Clothing design - Design process - Digital forms - Fashion design - Materiality - Physical state - Situated actions - Surface changing,"723 Computer Software, Data Handling and Applications - 819.5 Textile Products and Processing",,,,We would like to thank the reviewers for their insight and feedback. This research was supported by Philips Lighting and the H2020 Marie Sk&lstrok;odowska-Curie Actions grant agreement No. 642328.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Virtual, Augmented, and Mixed Reality for Human-Robot Interaction (VAM-HRI)","Williams, T. (1); Szafir, D. (2); Chakraborti, T. (3); Phillips, E. (4) ","(1) Colorado Sch. of Mines, Golden, CO, United States (2) Univ. of Colorado Boulder, Boulder, CO, United States (3) IBM Res. AI, Cambridge, MA, United States (4) U.S. Air Force Acad., Colorado Springs, CO, United States ",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE Robot. & Autom. Soc.,"IEEE, Piscataway, NJ, USA",,671-2,2019,,,,,,978-1-5386-8555-6,10.1109/HRI.2019.8673207,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,"Daegu, South Korea","The 2 nd International Workshop on Virtual, Augmented, and Mixed Reality for Human-Robot Interactions (VAM-HRI) will bring together HRI, Robotics, and Mixed Reality researchers to identify challenges in mixed reality interactions between humans and robots. Topics relevant to the workshop include development of robots that can interact with humans in mixed reality, use of virtual reality for developing interactive robots, the design of new augmented reality interfaces that mediate communication between humans and robots, comparisons of the capabilities and perceptions of robots and virtual agents, and best design practices. VAM-HRI was held for the first time at HRI 2018, where it served as the first workshop of its kind at an academic AI or Robotics conference, and served as a timely call to arms to the academic community in response to the growing promise of this emerging field. VAM-HRI 2019 will follow on the success of VAM-HRI 2018, and present new opportunities for expanding this nascent research community.",23,,augmented reality - control engineering computing - human-robot interaction - user interfaces,mixed reality interactions - interactive robots - virtual agents - human-robot interaction - virtual augmented and mixed reality - VAM-HRI,C3390 Robotics - C6130V Virtual reality - C6180 User interfaces - C7420 Control engineering computing,G05B15/00,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Demo: HiveTracker - 3D positioning for ubiquitous embedded systems,"Honnet, Cédric (1); Lopes, Gonçalo (2) ","(1) Sorbonne University, ISIR, CNRS, Paris, France (2) NeuroGEARS Ltd, London, United Kingdom ",UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,p 288-291,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,10.1145/3341162.3349295,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,"Recent advances in positional tracking systems for virtual and augmented reality have opened up the possibilities for ubiquitous motion capture technology in the consumer market. However, for many applications such as in performance art, athletics, neuroscience, and medicine, these systems remain too bulky, expensive, and limited to tracking a few objects at a time. In this work, we present a small wireless device that takes advantage of existing HTC Vive lighthouse tracking technology to provide affordable, scalable, and highly accurate positional tracking capabilities. This open hardware and open software project contains several elements, and the latest contributions described in this paper include: (1) a characterization of the optical distortions of the lighthouses, (2) a new cross-platform WebBLE interface, and (3) a real-time in-browser visualization. We also introduce new possibilities with an adaptive calibration to estimate transformation matrices of lighthouses, and an FPGA approach to improve precision and adaptability. Finally, we show how the new developments reduce setup costs and increase the accessibility of our tracking technology. © 2019 Copyright held by the owner/author(s).",8,Ubiquitous computing,Augmented reality - Embedded systems - Lighthouses - Linear transformations - Navigation - Open source software - Virtual reality - Wearable computers,3D tracking - Indoor positioning - Low costs - Open sources - Wireless devices,"402.4 Towers - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 921.3 Mathematical Transformations",,,"Number: ANR-11-LABX-65, Acronym: -, Sponsor: -; ","The authors would like to thank Adam Kampff, Yvonne Jansen and Alexis Polti for their support offered during the development of this project. We would also like to thank Julien Mellet for his early work on reconstruction algorithms, Chinmay Pendharkar for his vital WebBLE contribution, Olivia Seow and Chris Davis for their design and media expertise, and NeuroGEARS Ltd. for the financial support as well as the provided help. This project was partially funded by the Human Computer Interface Prize"" of the Hack-A-Day conference. Finally", this work was also partially performed within the Labex SMART (ANR-11-LABX-65)," supported by French state funds managed by the ANR under reference ANR-11-IDEX-0004-02.""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,
Eyewear 2019: Third workshop on eyewear computing - FOcUs: Social interactions,"Tag, Benjamin (1); Ward, Jamie A. (2); Uema, Yuji (3); Kunze, Kai (1) ","(1) Keio University, Japan (2) Goldsmiths University of London, United Kingdom (3) J!NS Inc, Japan ",UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,p 616-618,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,10.1145/3341162.3347761,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,"Computing devices worn on the human body have a long history in academic and industrial research, most importantly in wearable computing, mobile eye tracking, and mobile mixed and augmented reality. As humans receive most of their sensory input via the head, it is a very interesting body location for simultaneous sensing and interaction as well as cognitive assistance. Eyewear Computing devices have recently emerged as commercial products and can provide an research platform for a range of fields, including human-computer interaction, ubiquitous computing, pervasive sensing, psychology and social sciences. The proposed workshop will bring together researchers from a wide range of disciplines, such as mobile and ubiquitous computing, eye tracking, optics, computer vision, human vision and perception, usability, as well as systems research. This year it will also bring in researchers from psychology, with a focus on the social and interpersonal aspects of eyewear technology. The workshop is a continuation from 2016/2018 and will focus on discussing application scenarios as well as focusing on eyewear sensing and supporting social interactions. © 2019 Copyright held by the owner/author(s).",9,Ubiquitous computing,Augmented reality - Eye tracking - Human computer interaction - Industrial research - Social sciences computing - Wearable computers,Cognitive assistance - Context - Eyewear - Interpersonal aspects - Mixed and augmented realities - Mobile and ubiquitous computing - Sensing - Simultaneous sensing,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 901.3 Engineering Research",,,"Number: 18H03278, Acronym: JSPS, Sponsor: Japan Society for the Promotion of Science; Number: JPMJCR16E1, Acronym: JST, Sponsor: Japan Science and Technology Corporation; ","This workshop is based on a Dagstuhl Seminar on EyeWear Computing No. 16042 http://www.dagstuhl.de/16042, and provides a continuation to the successful workshops on Eyewear from the years 2016 and 2018. This work is also partly supported by JST CREST Grant No. JPMJCR16E1 and JSPS Grant No. 18H03278, as well as the Osaka University Grand Challenge Society 5.0.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Lessons learned building a secure network measurement framework using basic NDN,"Nichols, Kathleen (1) ","(1) Pollere, Inc., Montara; CA, United States ",ICN 2019 - Proceedings of the 2019 Conference on Information-Centric Networking,ACM SIGCOMM; City University of Hong Kong,"Association for Computing Machinery, Inc",,p 112-122,24-Sep-19,ICN 2019 - Proceedings of the 2019 Conference on Information-Centric Networking,2019,,,,9.78145E+12,10.1145/3357150.3357397,,"6th ACM Conference on Information-Centric Networking, ICN 2019","September 24, 2019 - September 26, 2019",,"The Named-Data Networking Project has moved from a multi-university NSF-funded Future Internet Architecture project to an open source codebase with world wide contributors and a growing body of applications. Researchers have applied NDN to applications like lighting control, vehicular communications, and augmented reality but more work is needed to make the data-centric and security features of NDN accessible. Users are currently required to become experts on the internals of the codebase, a difficult task further complicated by the lack of well-documented examples and the project adding new features. While implementing a secure, distributed network measurement framework for NDN, we encountered two major difficulties: the lack of a library of application-usable communications models (built on top of the NDN layer) and the difficulty of integrating trust rules with the NDN codebase. This paper describes our NDN network measurement framework and the co-developed tools that implement its secure, publish/subscribe communications model. Our goals are both to present the network measurement framework and to motivate developers to evolve NDN by creating frameworks, libraries, and includible headers rather than bloating NDN’s waist. © 2019 Copyright held by the owner/author(s).",41,Network security,Augmented reality,Communications modeling - Distributed networks - Future internet architecture - Named data networkings - Network measurement - Transport - Trust schema - Vehicular communications,"723 Computer Software, Data Handling and Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A theoretical framework to study long-term use of smart eyewear,"Zuidhof, Niek (1); Allouch, Somaya Ben (2); Peters, Oscar (1); Verbeek, Peter-Paul (3) ","(1) Saxion University of Applied Sciences, Enschede, Netherlands (2) University of Amsterdam, Netherlands (3) University of Twente, Enschede, Netherlands ",UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,p 667-670,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,10.1145/3341162.3348382,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,"Wearable displays with augmented reality are in an early phase of adoption by the public. The uptake is slow and user studies regarding acceptance and use are scarce and have limitations to understand long-term use and their influence on daily life. The complexity of understanding long-term use requires a multidisciplinary approach to different stakeholder perspectives. A variety of complementary theoretical perspectives are needed to create a framework to study the impact and perils of the use of smart eyewear. In this position paper, we develop such a framework, integrating theoretical perspectives from Philosophy, Psychology, Science and Technology Studies and Information Systems. The resulting framework has concrete implications for future studies regarding the design, acceptance and long-term use of smart eyewear. © 2019 Association for Computing Machinery.",28,Ubiquitous computing,Augmented reality - Human computer interaction - Wearable computers,Acceptance - Adoption - Implications for futures - Mediation - Multi-disciplinary approach - Science and technology studies - Theoretical framework - Wearable displays,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Collaborative hands-on training on haptic simulators,"Licona, Angel R.R. (1); Liu, Fei (1); Lelevé, Arnaud (1); Pham, Minh Tu (1) ","(1) Université de Lyon, INSA Lyon, Ampère, Villeurbanne; F69621, France ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 39-45,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332318,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"Medical trainees are required to acquire sufficient skills before touching a real patient. Nowadays, haptic simulators provide an effective solution but they do not facilitate an active supervision by a trainer who should show the right gestures in terms of motions and forces to apply, in the simulated environment. Dual user training systems aim at this purpose. Even though they permit a cooperative training, they generally dot not enable efficient demonstration/evaluation modes where the user who observes the person performing a manipulation is also able to feel the interaction forces, not only the motion. We earlier introduced the Energy Shared Control (ESC) architecture aiming at providing the latter function. It is modeled with the Port Hamiltonian framework and it embeds a Time Domain Passivity Controller, to compose a one degree-of-freedom (dof) dual-user haptic system for hands-on training. In this paper, we extend it to three dof with three identical haptic devices. Experiments bring information about its performance. © 2019 Association for Computing Machinery.",14,Hamiltonians,Augmented reality - Degrees of freedom (mechanics) - Personnel training,Degree of freedom (dof) - Dual-user teleoperation - Effective solution - Haptics - Interaction forces - Simulated environment - Time domain passivity - Training Systems,"723 Computer Software, Data Handling and Applications - 912.4 Personnel - 931.1 Mechanics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Virtual reality for anatomical vocabulary learning,"Yossatorn, Yossiri (1); Nimnual, Ratchadawan (2) ","(1) Navamindradhiraj University (NMU), 131/6 Khao Rd., Wachira Phayaban Dusit, Bangkok; 10300, Thailand (2) King Mongkut’s University of Technology Thonburi (KMUTT), 126 Pracha Uthit Rd., Bang Mod, Thung Khru, Bangkok; 10140, Thailand ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 16-20,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332311,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"Virtual Reality has become popularly pervasive among people in the digital society. The prevalent availability and usefulness of VR lend facilitations to diverse disciplines, especially medicine. The current study developed a Virtual Reality (VR) anatomical vocabulary learning program. The anatomical contents were developed by using three-dimensional (3D) and game-engine softwares. The developed program was operated on a high-performance personal computer and presented to the participants through fully immersive VR gears: head-mounted display and sensory handheld remote controls. The effectiveness of the program system and usage satisfaction were examined by medical and VR experts, and thirty health practitioners. The system program was assessed to be at a highest level (x = 4.65). The operating system and satisfaction were highly rated (x = 4.57). The results indicated that the program development in the study is practical and applicable to educational training and future research. © 2019 Association fo rComputing Machinery.",19,Virtual reality,Augmented reality - E-learning - Helmet mounted displays - Personal computers - Remote control,Anatomy - Digital society - Head mounted displays - High-performance personal computers - Program development - Program systems - Threedimensional (3-d) - Vocabulary learning,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 731.1 Control Systems",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Therapeutic virtual reality for nyctophobic disorder,"Nimnual, Ratchadawan (1); Yossatorn, Yossiri (2) ","(1) King Mongkut’s University of Technology Thonburi (KMUTT), 126 Pracha Uthit Rd., Bang Mod, Thung Khru, Bangkok; 10140, Thailand (2) Navamindradhiraj University (NMU), 131/6 Khao Rd., Wachira Phayaban Dusit, Bangkok; 10300, Thailand ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 11-15,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332310,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"Virtual Reality has become prevalent and available in the present digital era. The technological advancement generates not only beneficial but also apprehensive consequences, such as life threatening and mental hallucination. Psychological anxious disorders, for example fear of darkness and height, have been found to be an impediment in living among societies. In order to lend an assistance to alleviate these hindering symptoms, the current study developed a Virtual Reality (VR) based therapy for a nyctophobic disorder. The therapeutic program was developed by using three-dimensional (3D) and game-engine softwares. The developed system was operated on a high-performance personal computer and presented to users through fully immersive VR gears: head-mounted display and sensory handheld remote controls. The effectiveness of the program system and usage satisfaction were examined by medical and VR experts, and twenty diagnosed individuals with a nyctophobic disorder. The system program was assessed to be at a highest level (x = 4.56). The operating system and satisfaction were highly rated (x = 4.60). The results indicated that the program development in the study is practical for treating nyctophobia and can be applicable to future research. © 2019 Association fo rComputing Machinery.",25,Virtual reality,Augmented reality - Helmet mounted displays - Personal computers - Remote control,Anxious disorders - Head mounted displays - High-performance personal computers - Nyctophobia - Program development - Technological advancement - Therapy - Threedimensional (3-d),"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 731.1 Control Systems",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Adaptive aesthetic photo filter by deep learning,"Tang, Zineng (1) ","(1) UNC at chapel hill, United States ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 67-72,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332323,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"The paper proposes a deep end-to-end model with full differentiability, FilterNet, for image enhancement that could optimize the image filter parameters and recommend the best photo filter for a given image to achieve optimum aesthetic effect. The model learns the aesthetic distribution of images from evaluation network that is pretrained and identifies the binary aesthetic quality of an image, similar to the structure of GAN (generative adversarial network). The proposed model is weakly-supervised and one stage and accompanied by new training methods compared to other state of art models and conditional inputs that help the model to be more content-aware, yielding competitive results compared to professional photo editing. The performance of FilterNet is evaluated on both deep learning and traditional methods including online user studies. © 2019 Association for Computing Machinery.",27,Deep learning,Adaptive filtering - Augmented reality - Image enhancement - Quality control,Adaptation - Aesthetic - AVA Dataset - Filternet - Photo Filter - Recommendation,"723 Computer Software, Data Handling and Applications - 913.3 Quality Assurance and Control",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
IMWeb: Cross-platform immersive web browsing for online 3D neuron database exploration,"Fulmer, Willis (1); Zhang, Shaoting (1); Mahmood, Tahir (1); Huang, Jian (2); Li, Zhongyu (1); Lu, Aidong (1) ","(1) University of North Carolina, Charlotte; NC, United States (2) University of Tennessee, Knoxville; TN, United States ","International Conference on Intelligent User Interfaces, Proceedings IUI",ACM Special Interest Group on Artificial Intelligence (SIGAI); ACM Special Interest Group on Computer-Human Interaction (SIGCHI),Association for Computing Machinery,v Part F147615,p 367-378,2019,,2019,,,,,10.1145/3301275.3302319,,"24th ACM International Conference on Intelligent User Interfaces, IUI 2019","March 17, 2019 - March 20, 2019",,"Web services have become one major way for people to obtain and explore information nowadays. However, web browsers currently only offer limited data analysis capabilities, especially for large-scale 3D datasets. This project presents a method of immersive web browsing (ImWeb) to enable effective exploration of multiple datasets over the web with augmented reality (AR) techniques. The ImWeb system allows inputs from both the web browser and AR and provides a set of immersive analytics methods for enhanced web browsing, exploration, comparison, and summary tasks. We have also integrated 3D neuron mining and abstraction approaches to support efficient analysis functions. The architecture of ImWeb system flexibly separates the tasks on web browser and AR and supports smooth networking among the system, so that ImWeb can be adopted by different platforms, such as desktops, large displays, and tablets. We use an online 3D neuron database to demonstrate that ImWeb enables new experiences of exploring 3D datasets over the web. We expect that our approach can be applied to various other online databases and become one useful addition to future web services. © 2019 Association for Computing Machinery.",41,Web browsers,Augmented reality - Database systems - Large dataset - Neurons - User interfaces - Web services - Websites,Analysis capabilities - Cross-platform - Efficient analysis - Immersive - Large displays - Multiple data sets - Online database - Visual analytics,"461.9 Biology - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.3 Database Systems",,,"Number: 1564039, Acronym: NSF, Sponsor: National Science Foundation; Number: 1629913, Acronym: NSF, Sponsor: National Science Foundation; Number: 1840080, Acronym: NSF, Sponsor: National Science Foundation; ","This work was supported by the National Science Foundation under Grant Nos. 1564039, 1629913, and 1840080.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Autonomous racing: A comparison of SLAM algorithms for large scale outdoor environments,"Nobis, Felix (1); Betz, Johannes (1); Hermansdorfer, Leonhard (1); Lienkamp, Markus (1) ","(1) Technical University of Munich, Germany ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 82-89,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332319,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,The task of simultaneous localization and mapping (SLAM) is a widely studied field in robotics research in the last decades. The goal of SLAM is to create an accurate map of the environment considering uncertainties in the pose as well as the environmental perception of the robot. Historically SLAM algorithms are applied in the field of indoor robotics. Recent developments in the area of autonomous driving surge a focus for SLAM applications in large scale outdoor environments. Two notable open source SLAM software packages are Gmapping and Google Cartographer. This paper focuses on a qualitative comparison of the aforementioned algorithms for such a scenario. We discuss the underlying algorithmic differences of the two packages. This serves as the foundation to present the SLAM results for different parameter configurations. We evaluate the accuracy of the resulting maps and the respective computational limitations. The maps are further evaluated against manually measured ground truth track boundaries. We show that the existing approaches can be adapted to large-scale outdoor environments. © Association for Computing Machinery.,25,Robotics,Augmented reality - Autonomous vehicles - Indoor positioning systems - Open source software - Open systems - Optical radar - Racing automobiles - Sensory perception,Autonomous driving - Cartographer - Gmapping - Large scale SLAM - Sparse features,"461.4 Ergonomics and Human Factors Engineering - 662.1 Automobiles - 716.2 Radar Systems and Equipment - 723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Omnidirectional Transport System for Classification and Quality Control using Artificial Vision,"Salazar, Erick Barrionuevo (1); Escudero, Bryan Navas (1); Rea Minango, Sylvia Nathaly (1) ","(1) Universidad de las Fuerzas Armadas ESPE, Av. General Rumiñahui s/n, Sangolquí, Ecuador ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 62-66,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332321,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"Classification and transport of material is a crucial stage in the manufacture of parts and represents a large portion of the lead time in a production line, which is why optimization is crucial. This work deals with an omnidirectional transport mechanism for classification and quality control of parts coming from rapid prototyping processes. Said mechanism is constituted by an artificial vision system, which will be responsible for taking the necessary information to perform the classification and quality control using a neural network; and, a matrix of omnidirectional wheels that allows the movement of the piece on the XY plane. The purpose of this investigation was to demonstrate that omnidirectional mechanisms can also be used to transport and classify parts within industrial processes, being another alternative of use to conventional systems. The system is able to classify three types of pieces of different forms, sizes and perspectives with high reliability and speed; it also allows a better human-machine interaction due to a graphical interface where the performed processes are detailed. © 2019 Association for Computing Machinery.",6,Quality control,Augmented reality - Classification (of information) - Computer vision - Conveyors - Quality assurance - Vision,Artificial vision system - Conventional systems - Human machine interaction - Industrial processs - Omnidirectional mechanisms - Omnidirectional wheel - Transport mechanism - Transport systems,"692.1 Conveyors - 716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 913.3 Quality Assurance and Control",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Development of dual cognitive task virtual reality game addressing stroke rehabilitation,"Mohd Hashim, Siti Hazyanti (1); Ismail, Marina (1); Manaf, Haidzir (2); Hanapiah, Fazah Akhtar (3) ","(1) Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia (2) Faculty of Health Science, Universiti Teknologi MARA Puncak Alam, Selangor, Malaysia (3) Faculty of Medicine, Universiti Teknologi MARA, Sg. Buloh Selangor, Malaysia ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 21-25,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332312,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"Components of stroke rehabilitation requires the input of dual cognitive tasks, which involves the multitask of both motor and cognitive skills simultaneously. An individual who had suffered a stroke may have problems executing dual cognitive task. Rehabilitation can help to improve the function of dual cognitive tasking. Games have been used for rehabilitation intensely. Virtual reality games have been acknowledged to assist the stroke patient in the rehabilitation process. However, stroke rehabilitations require special virtual reality game design for the patient to perform the dual cognitive task. A study has been conducted to determine the criteria and components that are directed for dual cognitive task for patients with stroke. The information was gathered through literature review and expert opinion from both the game experts, rehabilitation physicians and therapists. This paper will discuss the development of the virtual reality game incorporating dual cognitive task for stroke rehabilitation based on the principles and elements that has been identified. The focus of the study is to develop a virtual reality game on dual cognitive task for stroke patients that combine all the game elements, game type, virtual reality environment and also game theory for dual cognitive task rehabilitation. The adoption of the correct rehabilitation elements and principles in the design and development of a virtual reality game addressing dual cognitive task will be able to assist the stroke patient in the rehabilitation process. © 2019 Association fo rComputing Machinery.",18,Electronic medical equipment,Augmented reality - Game theory - Neuromuscular rehabilitation - Serious games - Virtual reality,Cognitive skill - Cognitive task - Design and Development - Expert opinion - Literature reviews - Stroke patients - Stroke rehabilitation - Virtual-reality environment,"461.5 Rehabilitation Engineering and Assistive Technology - 462.1 Biomedical Equipment, General - 723 Computer Software, Data Handling and Applications - 922.1 Probability Theory",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ASV: Accelerated stereo vision system,"Feng, Yu (1); Whatmough, Paul (2); Zhu, Yuhao (1) ","(1) University of Rochester, United States (2) Arm Research, United States ","Proceedings of the Annual International Symposium on Microarchitecture, MICRO",AMD; arm; et al.; HUAWEI; IBM; Microsoft,IEEE Computer Society,,p 643-656,12-Oct-19,"MICRO 2019 - 52nd Annual IEEE/ACM International Symposium on Microarchitecture, Proceedings",2019,,10724451,,9.78145E+12,10.1145/3352460.3358253,,"52nd Annual IEEE/ACM International Symposium on Microarchitecture, MICRO 2019","October 12, 2019 - October 16, 2019",,"Estimating depth from stereo vision cameras, i.e., depth from stereo, is critical to emerging intelligent applications deployed in energy- and performance-constrained devices, such as augmented reality headsets and mobile autonomous robots. While existing stereo vision systems make trade-offs between accuracy, performance and energy-efficiency, we describe ASV, an accelerated stereo vision system that simultaneously improves both performance and energy-efficiency while achieving high accuracy. The key to ASV is to exploit unique characteristics inherent to stereo vision, and apply stereo-specific optimizations, both algorithmically and computationally. We make two contributions. Firstly, we propose a new stereo algorithm, invariant-based stereo matching (ISM), that achieves significant speedup while retaining high accuracy. The algorithm combines classic hand-crafted stereo algorithms with recent developments in Deep Neural Networks (DNNs), by leveraging the correspondence invariant unique to stereo vision systems. Secondly, we observe that the bottleneck of the ISM algorithm is the DNN inference, and in particular the deconvolution operations that introduce massive compute-inefficiencies. We propose a set of software optimizations that mitigate these inefficiencies. We show that with less than 0.5% hardware area overhead, these algorithmic and computational optimizations can be effectively integrated within a conventional DNN accelerator. Overall, ASV achieves 5 speedup and 85% energy saving with 0.02% accuracy loss compared to today's DNN-based stereo vision systems. © 2019 Association for Computing Machinery.",78,Stereo vision,Augmented reality - Computer architecture - Constrained optimization - Deconvolution - Deep neural networks - Economic and social effects - Energy efficiency - Inference engines - Intelligent robots - Mobile computing - Stereo image processing,Computational optimization - Data flow - Depth from stereo - Intelligent applications - Mobile autonomous robots - Software optimization - Stereo vision system - Tiling,"525.2 Energy Conservation - 723 Computer Software, Data Handling and Applications - 731.6 Robot Applications - 921 Mathematics - 961 Systems Science - 971 Social Sciences",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Development of automated platform for image capturing and counting algorithm for viral plaque,"Premsattham, Pannawich (1); Salakij, Saran (1); Boonyasuppayalorn, Siwaporn (1); Phanomchoeng, Gridsada (2) ","(1) Department of Mechanical Engineering, Chulalongkorn University, Bangkok; 10300, Thailand (2) Department of Mechanical Engineering, Smart Wireless Communication Ecosystem Research Group, Chulalongkorn University, Bangkok; 10300, Thailand ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 52-56,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332316,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"Automatic analyzing tools are essential for biomedical research such as evaluating quantitation of virion. Counting dengue 96-well plaque assay with automated analysis system is one of the interesting and important applications since it can reduce researchers’ burden and help researchers analyze the results quickly. Thus, automated platform for image capturing and counting algorithm for viral plaque have been developed. The automated platform for image capturing consists of xy-table, which have work space of 220 mm x 220 mm,accuracy of 0.05 mm, and vertical z-axis for image focusing. With the automated platform, each well of 96-well plate can be automatically captured with high resolution. Next, the counting algorithm is a program for counting and analyzing the images. The algorithm is developed from Halcon library. The results show that the accuracy of the counting algorithm is good enough for the application. Moreover, with this system, there is no additional process of nontransparent liquid to achieve contrast-enhanced image during image capturing. Thus, this system can reduce workload of researchers during assay preparation. © 2019 Association for Computing Machinery.",13,Image enhancement,Augmented reality - Automation - Image segmentation - Watersheds,Automated analysis systems - Biomedical research - Contrast-enhanced images - High resolution - Image counting - Nontransparent liquids - Viral plaque - Well plates,"444.1 Surface Water - 723 Computer Software, Data Handling and Applications - 731 Automatic Control Principles and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Computer system prototype for qualitative and quantitative evaluation of selected movement activities,"Hachaj, Tomasz (1); Ogiela, Marek R. (2) ","(1) Institute of Computer Science, Pedagogical University of Cracow, Cracow, Poland (2) Cryptography and Cognitive Informatics Research Group, AGH University of Science and Technology, Krakow, Poland ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 73-76,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332309,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"The aim of this paper is to propose prototype of a computer system for complex analysis of motion kinematic including data acquisition, processing, classification to one of the previously created classes and finally analysis based on comparison of input signal to motion template. We have used wireless inertial measurement unit sensors based costume, Gesture Description Language motion segmentation module, Hidden Markov Models for classification, and Dynamic Time Warping for comparison of acquired motions to actions templates. We have evaluated the proposed solution on karate motions dataset containing 8 motion classes. We have obtained total recognition rate 0.92. The analysis done by an algorithm has been evaluated by an expert. All differences indicated by our method between templates and recordings were approved by an expert and judged as meaningful. © 2019 Copyright is held by the owner/author(s). Publication rights",19,Hidden Markov models,Augmented reality - Data acquisition - Data handling - Motion analysis,Description languages - Dynamic time warping - Inertial measurement unit - Motion capture - Motion classification - Motion kinematics - Motion segmentation - Quantitative evaluation,"723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 922 Statistical Methods",,,"Number: 2015/17/D/ST6/04051, Acronym: NCN, Sponsor: Narodowe Centrum Nauki; ","This work has been supported by the National Science Centre, Poland, under project number 2015/17/D/ST6/04051.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
PD Pattern Recognition in Transformers Based on Grey-scale Images and Affinity Propagation Algorithm,"Wei, Bengang (1); Huo, Kaixuan (2); Yao, Zhoufei (1); Lou, Jie (2); Li, Xiangyao (2) ","(1) Electric Power Research Institute, Shanghai Electric Power Company, No.171 Handan Road, Shanghai, China (2) School of Electrical Engineering, Shandong University, No.17923 Road, Jinan, Shandong Province, China ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 46-51,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332315,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"Partial discharge (PD) pattern recognition plays an important role in determining insulation defects and understanding insulation condition of transformers. In this paper, four PD models are set up in laboratory and pulse current method is used to measure the amplitude of apparent charge, the power frequency phase of PD pulses and the frequency of PD pulses. There are 19 feature parameters which include fractal features, moment features, and textural features are extracted form grey-scale images of phase resolved partial discharge (PRPD) patterns. In order to reduce the computational complexity of the classifier, principal component analysis (PCA) is used to reduce the dimensions of feature parameters and five new feature parameters which explain 93.50% of total variance are obtained. The kernel function and shared nearest neighbors (SNN) are used to improve affinity propagation (AP) algorithm. A classifier based on improved AP with particle swarm optimization (PSO) is established for PD pattern recognition in transformers. Based on these new feature parameters, the PD patterns are recognized by AP classifier, back propagation neural networks (BPNN) and least squares support vector machine (LSSVM). Recognition rate of AP classifier is 85%. © 2019 Association for Computing Machinery.",15,Pattern recognition,Augmented reality - Backpropagation - Neural networks - Partial discharges - Particle swarm optimization (PSO) - Principal component analysis - Support vector machines,Affinity propagation - Back-propagation neural networks - Grey scale images - Insulation conditions - Kernel function - Least squares support vector machines - Phase resolved partial discharge patterns - Shared nearest neighbors,"701.1 Electricity: Basic Concepts and Phenomena - 723 Computer Software, Data Handling and Applications - 723.4 Artificial Intelligence - 922.2 Mathematical Statistics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Thai license plate recognition algorithm with service routine procedure for automatic barrier gate,"Toolpeng, Sirawit (1); Wannapoporn, Kittipak (1); Phanomchoeng, Gridsada (2) ","(1) Department of Mechanical Engineering, Chulalongkorn University, Bangkok; 10300, Thailand (2) Department of Mechanical Engineering, Smart Wireless Communication Ecosystem Research Group, Chulalongkorn University, Bangkok; 10300, Thailand ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 77-81,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332314,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"This paper presents a development of a Thai license plate recognition algorithm for an automatic barrier gate. The designed algorithm is developed base on MVTec Halcon image processing library. The algorithm is robust and practical for different environment. Also, this algorithm is developed for a barrier gate at faculty of engineering Chulalongkorn University, Thailand, to authorize the registered cars for entering the faculty. The barrier gate system gives convenience and safety to the drivers and saves cost for the faculty. Base on experiments, the accuracy of license plate segmentation is 100% and the accuracy rate from multilayer perceptron method of the Thai license plate recognition algorithm is 100% due to the number of samples which might not support every type of variation. However, the error could be fixed by service routine procedure. Also, the service routine procedure improves the Thai license plate recognition algorithm by updating training data. © 2019 Association for Computing Machinery.",8,License plates (automobile),Augmented reality - Image processing - Multilayer neural networks - Multilayers - Optical character recognition - Optical data processing - Optical multilayers - Support vector machines,Accuracy rate - Chulalongkorn University - Image processing libraries - License plate recognition - License plate segmentations - Number of samples - Thai Characters - Training data,"662.1 Automobiles - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 741.1 Light/Optics - 741.3 Optical Devices and Systems",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
AR/VR strategy considerations for academic computing services,"McGrath, Owen G. (1) ","(1) Educational Technology Services, UC Berkeley, Berkeley; CA, United States ",Proceedings ACM SIGUCCS User Services Conference,ACM SIGUCCS,Association for Computing Machinery,,p 15-18,26-Oct-19,SIGUCCS 2019 - Proceedings of the 2019 ACM SIGUCCS Annual Conference,2019,,,,9.78145E+12,10.1145/3347709.3347783,,"47th ACM SIGUCCS Annual User Services Conference, SIGUCCS 2019","November 3, 2019 - November 6, 2019",,"As higher education institutions take up augmented and virtual reality (AR/VR) tools and applications, important choices and challenges arise in deciding how to approach the planning and design of infrastructure, spaces, and the services to be offered. The observations in this paper are based upon early experiences of growing AR/VR spaces and support in one university setting. The emerging services, space designs, and choice of technology and training offered are informed both by consideration of the varied AR/VR environments being explored in the curriculum as well as a general interest in supporting AR/VR use by individuals and groups in non-curricular settings. The goals of this paper include: 1) briefly surveying commonalities and differences of AR and VR as they’re being applied to teaching and learning; 2) considering service design approaches and decision points around AR/VR technologies; 3) and noting challenges for developing sustainable approaches to AR/VR support. © 2019 Association for Computing Machinery.",15,Virtual reality,Augmented reality,Accessibility - Augmented and virtual realities - Higher education - Higher education institutions - Planning and design - Teaching and learning - Tools and applications - University settings,"723 Computer Software, Data Handling and Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Research of cyclone optimization based on CFD, GMDH-type neural network and genetic algorithm","Park, Donggeun (1) ","(1) Department of Advanced Materials and Parts of Transportation Systems, Pusan National University, Busan; KS012, Korea, Republic of ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 90-96,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332322,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"Gas cyclone has two main parameters for evaluating separation performance, separation efficiency and pressure drop through cyclone. They are closely influenced by the geometrical design variables of the cyclone. This study performed optimization of the cyclone performance for the cyclone shape based on computational fluid dynamics (CFD), GMDH type neural network and genetic algorithm (GA). First, CFD was used to obtain the data of the cyclone performance parameters. As result of the CFD validation, the errors of the reference model and CFD were 0.5 % and 2 % for the pressure drop and the separation efficiency. Secondly, the meta-model of the cyclone performance was derived by using GMDH algorithm based on supervised learning of machine learning. The fitness of the modelling results was shown using the correlation coefficient. As results of the GMDH, the correlation coefficients of meta models of the separation efficiency and the pressure drop were 98.9 %, 99.7 %, respectively. Finally, we performed optimization of the meta model by applying GA. When the optimal point was compared with the reference model, the performance of the optimal point was improved by 24.31 % and 8.32 % for pressure drop and the separation efficiency, respectively. © 2019 Association for Computing Machinery.",12,Computational fluid dynamics,Augmented reality - Cyclone separators - Drops - Efficiency - Genetic algorithms - Machine learning - Neural networks - Optimization - Pressure drop - Separation,Correlation coefficient - Geometrical designs - GMDH-type neural networks - Main parameters - Performance parameters - Reference modeling - Separation efficiency - Separation performance,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 802.1 Chemical Plants and Equipment - 802.3 Chemical Operations - 913.1 Production Engineering - 921.5 Optimization Techniques",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A new method to render virtual walls for haptic systems: 'Tracking wall'. Application to needle insertion simulation,"Ma de los Angeles Alamilla, D. (1); Moreau, Richard (1); Redarce, Tanneguy (1) ","(1) University of Lyon, INSA Lyon, CNRS, Ampère, Villeurbanne; F-69621, France ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 33-38,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332317,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"For medical training, the use of haptic systems is a way to improve skills of novices, due to their capabilities to reproduce the natural gestures or replicate different environments. A simulator based on a haptic interface allows to reproduce the feeling of inserting a needle and render the different stiffness tissues while a needle is inserted. The majority of needle insertion simulators make use of prosthesis or limbs which limits the training procedures. Some simulators offer more realistic behavior using control laws based on active walls. However, problems occur with the speed estimation and the damping factor. To overcome this, an algorithm which can be used in a haptic simulator for the intraarticular punction procedure was developed. This algorithm allows rendering different tissues stiffness while a needle penetrates it avoiding speed estimation problems and rumbling phenomena caused by high damping factors. © 2019 Association for Computing Machinery",14,Haptic interfaces,Augmented reality - Control engineering - Damping - Histology - Needles - Simulators - Stiffness - Tissue,Haptic simulators - Hapticsystems - Medical robotics - Medical simulators - Medical training - Needle insertion - Speed estimation - Training procedures,"461.2 Biological Materials and Tissue Engineering - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 931.1 Mechanics - 951 Materials Science",,,"Number: ANR11-IDFI-0034, Acronym: ANR, Sponsor: Agence Nationale de la Recherche; ",The authors would like to thank the ANR (French National Research Agency) for financing SAMSEI project (ANR11-IDFI-0034) under the supervision of Pr. X. Martin.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Can a phone hear the shape of a room?,"Shih, Oliver (1); Rowe, Anthony (1) ","(1) Carnegie Mellon University, Pittsburgh; PA, United States ",IPSN 2019 - Proceedings of the 2019 Information Processing in Sensor Networks,ACM SIGBED; IEEE Signal Processing Society (SPS),"Association for Computing Machinery, Inc",,p 277-288,16-Apr-19,IPSN 2019 - Proceedings of the 2019 Information Processing in Sensor Networks,2019,,,,9.78145E+12,10.1145/3302506.3310407,,"18th ACM/IEEE International Conference on Information Processing in Sensor Networks, IPSN 2019","April 16, 2019 - April 18, 2019",,"Understanding the location of acoustically relective surfaces in a room is a critical component in advanced sound processing. For example, intelligent speakers can use a room’s acoustic geometry to improve playback quality, source separation accuracy, and speech recognition. In this paper, we present Synesthesia, a system for capturing the acoustic properties of a room using a single ixed speaker and a mobile phone that records audio at multiple locations. Using the arrival time of echoes, the system is able to reconstruct the position of relective surfaces like walls and then estimate properties like surface absorption. Previous work has shown how the acoustic room impulse response (RIR) of an environment can be used to analyze echoes within a space to reconstruct room geometry. The best current RIR-based approaches rely on high-end equipment and capturing an acoustic signal broadcast into space from a known ixed constellation of microphones. They also require the precise calibration and measurement of microphone positions. In addition, most approaches pose constraints on room geometries and limit the order of RIR to achieve accurate and consistent results. In this paper, we introduce a new approach that performs RIR imaging using a mobile phone that tracks its location with visual inertial odometry (VIO) to record a dense set of samples albeit with noise in their locations. We present a new approach that is able to relax several key assumptions on RIR and show through both experimentation and simulation that even with 20cm of uncertainty in the microphone locations provided by VIO, we are still able to reconstruct the room geometry with accurate shape and dimensions. We demonstrate this capability by prototyping a tool for acoustic engineers, that allows a user to view a room’s estimated geometry and absorption overlaid on the actual sensed space with augmented reality. © 2019 Association for Computing Machinery.",39,Audio acoustics,Acoustic properties - Augmented reality - Cellular telephones - Geometry - Impulse response - Location - Mapping - Microphones - Sensor networks - Source separation - Speech recognition,Acoustic room impulse response - Acoustic sensing - Calibration and measurements - Critical component - Microphone positions - Pose constraints - Sound processing - Surface absorption,"405.3 Surveying - 716.1 Information Theory and Signal Processing - 718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications - 751 Acoustics, Noise. Sound - 752.1 Acoustic Devices - 921 Mathematics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Designing for Impact: Shifting children's perspectives of civic and social issues through making mobile games,"Lamarra, Julie (1); Chauhan, Apoorva (2); Litts, Breanne (1) ","(1) Instructional Technology and Learning Sciences, Utah State University, Logan; UT, United States (2) Computer Science, Utah State University, Logan; UT, United States ","Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",Boise Osmo; Boise State University; et al.; Langan Barber Foundation; St. Luke's; STEM Action Center,"Association for Computing Machinery, Inc",,p 274-279,12-Jun-19,"Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",2019,,,,9.78145E+12,10.1145/3311927.3323338,,"18th ACM International Conference on Interaction Design and Children, IDC 2019","June 12, 2019 - June 15, 2019",,"Within the growing movement to teach children computational skills and practices, it is important to understand how children engage and identify with the content they are designing. In this paper, we explore how children's perspectives of civic and social issues shift or do not shift as they make a location-based mobile game using augmented reality and location-based mobile technologies. We conducted two workshops with children, where they individually or in pairs created a narrative-based game around civic and social engagement topics such as pollution, waste management, or animal rights. We present one illustrative case in this paper to highlight how mobile, augmented reality, and location-based mobile technologies afford impactful shifts in perspective. Findings indicate that these technologies may contribute to a shift in children's perspectives about the world around them and in some cases may prompt meaningful action towards civic engagement. © 2019 Copyright held by the owner/author(s).",16,Waste management,Augmented reality - Location - Social aspects - Telecommunication equipment,ARIS - Children - Civic engagement - Computational skills - Location based - Mobile games - Mobile Technology - Social engagement,"723 Computer Software, Data Handling and Applications - 901.4 Impact of Technology on Society",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Mixed reality for learning programming,"Kim, Joonyoung (1); Agarwal, Sudeep (1); Marotta, Kristina (1); Li, Siwei (1); Leo, Jonathan (1); Chau, Duen Horng (1) ","(1) Georgia Institute of Technology, Atlanta; GA, United States ","Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",Boise Osmo; Boise State University; et al.; Langan Barber Foundation; St. Luke's; STEM Action Center,"Association for Computing Machinery, Inc",,p 574-579,12-Jun-19,"Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",2019,,,,9.78145E+12,10.1145/3311927.3325335,,"18th ACM International Conference on Interaction Design and Children, IDC 2019","June 12, 2019 - June 15, 2019",,"We present our ongoing investigation into leveraging mixed reality (MR) to help students learn coding more easily and with more fun. We have developed an MR coding learning platform using Apple's ARKit 2 on iOS, with a physical user-configurable coding game board. Our approach could provide major benefits over conventional augmented reality (AR) approaches for learning coding and debugging: (1) allowing teachers to tailor the platform to their instructional needs, and spark creativity and engagement among students in designing programming problems that interest them; (2) enabling students to physically interact with a program, concretizing coding errors and providing real-time visual feedback to aid students' program understanding and reduce cognitive load. We present our preliminary results that uses ARKit's image tracking and object detection to enable core mixed-reality interaction capabilities on our platform. © 2019 Association for Computing Machinery.",7,Mixed reality,Augmented reality - Coding errors - Object detection - Program debugging - Students - Visual communication,ARKit - Coding - Learning platform - Learning programming - Program understanding - Programming education - Programming problem - Real time visual feedback,"717.1 Optical Communication Systems - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Broadening participation for remote communities: Situated distance telepresence,"Okundaye, Osazuwa (1); Quek, Francis (1); Chu, Sharon (2) ","(1) Texas A and M University, United States (2) University of Florida, United States ","Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",Boise Osmo; Boise State University; et al.; Langan Barber Foundation; St. Luke's; STEM Action Center,"Association for Computing Machinery, Inc",,p 494-500,12-Jun-19,"Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",2019,,,,9.78145E+12,10.1145/3311927.3325318,,"18th ACM International Conference on Interaction Design and Children, IDC 2019","June 12, 2019 - June 15, 2019",,"Our work is concerned with how embodied communication involving speech and gestures may be mediated through mobile tele-robotics and augmented reality to support hands-on distance mentoring. Following work in the psycholinguistics of embodied communication (e.g., meaning is expressed through gesture, gaze, and speech), a four design-implement-test-deploy-evaluate study was undertaken. We investigated whether and how powerful multimodal language to support explanation and mentoring may be mediated over distance through the designs. © 2019 Association for Computing Machinery.",36,Speech communication,Augmented reality - Visual communication,Apprenticeship - Communities of Practice - Maker movement - Public schools - Zone of proximal development,"717.1 Optical Communication Systems - 723 Computer Software, Data Handling and Applications - 751.5 Speech",,,"Number: -, Acronym: NSF, Sponsor: Norsk Sykepleierforbund; ","This project is partially supported by NSF grant Making in the Colonias-Motivating STEM Participation through a Making as Micro-Manufacturing Model, DRL 1623543'.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ARCat: A tangible programming tool for DFS algorithm teaching,"Deng, Xiaozhou (1); Wang, Danli (1); Jin, Qiao (1); Sun, Fang (2) ","(1) State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, School of Computer and Control Engineering, University of Chinese, Academy of Sciences, Beijing, China (2) School of Computer and Information Technology, Liaoning Normal University, Dalian, China ","Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",Boise Osmo; Boise State University; et al.; Langan Barber Foundation; St. Luke's; STEM Action Center,"Association for Computing Machinery, Inc",,p 533-537,12-Jun-19,"Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",2019,,,,9.78145E+12,10.1145/3311927.3325308,,"18th ACM International Conference on Interaction Design and Children, IDC 2019","June 12, 2019 - June 15, 2019",,"In this paper we present ARCat, a tangible programming tool designed to help children learn Depth First Search (DFS) algorithm with augmented reality (AR) technology. With this tool, children could use tangible programming cards to control a search process, rather than control virtual characters directly. With the special design of card semantics and real-time feedback, the cognitive load of the learning process had been proved to be affordable to children (ages 8-9) with the result of our preliminary evaluation, which shows the possibility of basic algorithm education for young children with tangible interface. © 2019 Association for Computing Machinery.",10,Process control,Augmented reality - Learning algorithms - Semantics - User interfaces,Algorithm educations - Computational thinkings - Depth-First-Search (DFS) - Real-time feedback - Tangible interfaces - Tangible programming - Tangible user interfaces - Virtual character,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: 61272325, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; Number: 61501463, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; Number: 61562063, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; Number: 61672507, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; Number: 61872363, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; ","This research is supported by the National Key Research and Development Program under Grant No.2016YFB0401202, and the National Natural Science Foundation of China under Grant No. 61872363, 61672507, 61272325, 61501463 and 61562063.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The limitations of reality,"Marshall, Joe (1); Tennent, Paul (1) ","(1) Mixed Reality Lab., School of Computer Science, University of Nottingham, Nottingham, United Kingdom ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,19-Nov-19,HTTF 2019 - Proceedings of the Halfway to the Future Symposium 2019,2019,,,,9.78145E+12,10.1145/3363384.3363475,3363475,"2019 Halfway to the Future Symposium: Exploring the Past, Present, and Future of HCI and Design-Based Research, HTTF 2019","November 19, 2019 - November 20, 2019",,"Existing conceptualisations of mixed reality technologies embody a fundamental assumption, that they are a tool to simulate consistent ‘realities’. We present three examples of how this assumption constrains designers: • Systems stimulate only a subset of senses. Aiming for consistent simulation ignores exciting potential to use externally stimulated senses in new ways. • Deliberately breaking convincing simulated reality can create new, thrilling experiences. • Compelling experiences can be create when people interacting in ways which break boundaries of conceptual realities. One way to avoid these issues is to define such ‘immersive technologies’ purely as motion tracked sensory stimulation devices. This offers a basic building block for design, however in the the future a range of higher level concepts will be required to enable new and exciting uses of VR and AR equipment. Convincing simulation of realities is only a small subset of the potential uses. © 2019 Copyright held by the owner/author(s).",12,Mixed reality,Augmented reality - Virtual reality,Basic building block - Immersive technologies - Mixed reality technologies - Sensory stimulation - Simulated reality,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Immersive Intelligence Genomic Data Visualisation,"Lau, Chng Wei (1); Nguyen, Quang Vinh (2); Qu, Zhonglin (1); Simoff, Simeon (2); Catchpoole, Daniel (3) ","(1) School of Computing, Engineering and Mathematics, Western Sydney University, NSW, Australia (2) MARCS Institute, School of Computing, Engineering and Mathematics, Western Sydney University, NSW, Australia (3) Kids Research Institute, Children's Hospital at Westmead, Westmead; NSW, Australia ",ACM International Conference Proceeding Series,CORE - Computing Research and Education; Macquarie University-Sydney,Association for Computing Machinery,,,29-Jan-19,"Proceedings of the Australasian Computer Science Week Multiconference, ACSW 2019",2019,,,,9.78145E+12,10.1145/3290688.3290722,a18,"2019 Australasian Computer Science Week Multiconference, ACSW 2019","January 29, 2019 - January 31, 2019",,"Genomics data are very complex and could contain crucial information about a disease or how a treatment method may perform well on one but not on another. Understanding such genomic data would enable better insight into the correlation between genes and diseases, which could facilitate personalised treatments for the patients. Although visualisations have been increasingly used in the genomic analysis, there is still limited research work on interactive visualisations on immersive platforms, such as in Augmented and Virtual Reality. This paper presents a new interactive visualisation and navigation of genomics data in such environments. We provide an overview of the patient cohort in 3D genetic similarity-space as well as the views of the genes of interests for detail study. The visualisation employs avatars to represent the patients to enhance the realistic look-and-feel of the patients in the immersive environments. We illustrate the effectiveness of our platform through a childhood cancer dataset, B-cell Acute Lymphoblastic Leukaemia. © 2019 Association for Computing Machinery.",65,Visualization,Augmented reality - Data visualization - Diseases - Genes - Virtual reality,3D Visualisation - Acute lymphoblastic leukaemias - Augmented and virtual realities - Genetic similarities - Genomic data - Immersive environment - Interactive visualisation - Multidimensional data,"461.2 Biological Materials and Tissue Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Towards collaborative photorealistic VR meeting rooms,"Schäfer, Alexander (1); Reis, Gerd (1); Stricker, Didier (1) ","(1) German Research Center for Artificial Intelligence, TU Kaiserslautern, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 599-603,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,10.1145/3340764.3344466,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"When designing 3D applications it is necessary to find a compromise between cost (e.g. money, time) and achievable realism of the virtual environment. Reusing existing assets has an impact on the uniqueness of the application and creating high quality 3D assets is very time consuming and expensive. We aim for a low cost, high quality and minimal time effort solution to create virtual environments. This paper’s main contribution is a novel way of creating a virtual meeting application by utilizing augmented spherical images for photo realistic virtual environments. © 2019 Association for Computing Machinery.",18,Mixed reality,Augmented reality - Teleconferencing - Virtual reality - Visual communication,3D application - High quality - Low costs - Photo-realistic - Spherical images - Telepresence - Virtual meetings,"717.1 Optical Communication Systems - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Implement AI service into VR training,"Suttor, Joshua (1); Marin, Julian (1); Verbus, Evan (1); Su, Meng (1) ","(1) Department of Computer Science, Penn State University, Behrend College, United States ",ACM International Conference Proceeding Series,Ritsumeikan University,Association for Computing Machinery,,p 114-121,27-Nov-19,"Proceedings of 2019 2nd International Conference on Signal Processing and Machine Learning, SPML 2019",2019,,,,9.78145E+12,10.1145/3372806.3374909,,"2nd International Conference on Signal Processing and Machine Learning, SPML 2019","November 27, 2019 - November 29, 2019",,"In this paper, we described the implementation of using a collection of AI services in IBM Watson to facilitate user interaction in a virtual reality space for training simulations. The project aims to increase the efficiency of training employees in an organization, by creating an immersive 3D VR environment tailored to a specific profession. Current training methods usually require an expert of the field to be hired in order to personally train these employees. The main goal of the project is to create a standard training environment which can be used and tailored by companies to train these employees without adding an additional cost. © 2019 Association for Computing Machinery.",10,Personnel training,Artificial intelligence - Augmented reality - Machine learning - Signal processing - Virtual reality,Additional costs - IBM Watson - Immersive - Training employees - Training methods - Training simulation - User interaction,"716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications - 723.4 Artificial Intelligence - 912.4 Personnel",,,,Our thanks to ImmersiMap Global sponsored and guided us throughout the entire project.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Vitalab.Mobile - A mobile living lab,"Schmelter, Thereza (1); Rings, Sebastian (2); Prasuhn, Caspar (2); Villwock, Joachim (1); Steinicke, Frank (2); Hildebrand, Kristian (1) ","(1) Beuth University of Applied Science, Berlin, Germany (2) Universität Hamburg, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 917-920,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,10.1145/3340764.3345381,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"VITALab.Mobile is a mobile VR/AR laboratory for case and field studies as well as clinical trials to evaluate novel forms of virtual therapies and scientific research. The laboratory is build into a truck, which can reach a variety of different user groups everywhere. The focus of this interdisciplinary project is the creation of a user-friendly and real-world research and interactive environment with the topic of diagnostics and therapy in the future within a medico-therapeutic context. The project provides a platform for other scientists, doctors, therapists and patients. © 2019 Association for Computing Machinery.",9,Human computer interaction,Augmented reality - Carrier mobility - Clinical research - Diagnosis - Health - Research - Trucks - Virtual reality,Clinical trial - Field studies - Interactive Environments - Interdisciplinary project - Living lab - Scientific researches - User friendly - User groups,"461.6 Medicine and Pharmacology - 663.1 Heavy Duty Motor Vehicles - 712.1 Semiconducting Materials - 723 Computer Software, Data Handling and Applications - 901.3 Engineering Research",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Vague gesture control: Implications for burns patients,"Zsolczay, Rodney (1); Brown, Ross (1); Maire, Frederic (1); Turkay, Selen (1) ","(1) Queensland University of Technology (QUT), Brisbane; QLD, Australia ",ACM International Conference Proceeding Series,Curtin University; Edith Cowan University (ECU); Human Factors and Ergonomics Society of Australia (HFESA); Perth Convention Bureau; The University of Western Australia (UWA); UX Machines Pty Ltd,Association for Computing Machinery,,p 524-528,2-Dec-19,"Proceedings of the 31st Australian Conference on Human-Computer-Interaction, OzCHI 2019",2019,,,,9.78145E+12,10.1145/3369457.3369512,,"31st Australian Conference on Human-Computer-Interaction, OzCHI 2019","December 2, 2019 - December 5, 2019",,"In the rehabilitation of burns patients, remedial exercises are an important part of maintaining and regaining range-of-motion. To encourage patient rehabilitation and reduce discomfort, this paper presents the preliminary stages of a project which makes use of both vision based sensors and virtual reality technologies to create an immersive environment. The goal of our research is to design and develop effective vague gesture control. Hand gesture detection may lack clarity/resolution or otherwise be vague due to various reasons including patients' difficulty to enact gestures fully due to injuries, interference with the visual detection of gestures due to bandages, and the difficulty that vision based sensors have detecting small details like fingers at range. Extra environmental and situational information can enhance gesture recognition and provide better gestural classification. To achieve this, temporal machine learning is used to develop an application which uses past environmental interactions to help clarify present vague gestures. A gesture recognition system that can handle imperfect gestures in an intuitive way contributes towards naturalistic computer human interactions, in particular for gamified burns rehabilitation systems. © 2019 Association for Computing Machinery.",37,Patient rehabilitation,Augmented reality - Classification (of information) - Computer vision - Gesture recognition - Human computer interaction - Learning systems - Machine learning - Virtual reality,Computer Human Interaction - Environmental interactions - Gesture recognition system - Immersive environment - Range of motions - Rehabilitation System - Virtual reality technology - Vision-based sensors,"461.5 Rehabilitation Engineering and Assistive Technology - 716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Teaching physics using equidistant AR/VR-projections,"Yavoruk, Oleg (1) ","(1) Yugra State University, 16, Chekhova Street, KhMAO-Yugra, Khanty-Mansiysk; 628012, Russia ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 48-51,25-Oct-19,ICDTE 2019 - 2019 the 3rd International Conference on Digital Technology in Education,2019,,,,9.78145E+12,10.1145/3369199.3369242,,"3rd International Conference on Digital Technology in Education, ICDTE 2019","October 25, 2019 - October 27, 2019",,"Modern augmented/virtual reality (AR/VR) technologies offer us new educational opportunities. They possess the immense untapped resources for significant improvements of the physics education. The transformation of the learning scene (or arena) to the abstract symbolic environment leads to the emergence of the new views at teaching. The AR/VR-glasses allow us to do this, changing, supplementing and augmenting the picture of the physical world around the observer (teacher or student) and even completely replacing it. The paper deals with the teaching experience of the equidistant (spherical) projection use in the physics classes. It describes the list of useful tools to facilitate the perception of AR/VR 360-panoramas, recommendations for the practical use, the students opinion about this technology. It also presents the description of equidistant panoramic slides that are already available, tested in the practice, and ready for physics teaching. Here we consider the place of equidistant AR/VR-projections in the educational process, characteristics of the educationally tested AR/VR-devices, the most effective techniques of their use. © 2019 Association for Computing Machinery.",18,E-learning,Augmented reality - Educational technology - Students - Virtual reality,Augmented/virtual reality - Educational opportunities - Educational process - Equidistant Projection - Physics education - Physics teaching - Symbolic environment - Teaching experience,"723 Computer Software, Data Handling and Applications - 901.2 Education",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The Apprentice Gaze - AR Experience on Serralves Museum,"Silva, Manuel (1); Assaf, Rodrigo (1); Pollini, Denise (2); Morais, Diogo (1); Teixeira, Luis (3) ","(1) School of Arts - Digital Creativity Center, Universidade Católica Portuguesa, Porto, Portugal (2) Education Department, Fundação Serralves, Porto, Portugal (3) School of Arts - Digital Creativity Center, CITAR, Universidade Católica Portuguesa, Porto, Portugal ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,23-Oct-19,ARTECH 2019: Digital Media Art Ecosystems - Proceedings of the 9th International Conference on Digital and Interactive Arts,2019,,,,9.78145E+12,10.1145/3359852.3359977,a93,"9th International Conference on Digital and Interactive Arts: Digital Media Art Ecosystems, ARTECH 2019","October 23, 2019 - October 25, 2019",,"The main goal of the conference is to promote the interest in the current digital culture and its intersection with art and technology as an important research field, and also to create a common space for discussion and exchange of new experiences. It seeks to foster greater understanding about digital arts and culture across a wide spectrum of cultural, disciplinary, and professional practices. To this end, many scholars, teachers, researchers, artists, comput-er professionals, and others who are working within the broadly defined areas of digital arts, culture and education across the world, submitted their innovative work to the conference. © 2019 Authors.",4,Arts computing,Augmented reality - Digital storage - Ecosystems - Education - Mobile computing - Museums,Art and technology - Common spaces - Digital art - Digital culture - Interactive exhibits - Professional practices - Research fields - Wide spectrum,"402.2 Public Buildings - 454.3 Ecology and Ecosystems - 722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Automatic eyeglasses replacement for a 3d virtual try-on system,"Kobayashi, Takumi (1); Sugiura, Yuta (1); Saito, Hideo (1); Uema, Yuji (2) ","(1) Keio University Yokohama, Kanagawa, Japan (2) JINS Inc, Tokyo, Japan ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,11-Mar-19,"Proceedings of the 10th Augmented Human International Conference, AH 2019",2019,,,,9.78145E+12,10.1145/3311823.3311854,a30,"10th Augmented Human International Conference, AH 2019","March 11, 2019 - March 12, 2019",,"This paper presents a 3D virtual eyeglasses try-on system for practical use. For fitting eyeglasses in a shop, consumers wish to look at themselves in a mirror while trying on various eyeglass styles. However, for people who need to wear eyeglasses for correcting problems with eyesight, it is impossible for them to clearly observe their face in the mirror without wearing eyeglasses. This makes fitting them for new eyeglasses difficult. This research proposes a virtual try-on system that can be used while wearing eyeglasses.We replace the user's eyeglasses in the input video with new eyeglasses virtually. Moreover, a fast and accurate face tracking tool enables our system to automatically display 3D virtual glasses following a user's head motion. Experimental results demonstrate that the proposed method can render virtual glasses naturally while the user is wearing real eyeglasses. ©2019 Association for Computing Machinery.",10,Eyeglasses,Augmented reality - Glass - Mirrors - Mixed reality,Correcting problems - Eyeglasses removal - Face Tracking - Head motion - Input videos - Practical use - Virtual try-on,"723 Computer Software, Data Handling and Applications - 741.3 Optical Devices and Systems - 812.3 Glass",,,"Number: JP-MJCR18Y2, Acronym: -, Sponsor: -; ","This work was supported by JST AIP-PRISM Grant Number JP-MJCR18Y2, JAPAN.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
E-EMOTION CAPSULE: As artes digitais na criação de emoções,"Gomes, Paulo Veloso (1); Marques, António (1); Pereira, Javier (2); Correia, António (1); Donga, João (1); Sá, Vítor J. (1) ","(1) LabRP-Laboratório de Reabilitação Psicossocial, Escola Superior de Saúde Do Politécnico Do Porto, Porto, Portugal (2) CITIC-Centro de Investigacao em Tecnologias de Informacao e Comunicacao, Universidade da Corunha, Spain ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,23-Oct-19,ARTECH 2019: Digital Media Art Ecosystems - Proceedings of the 9th International Conference on Digital and Interactive Arts,2019,Portuguese,,,9.78145E+12,10.1145/3359852.3359962,a86,"9th International Conference on Digital and Interactive Arts: Digital Media Art Ecosystems, ARTECH 2019","October 23, 2019 - October 25, 2019",,"The main goal of the conference is to promote the interest in the current digital culture and its intersection with art and technology as an important research field, and also to create a common space for discussion and exchange of new experiences. It seeks to foster greater understanding about digital arts and culture across a wide spectrum of cultural, disciplinary, and professional practices. To this end, many scholars, teachers, researchers, artists, comput-er professionals, and others who are working within the broadly defined areas of digital arts, culture and education across the world, submitted their innovative work to the conference. © 2019 Authors.",6,Arts computing,Augmented reality - Digital storage - Ecosystems - Virtual reality,Empathy - Immersive environment - Mental health - Schizophrenia - Video 360,"454.3 Ecology and Ecosystems - 722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Is it real? Understanding interaction mechanics within the reality-virtuality continuum,"Lakhnati, Younes (1); Springer, Raphael (1); White, Edward (2); Gerken, Jens (1) ","(1) Westfälische Hochschule, Human-Computer Interaction Group, Gelsenkirchen, Germany (2) Finland ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,10.1145/3365610.3365634,3365634,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"The concept of Mixed Reality has existed in research for decades but has experienced rapid growth in recent years, mainly due to technological advances and peripherals such as the Microsoft HoloLens reaching the market. Despite this, certain design aspects of Mixed Reality experiences, such as the different nuances of real and virtual elements, remain largely unexplored. This paper presents an explorative study with 15 participants which aims to investigate and gain a better understanding of the different qualities of real and virtual objects. To that end, we developed a Mixed Reality board game that offered different combinations of real and virtual game components, such as the board, the pieces and the dice. Our analysis shows that the participants generally preferred the completely virtual variant but appreciated different qualities of real and virtual elements. The results also indicate that virtual interaction elements work better on a real background than vice versa. However, this conflicts with some participants’ preference of using physical pieces for the haptic experience, creating a design trade-off. This study represents a first step in exploring how the experience changes when swapping elements of differing realities for one another and identifying these trade-offs. © 2019 Association for Computing Machinery.",32,Mixed reality,Augmented reality - Economic and social effects - Interactive computer graphics,Augmented virtualities - Board games - Technological advances - Trade off - User study - Virtual elements - Virtual interactions - Virtuality continuum,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 971 Social Sciences",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A mixed reality environmental simulation to support learning about maritime habitats,"Taulien, Andre (1); Paulsen, Anika (1); Streland, Tim (1); Jessen, Benedikt (1); Wittke, Stefan (1); Teistler, Michael (1) ","(1) Fachbereich Information und Kommunikation Hochschule Flensburg, Flensburg, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 921-925,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,10.1145/3340764.3345382,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"Environmental simulations allow for an easy and safe acquisition of knowledge about hard-to-reach habitats. We present a mixed-reality simulation that enables the user to convert (parts of) an arbitrary place like his or her own living room into a maritime habitat, using the example of the Baltic Sea. The user explores the virtual underwater world by walking in the real world. Typical characteristics and elements of the simulated habitat are integrated by adjusting light refraction and textures, and by adding animals, plants, and stones. The virtual animals respond to the user’s movement. Rearranging real world objects causes the virtual world to change its appearance as well. User tests show that the environmental simulation provides an authentic insight into the Baltic Sea habitat and is well received with regard to the overall user experience. However, for first-time users, conveying more formal knowledge does not seem to work as well (yet) as compared to a conventional textbook approach. © 2019 Copyright is held by the owner/author(s).",7,Mixed reality,Animals - Augmented reality - Ecosystems - Interactive computer graphics - Lighting - Textures - Texturing,3D-scanning - Computer supported learning - Environmental simulation - Habitat - SWARM simulation,"454.3 Ecology and Ecosystems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 819.5 Textile Products and Processing",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Mensch ARgere Dich nicht - A board game testbed for mixed reality interactions,"Lakhnati, Younes (1); Springer, Raphael (1); Gerken, Jens (1) ","(1) Westfälische Hochschule, Gelsenkirchen, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,10.1145/3365610.3368471,3368471,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"'Mensch ARgere Dich Nicht' is a Mixed Reality (MR) game based on a popular German board game 'Mensch Ärgere Dich Nicht', from the early 1900s, similar to Pachisi. Developed to be a test bed for investigating mixed interactions, the application offers real and virtual variations of all core game elements (board, dice, pieces) for a fully modifiable experience. Using the Microsoft HoloLens, players can interact with the virtual game elements and receive additional information about the current game state. The game also recognizes interactions with real game elements and translates them into the virtual world. © 2019 Copyright held by the owner/author(s).",12,Mixed reality,Augmented reality - Economic and social effects - Interactive computer graphics,Augmented virtualities - Board games - Game elements - Game-Based - MicroSoft - Trade off - Virtual games - Virtual worlds,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 971 Social Sciences",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Assisting viewpoint to understand own posture as an avatar in-situation,"Hamanishi, Natsuki (1); Miyaki, Takashi (1); Rekimoto, Jun (1) ","(1) University of Tokyo, Sony Computer Science Laboratories, Tokyo, Japan ",ACM International Conference Proceeding Series,mozilla,Association for Computing Machinery,,p 1-8,1-Apr-19,Proceedings of CHIuXiD 2019 - 5th International Conference on HCI and UX: Empowering Digital Transformation,2019,,,,9.78145E+12,10.1145/3328243.3328244,,"5th International Conference on Human-Computer Interaction (HCI) and User Experience (UX): Empowering Digital Transformation, CHIuXiD 2019","April 1, 2019 - April 9, 2019",,"Understanding the correct self-posture is known to improve the performance of motor skills in sports, dance, ballet, walking, and running. However, it is difficult to understand the accurate self-posture and move one’s body as intended for imitation or learning, especially in-situation. Based on previous physiological research, we herein propose the addition of a new assisting viewpoint to the human body interface for understanding the correct self-posture without interrupting the training process. This new viewpoint enables the users to see their current posture as a three-dimensional skeletal image that is an avatar which synchronizes with the owner’s movements. The Optical See-Through Head Mounted Display (OST-HMD) and Full-Body Motion Capture (MoCap) are used for creating the proposed viewpoint. The position and the angle of view of an avatar is determined by the rotation of the user’s head interactively. Moreover, visualizing the avatar’s trail helps users to understand how to moved their own body correctly. We conducted several experiments to confirm the avatar system’s validity and the availability for an expert athlete. Our results showed that the proposed viewpoint was successful in enabling the user to understand the accurate self-posture in-situation without occurring interruption. © 2019 Association for Computing Machinery.",26,Three dimensional computer graphics,Augmented reality - Helmet mounted displays - Human computer interaction - Interactive computer graphics - Mixed reality - Sports,Full-body interaction - Full-body motions - Human bodies - Motor skills - Optical see-through head-mounted displays - Spatial perception - Sports trainings - Training process,"461.3 Biomechanics, Bionics and Biomimetics - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented recreational volleyball court supporting the beginners' landing position prediction skill by providing peripheral visual feedback,"Sato, Koya (1); Sano, Yuji (1); Otsuki, Mai (1); Oka, Mizuki (1); Kato, Kazuhiko (1) ","(1) University of Tsukuba, Japan ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,11-Mar-19,"Proceedings of the 10th Augmented Human International Conference, AH 2019",2019,,,,9.78145E+12,10.1145/3311823.3311843,a15,"10th Augmented Human International Conference, AH 2019","March 11, 2019 - March 12, 2019",,"Volleyball is widely popular as a way to share a sense of unity and achievement with others. However, errors detract beginners from enjoying the game. To overcome this issue, we developed a system that supports the beginners' skill to predict the ball landing position by indicating the predicted ball landing position on the floor as a visual feedback. In volleyball, it is necessary to pay attention to the ball that has been launched in air, and visual feedback on the floor surface must be perceived through peripheral vision. The effect of such visual feedback in supporting beginners' prediction skill was not clear. Therefore, we evaluated the effectiveness of the proposed system via a simulated serve-reception experiment. As a result, we confirmed that the proposed system improved the prediction skill in terms of the prediction speed and accuracy in the left-right direction, and that beginners felt an improvement in the prediction accuracy and ease of ball manipulation, thereby increasing the enjoyment. These results also indicate that it is possible to utilize peripheral vision supports in other disciplines in which there is a distance between the object of attention and the sports field on which visual feedback can be presented. ©2019 Association for Computing Machinery.",35,Forecasting,Augmented reality - Floors - Sports - Vision - Visual communication,Floor projections - Floor surface - Peripheral vision - Position predictions - Prediction accuracy - Recreational volleyball - Visual feedback,"402 Buildings and Towers - 461.3 Biomechanics, Bionics and Biomimetics - 717.1 Optical Communication Systems - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Enhancing value proposition through AI strategy; A casestudy on a targeted application of AR in field support,"Bhale, Sanjay (1, 2) ","(1) Indira Institute of Management, Pune; 411033, India (2) XLRI, Jamshedpur, India ",ACM International Conference Proceeding Series,Faculty of Science and Engineering; IEDRC.org,Association for Computing Machinery,,p 453-457,10-Jan-19,"IC4E 2019 - 2019 10th International Conference on E-Education, E-Business, E-Management and E-Learning",2019,,,,9.78145E+12,10.1145/3306500.3306576,,"10th International Conference on E-Education, E-Business, E-Management and E-Learning, IC4E 2019","January 10, 2019 - January 13, 2019",,"Artificial intelligence (AI) is increasingly being integrated into core business strategies for cognitive advancement. AI and its allied applications are impacting the function of all disciplines, processes and the fellow professionals in emerging business ecosystem today. It has transformed the way an enterprise work and how it exploits new technology and business avenues improving the quality of solutions in field support. This in turn, has also built an ecosystem right from the design, construction of system application and managing it through virtual interface. This study aims to uncover strategic imperatives of AI and productive impact of corresponding AR (Augmentative Reality) technologies with the help of a case-study of an IT (information-technology) firm. In this paper, the author explores the conceptual as well as application competence of AR by thoroughly examining the literature, contemporary trends and innovative applications. © 2019 Association for Computing Machinery.",21,Ecosystems,Augmented reality - E-learning - Electronic commerce,Business ecosystem - Information architectures - Quality of solution - Servitization - Strategic imperative - System applications - Value chains - Virtual interfaces,"454.3 Ecology and Ecosystems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Gaze-contingent ocular parallax rendering for virtual reality,"Konrad, Robert (1); Angelopoulos, Anastasios (1); Wetzstein, Gordon (1) ","(1) Stanford University, United States ","ACM SIGGRAPH 2019 Talks, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Talks, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3306307.3328201,3328201,"ACM SIGGRAPH 2019 Talks - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019","July 28, 2019 - August 1, 2019",,"Current-generation virtual reality (VR) displays aim to generate perceptually realistic user experiences by accurately rendering many perceptually important effects including perspective, disparity, motion parallax, and other depth cues. We introduce ocular parallax rendering, a technology that renders small amounts of gaze-contingent parallax capable of further increasing perceptual realism in VR. Ocular parallax, small depth-dependent image shifts on the retina created as the eye rotates, occurs because the centers of rotation and projection of the eye are not the same. We study the perceptual implications of ocular parallax rendering by designing and conducting a series of user experiments. We estimate perceptual detection and discrimination thresholds for this effect and demonstrate that it is clearly visible in most VR applications. However, our studies also indicate that ocular parallax rendering does not significantly improve depth perception in VR. © 2019 Copyright held by the owner/author(s).",13,Eye tracking,Augmented reality - Depth perception - Geometrical optics - Interactive computer graphics - Rendering (computer graphics) - Virtual reality,Current generation - Depth dependents - Detection and discriminations - Gaze-contingent - Motion parallax - User experience - User experiments - VR applications,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 741.1 Light/Optics",,,"Number: 1553333, Acronym: NSF, Sponsor: National Science Foundation; Number: -, Acronym: -, Sponsor: Intel Corporation; Number: -, Acronym: -, Sponsor: Okawa Foundation for Information and Telecommunications; ","This project was supported by Intel, a Sloan Fellowship, the Okawa Foundation, and the National Science Foundation (NSF; 1553333, 1839974).",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Deeplight: Learning illumination for unconstrained mobile mixed reality,"LeGendre, Chloe (1); Ma, Wan-Chun (1); Fyffe, Graham (1); Flynn, John (1); Charbonnel, Laurent (1); Busch, Jay (1); Debevec, Paul (1) ","(1) Google Inc., United States ","ACM SIGGRAPH 2019 Talks, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Talks, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3306307.3328173,3328173,"ACM SIGGRAPH 2019 Talks - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019","July 28, 2019 - August 1, 2019",,"We present a learning-based method to infer plausible high dynamic range (HDR), omnidirectional illumination given an unconstrained, low dynamic range (LDR) image from a mobile phone camera with a limited field of view (FOV). For training data, we collect videos of various reflective spheres placed within the camera’s FOV, leaving most of the background unoccluded, leveraging that materials with diverse reflectance functions reveal different lighting cues in a single exposure. We train a deep neural network to regress from the LDR background image to HDR lighting by matching the LDR ground truth sphere images to those rendered with the predicted illumination using image-based relighting, which is differentiable. Our inference runs at interactive frame rates on a mobile device, enabling realistic rendering of virtual objects into real scenes for mobile mixed reality. Training on auto-exposed and white-balanced videos, we improve the realism of rendered objects compared to the state-of-the art methods for both indoor and outdoor scenes. © 2019 Copyright held by the owner/author(s).",4,Mixed reality,Augmented reality - Cameras - Deep neural networks - Interactive computer graphics - Lighting - Rendering (computer graphics),Image-based lighting - Image-based relighting - Interactive frame rates - Learning-based methods - Lighting estimation - Mobile mixed realities - Reflectance functions - State-of-the-art methods,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 742.2 Photographic Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Towards a cyberphysicalweb science: A social machines perspective on pokémon go!,"De Roure, David (1); Hendler, James A. (2); James, Diccon (3); Nurmikko-Fuller, Terhi (4); Van Kleek, Max (5); Willcox, Pip (1) ","(1) Oxford E-Research Centre, University of Oxford, Oxford, United Kingdom (2) Institute for Data Exploration and Applications, Rensselaer Polytechnic Institute (RPI), Troy; NY, United States (3) Independent Researcher, Oxford, United Kingdom (4) Centre for Digital Humanities Research, Australian National University, Canberra, Australia (5) Dept of Computer Science, University of Oxford, Oxford, United Kingdom ",WebSci 2019 - Proceedings of the 11th ACM Conference on Web Science,ACM SIGWEB,"Association for Computing Machinery, Inc",,p 65-69,26-Jun-19,WebSci 2019 - Proceedings of the 11th ACM Conference on Web Science,2019,,,,9.78145E+12,10.1145/3292522.3326043,,"11th ACM Conference on Web Science, WebSci 2019","June 30, 2019 - July 3, 2019",,"The concept of Social Machines has become an established lens to describe the sociotechnical systems of Web Science, and has been applied to some archetypical cyberphysical systems. In this paper we apply this lens to a larger system, the location based online augmented reality game Pokémon Go!. The contributions are an illustrative application of the descriptive Social Machines lens to a system of this scale and type, and the use of simulation as a method for an executable description, which includes use of an ontology to represent partially the universe of the game. © 2019 Association for Computing Machinery.",9,Embedded systems,Augmented reality - Internet of things,Cyber physical systems (CPSs) - Game design - Location based - Sociotechnical systems - Web science,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Development of an acoustic AR gamification system to support physical exercise,"Kiriu, Takumi (1); Mittal, Mohit (1); Siriaraya, Panote (2); Kawai, Yukiko (3); Nakajima, Shinsuke (4) ","(1) Kyoto Sangyo University, Japan (2) Kyoto Institute of Technology, Japan (3) Kyoto Sangyo University, Osaka University, Japan (4) Kyoto Sangyo Univ, Japan ",MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,ACM SIGMM,"Association for Computing Machinery, Inc",,p 1056-1058,15-Oct-19,MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,2019,,,,9.78145E+12,10.1145/3343031.3350589,,"27th ACM International Conference on Multimedia, MM 2019","October 21, 2019 - October 25, 2019",,"In recent years, running has become increasingly popular as an effective exercise activity which could help improve and maintain one's physical health. However, it is generally difficult to motivate people to adhere to their fitness regimens and persist in such activities. To help address this problem, we developed a Gamified Acoustic AR running support system where we use the previous running records of a user to project a 'Virtual runner' into an augmented reality space for runners to compete against. The presence of the virtual runner is conveyed through the sound of running footsteps and breathing: users hear the sound of the virtual runner while they are running and are able to virtually compete against themselves in real time. In this paper, we describe how such a system could be implemented and discuss the results of an experiment study which highlights the effectiveness of our Gamified AR system. © 2019 Copyright held by the owner/author(s).",3,Virtual addresses,Augmented reality - Sports,AR system - Experiment study - Gamification - Physical exercise - Physical health - Real time - Support systems,"461.3 Biomechanics, Bionics and Biomimetics - 722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications",,,"Number: 16H01722, Acronym: JSPS, Sponsor: Japan Society for the Promotion of Science; ","The work carried out in this paper is supported by the research grant of Innovation Platform for Society 5.0 and the JSPS KAKENHI research grants (16H01722, 17H01822, 19K12240, 19H04118).",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Toward consistent state management of adaptive programmable networks based on P4,"He, Mu (1); Blenk, Andreas (1); Kellerer, Wolfgang (1); Schmid, Stefan (2) ","(1) Department of Communication Networks, Technical University of Munich, Germany (2) Faculty of Computer Science, University of Vienna, Austria ","NEAT 2019 - Proceedings of the 2019 ACM SIGCOMM Workshop on Networking for Emerging Applications and Technologies, Part of SIGCOMM 2019",ACM SIGCOMM,"Association for Computing Machinery, Inc",,p 29-35,14-Aug-19,"NEAT 2019 - Proceedings of the 2019 ACM SIGCOMM Workshop on Networking for Emerging Applications and Technologies, Part of SIGCOMM 2019",2019,,,,9.78145E+12,10.1145/3341558.3342202,,"2nd ACM SIGCOMM Workshop on Networking for Emerging Applications and Technologies, NEAT 2019",19-Aug-19,,"Emerging network applications (augmented reality, industrial Internet, etc.) introduce stringent new requirements on the performance, dependability, and adaptability of communication networks. Programmable data planes (e.g., based on P4) provide new opportunities to meet these requirements, by enabling adaptive network reconfigurations. However, ensuring consistency during such reconfigurations remains challenging. This paper makes a first step toward a more automated state management of adaptive data planes. In particular, we present an efficient P4 state management framework, P4State, which allows to quickly identify the network states from the source code that are critical for data plane reconfigurations (e.g., due to scaling, failure recovery). We report on first promising evaluation results of our prototype implementation in terms of correctness and efficiency, also considering two case studies using HULA (load balancing in data center) and HashPipe (line-rate measurement in data plane). © 2019 Association for Computing Machinery.",31,Information management,Augmented reality,Adaptive networks - Code analysis - Data planes - Evaluation results - Network applications - Programmable network - Prototype implementations - Stateful Reconfiguration,"723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: ERC, Sponsor: European Research Council; Number: 647158, Acronym: -, Sponsor: -; ",This work is part of a project that has received funding from the European Research Council (ERC) under the European Union&rsquo;s Horizon 2020 research and innovation program (grant agreement No 647158 - FlexNets). The authors alone are responsible for the content of the paper.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A Light-weight Wi-Fi fingerprint generator with AR,"Shen, Shuhan (1); Lu, Bingxian (1); Yang, Zhanwu (1); Dong, Yinghua (1); Han, Bin (1); Wang, Lei (1) ","(1) Dalian University of Technology, China ","SIGCOMM 2019 - Proceedings of the 2019 ACM SIGCOMM Conference Posters and Demos, Part of SIGCOMM 2019",ACM SIGCOMM,"Association for Computing Machinery, Inc",,p 110-111,19-Aug-19,"SIGCOMM 2019 - Proceedings of the 2019 ACM SIGCOMM Conference Posters and Demos, Part of SIGCOMM 2019",2019,,,,9.78145E+12,10.1145/3342280.3342325,,"2019 ACM SIGCOMM Conference Posters and Demos, SIGCOMM 2019","August 19, 2019 - August 23, 2019",,"Compared with outdoor localization, indoor localization remains knotty. To cope with this problem, people have developed some indoor localization systems based on Wi-Fi fingerprints. However, in order to get the fingerprints, precollection of ample Wi-Fi signal features at selected indoor locations is needed, which is time-consuming. In this poster, we presented a light way to obtain the indoor Wi-Fi RSS fingerprints with the combination of Augmented Reality (AR). We believe the new fingerprint generating approach will shorten the time in data collecting phase and improve the time-efficiency of such methods. © 2019 Association for Computing Machinery.",3,Indoor positioning systems,Argon - Augmented reality - Collector efficiency - Wireless local area networks (WLAN),Data collecting - Fingerprint - Indoor localization - Indoor localization systems - Indoor locations - Outdoor localizations - Time efficiencies - Wi-Fi signals,"702.3 Solar Cells - 723 Computer Software, Data Handling and Applications - 804 Chemical Products Generally",,,"Number: 2017YFC08210 03-2, Acronym: -, Sponsor: -; Number: 61842601, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; Number: -, Acronym: -, Sponsor: Fundamental Research Funds for the Central Universities; ","The work was supported by National Natural Science Foundation of China"" with No. 61733002 and 61842601"," ""National Key Research and Development Plan"" with No. 2017YFC08210 03-2"," and ""the Fundamental Research Funds for the Central Universities"" with No. DUT19RC(3)003.""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,
CalibDB: Enabling web based computer vision through on-the-fly camera calibration,"Rojtberg, Pavel (1); Gorschlüter, Felix (1) ","(1) Fraunhofer IGD, Germany ",Proceedings - Web3D 2019: 24th International ACM Conference on 3D Web Technology,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,26-Jul-19,Proceedings - Web3D 2019: 24th International ACM Conference on 3D Web Technology,2019,,,,9.78145E+12,10.1145/3329714.3338132,,"24th International ACM Conference on 3D Web Technology, Web3D 2019","July 26, 2019 - July 28, 2019",,"For many computer vision applications, the availability of camera calibration data is crucial as overall quality heavily depends on it. While calibration data is available on some devices through Augmented Reality (AR) frameworks like ARCore and ARKit, for most cameras this information is not available. Therefore, we propose a web based calibration service that not only aggregates calibration data, but also allows calibrating new cameras on-the-fly. We build upon a novel camera calibration framework that enables even novice users to perform a precise camera calibration in about 2 minutes. This allows general deployment of computer vision algorithms on the web, which was previously not possible due to lack of calibration data. © 2019 Association for Computing Machinery.",17,Web services,Augmented reality - Calibration - Cameras - Computer vision - Distributed computer systems - Websites,Calibration data - Camera calibration - Computer vision algorithms - Computer vision applications - Distributed systems - Overall quality - Web-based computer - Webxr,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 742.2 Photographic Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Designing AR visualizations to facilitate stair navigation for people with low vision,"Zhao, Yuhang (1); Kupferstein, Elizabeth (1); Castro, Brenda Veronica (1); Feiner, Steven (2); Azenkot, Shiri (1) ","(1) Jacobs Technion-Cornell Institute, Cornell Tech., Cornell University, New York; NY, United States (2) Department of Computer Science, Columbia University, New York; NY, United States ",UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 387-402,17-Oct-19,UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332165.3347906,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Navigating stairs is a dangerous mobility challenge for people with low vision, who have a visual impairment that falls short of blindness. Prior research contributed systems for stair navigation that provide audio or tactile feedback, but people with low vision have usable vision and don't typically use nonvisual aids. We conducted the first exploration of augmented reality (AR) visualizations to facilitate stair navigation for people with low vision. We designed visualizations for a projection-based AR platform and smartglasses, considering the different characteristics of these platforms. For projection-based AR, we designed visual highlights that are projected directly on the stairs. In contrast, for smart-glasses that have a limited vertical field of view, we designed visualizations that indicate the user's position on the stairs, without directly augmenting the stairs themselves. We evaluated our visualizations on each platform with 12 people with low vision, finding that the visualizations for projection-based AR increased participants' walking speed. Our designs on both platforms largely increased participants' self-reported psychological security. © 2019 Association for Computing Machinery.",88,User interfaces,Audio systems - Augmented reality - Flow visualization - Navigation - Stairs - Vision aids - Visualization,Accessibility - Low vision - Non-visual aids - Smart glass - Tactile feedback - Vertical fields - Visual impairment - Walking speed,"402 Buildings and Towers - 461.5 Rehabilitation Engineering and Assistive Technology - 631.1 Fluid Flow, General - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: NSF, Sponsor: National Science Foundation; ",This work was supported in part by the National Science Foundation under grant no. IIS-1657315. Feiner was funded in part by the National Science Foundation under grant no. IIS-1514429.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Adventures in hologram space: Exploring the design space of eye-to-eye volumetric telepresence,"Dos Anjos, Rafael Kuffner (1); Sousa, Maurício (2); Mendes, Daniel (2); Medeiros, Daniel (1); Billinghurst, Mark (3); Anslow, Craig (4); Jorge, Joaquim (2) ","(1) CMIC, Victoria University of Wellington, INESC-ID, New Zealand (2) INESC-ID, IST, ULisboa, Portugal (3) ITMS, University of South Australia, Australia (4) School of Engineering and Computer Science, Victoria University of Wellington, New Zealand ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364244,3364244,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Modern volumetric projection-based telepresence approaches are capable of providing realistic full-size virtual representations of remote people. Interacting with full-size people may not be desirable due to the spatial constraints of the physical environment, application context, or display technology. However, the miniaturization of remote people is known to create an eye gaze matching problem. Eye-contact is essential to communication as it allows for people to use natural nonverbal cues and improves the sense of 'being there'. In this paper we discuss the design space for interacting with volumetric representations of people and present an approach for dynamically manipulating scale, orientation and the position of holograms which guarantees eye-contact. We created a working augmented reality-based prototype and validated it with 14 participants. © 2019 Copyright held by the owner/author(s).",21,Holograms,Augmented reality - Virtual reality - Visual communication,Application contexts - Display technologies - Eye-to-eye - Physical environments - Spatial constraints - Virtual representations - Volumetric Projection - Volumetric representation,"717.1 Optical Communication Systems - 723 Computer Software, Data Handling and Applications - 743 Holography",,,,"This project was supported by the Entrepreneurial University Programme funded by TEC, also by FCT through grants IT-MEDEX PTDC/EEISII/6038/2014 and UID/CEC/50021/2019.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Painting with movement: Interaction design tool to paint, track and analyze the movement","Nogueira, Maria Rita (1); Menezes, Paulo (2); Patraõ, Bruno (2) ","(1) Institute of Systems and Robotics, Coimbra, Portugal (2) Electrical and Computer Engineering Department, Coimbra, Portugal ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365750,a53,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"The transdisciplinary experience between art and technology has grown over the last decade. The application of augmented reality and virtual reality on other areas has opened doors for hybrid projects and consequently new experimental ideas. Taking it as motivation, a new application concept is proposed in this work, which will allow to someone to walk through and see his body movement in a three-dimensional space. Currently, a user can choose which visual effect is used to draw the resulting movement (e.g. continuous/dashed line). The model has been extended in a way that the visual effect and shape are automatically generated according to movement type, speed, amplitude and intention. Our technological process includes real-time human body detection, movement visualization in real-time and movement tracking history. This project has a core focus on dance and performance, though we consider that the framework is targeted to anyone interesting in body movement and art work. In this sense, the proposed application tracks body movement inside a three-dimensional physical space, only by using a smartphone camera. Our main objective is to record the sequence of movements of a dance, or of someone moving in space, to further analyze their movements and the way they moved in space. And through this idea, we have created an application that aims to record the movement of a user and represent that record in a visual composition of simple elements. The possibility for the user to see the visual tracking of the choreography or performance, allows a clear observation of the space traveled by the dancer and the range of motion and accuracy of the symmetry that the body should or should not have in each step. Over this article the main concepts of the project are presented as well as the multiple applications to real-life scenarios. © 2019 Association for Computing Machinery.",8,Virtual reality,Augmented reality - Flow visualization - Interactive computer graphics - Visualization,Art and technology - Automatically generated - Human body detections - Interaction design - Multiple applications - Smart-phone cameras - Technological process - Three dimensional space,"631.1 Fluid Flow, General - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The impact of remote user's role in a mixed reality mixed presence system,"Norman, Mitchell (1); Lee, Gun A. (1); Smith, Ross T. (1); Billinghurst, Mark (1) ","(1) University of South Australia, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365691,a2,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"Research has shown that Mixed Presence (MP) systems are a valuable collaboration tool. However current research into MP systems is limited to a handful of tabletop and Virtual Reality (VR) systems with no exploration of Head-Mounted Display (HMD) based Mixed Reality (MR) solutions. In this paper, we present a prototype HMD based MR Mixed Presence system we designed and developed. We conducted a user study to investigate how different role assignment affects coordination and engagement in a group task with two local users using MR HMDs, and one remote user with a desktop-based Augmented Reality (AR) interface. The results indicated that the role of coordinator significantly increased the remote user's engagement with increased usage of visual communication cues. This is further supported by the mental effort and perceived dominance reported by users. Feedback from users also indicated visual communication cues are valuable for remote users in MP systems. © 2019 Association for Computing Machinery.",34,Mixed reality,Augmented reality - Groupware - Helmet mounted displays - Interactive computer graphics - Visual communication,Collaboration - Collaboration Tool - Coordination - Group roles - Head mounted displays - Mixed presence - Mixed presence groupware - Role assignment,"717.1 Optical Communication Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Optical see-through head-mounted display with deep depth of field using pinhole polarizing plates,"Katsumata, Yuki (1); Yamada, Wataru (1); Manabe, Hiroyuki (1) ","(1) Research Labs, NTT DOCOMO, Yokosuka, Kanagawa, Japan ",UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 102-104,14-Oct-19,UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332167.3356886,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Optical See-Through Head-Mounted Displays (OST-HMDs) have attracted much attention as augmented reality (AR) devices. Depth of field (DoF) is a key parameter for OST-HMDs because a shallow DoF often yields focal point gaps between computer-generated graphics (CGs) and real scenes forcing users to re-adjust their focus. However, it is difficult to achieve a deep DoF for CGs over real scenes in compact and low-cost devices because laser projectors or complicated optics are needed. In this paper, we propose an OST-HMD that uses polarizing plates with a pinhole to achieve a deep DoF. The optics of the proposed device comprises two polarizing plates and a beam splitter. The polarization planes of the plates are orthogonal and one of the plates has a pinhole. The key idea of the proposal is the pinhole effect using polarizing plates to achieve a deep DoF without any influence to the field of view (FoV) for real scenes. This method can be implemented at significantly low cost and ease in compact thanks to its simple optics and is applicable to OST-HMDs that employ smartphones. In addition, we confirm that CGs are clearly seen whatever the focal point by constructing a prototype. © 2019 Copyright is held by the owner/author(s).",4,Helmet mounted displays,Augmented reality - Costs - Street traffic control - User interfaces,Computer generated graphics - Field of views - Laser projectors - Low-cost devices - Optical see-through - Optical see-through head-mounted displays - Pinhole effects - Polarization planes,"406.2 Roads and Streets - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 911 Cost and Value Engineering; Industrial Economics",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Feasibility study of autonomous drone-based IoT device management in indoor environments,"Haus, Michael (1); Krol, Jan (1); Ding, Aaron Yi (2); Ott, Jörg (1) ","(1) Technical University of Munich, Germany (2) Delft University of Technology, Netherlands ","MAGESys 2019 - Proceedings of the 2019 ACM SIGCOMM Workshop on Mobile AirGround Edge Computing, Systems, Networks, and Applications, Part of SIGCOMM 2019",ACM SIGCOMM,"Association for Computing Machinery, Inc",,p 1-7,14-Aug-19,"MAGESys 2019 - Proceedings of the 2019 ACM SIGCOMM Workshop on Mobile AirGround Edge Computing, Systems, Networks, and Applications, Part of SIGCOMM 2019",2019,,,,9.78145E+12,10.1145/3341568.3342105,,"1st ACM SIGCOMM Workshop on Mobile AirGround Edge Computing, Systems, Networks, and Applications, MAGESys 2019",19-Aug-19,,"Future computing environments are embedded with many sensors for applications like augmented reality. Much of the deployed Internet of Things (IoT) technology is designed to be invisible. To support user's privacy awareness, a map of surrounding sensing devices is beneficial to determine the nature of data collection taking place in any given area. Moreover, security and governance issues are among the challenges IoT poses to organizations which might not know exactly which IoT devices are connected to their network. For instance, many employees bringing their own devices to the workplace. We explore the feasibility to use small COTS drones to create indoor maps of wireless devices. These comprehensive device maps serve as basis for device localization and monitoring to enhance user privacy and network security. We analyze the impact of our device management platform at the drone's energy consumption and evaluate the device detection rate, explored area, and localization error. Due to the restricted battery capacity of the drone, we simulate larger areas with a varying number of IoT devices to highlight the limits of our drone-based device management platform regarding area exploration and reachable IoT devices. © 2019 Copyright held by the owner/author(s).",19,Internet of things,Aircraft detection - Augmented reality - Commercial off-the-shelf - Drones - Edge computing - Energy utilization - Network security,Computing environments - Device management - Feasibility studies - Indoor environment - Internet of Things (IOT) - Localization errors - Privacy awareness - User privacy,"525.3 Energy Utilization - 716.2 Radar Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Development of chemical incident response training program by applying virtual reality technology,"An, Subin (1); Kim, Youngmi (2); Jung, Gayoung (3); Jang, Hee (1); Song, Chaehoon (2); Ma, Byungchol (3) ","(1) Center for Chemical Process Safety of Chonnam National University, Yongbongro77, Gwangju, Korea, Republic of (2) Skonec Entertainment Co., LTD., Seolleungro577, Gangnam-gu, Seoul, Korea, Republic of (3) School of Chemical Engineering of Chonnam national University, Yongbongro77, Gwangju, Korea, Republic of ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 6-10,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332308,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"Because chemicals have different hazard and risk according to their type, it is important to secure expertise for the incident response staff to deal effectively with chemical incidents. Although many organizations conduct incident response training, they find it difficult because of the high cost and risk of an incident during training. Meanwhile, virtual reality technology is applied in various fields, including safety education, military training, and the medical field. Such training using virtual reality can control the training scenario and environment, in addition to offering other benefits such as low cost, minimized risk of an incident during training, repeatability, and high immersion. Because of these benefits, in the present study, a chemical incident response training program was developed by applying virtual reality technology, and an enhanced training effect with a higher level of immersion and reality was obtained by establishing a virtual environment similar to an actual one. © 2019 Copyright is held by the owner/author(s). Publication rights licensedto ACM.",10,E-learning,Augmented reality - Chemical hazards - Curricula - Indicators (chemical) - Risk perception - Virtual reality,Chemical incidents - Enhanced training - Incident response - Military training - Safety education - Training program - Training scenario - Virtual reality technology,"723 Computer Software, Data Handling and Applications - 801 Chemistry - 804 Chemical Products Generally - 901.2 Education - 914.1 Accidents and Accident Prevention",,,"Number: 2018001950001, Acronym: KEITI, Sponsor: Korea Environmental Industry and Technology Institute; ",This work was supported by Korea Environmental Industry & Technology Institute (KEITI) through the Chemical Accident Prevention Technology Development Project (2018001950001) and National Institute of Chemical Safety (NICS) funded by Korea Ministry of Environment.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Is it real? Measuring the effect of resolution, latency, frame rate and jitter on the presence of virtual entities","Louis, Thibault (1); Troccaz, Jocelyne (1); Rochet-Capellan, Amélie (1); Bérard, François (1) ","(1) Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, TIMC-IMAG, Gipsa-Lab, Grenoble; F-38000, France ",ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 5-16,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,10.1145/3343055.3359710,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"The feeling of presence of virtual entities is an important objective in virtual reality, teleconferencing, augmented reality, exposure therapy and video games. Presence creates emotional involvement and supports intuitive and efficient interactions. As a feeling, presence is mostly measured via subjective questionnaire, but its validity is disputed. We introduce a new method to measure the contribution of several technical parameters toward presence. Its robustness stems from asking participant to rank contrasts rather than asking absolute values, and from the statistical analysis of repeated answers. We implemented this method in a user study where virtual entities were created with a handheld perspective corrected display. We evaluated the impact on two virtual entities' presence of four important parameters of digital visual stimuli: resolution, latency, frame rate and jitter. Results suggest that jitter and frame rate are critical for presence but not latency, and resolution depends on the explored entity. Copyright © 2019 Association of Computing Machinery.",34,Jitter,Augmented reality - Virtual reality,Efficient interaction - Exposure therapy - Feeling of presences - HPCD - Presence - Spatial augmented realities - User study - Virtual entities,"723 Computer Software, Data Handling and Applications",,,,This work was supported by the An@tomy2020 project ANR-16-CE38-0011.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Game Cinematography - Towards Understanding Relationship between Spatial Distortion and Game-play,"Lee Moh, Ian Soon (1); Mustafa Zaidi, Syed Fawad (1) ","(1) Media Design School, New Zealand ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 26-32,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332313,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"Although work is in progress to get a more realistic 3D simulation of a real environment, digital cinematography in games has so far delivered what it has promised in terms of high fidelity and realistic experience for the users. Therefore, with the available technology, we have investigated how video game cinematography could further be integrated into game design through the use of realistic camera lens distortion? To answer this question, our research develops a test bed using a camera model that simulates spatial distortion generalizing real-world lens behaviour. We implemented this camera in a test bed that simulates three use cases in games. Through this test bed, we tested our simulations with a survey sample of 21 people and made observations of how it impacted the game-play experience. Through these observations, we measured two metrics that comprise the game- play experience - agency and experience. From our observations and discussion, we demonstrate the use cases where realistic lens simulation and spatial distortion in game-cinematography could improve the game-play experience. Our research shows that further research should be made in exploring how game play experiences are shaped by the implementation of cinematographic techniques in a interactive game setting. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",14,Human computer interaction,Augmented reality - Cameras - Equipment testing - Software design,Camera design - Lens distortion - Player experience - Video game - Virtual cinematography,"723 Computer Software, Data Handling and Applications - 742.2 Photographic Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Demo: An immersive visualization of micro-climatic data using USC AiR,"Ramachandran, Gowri Sankar (1); Bogosian, Biayna (2); Vasudeva, Kunal (1); Sriramaraju, Sushanth Ikshwaku (1); Patel, Jay (1); Amidwar, Shubhesh (1); Malladi, Lavanya (1); Shylaja, Rohan Doddaiah (1); Kumar, Nishant Revur Bharath (1); Krishnamachari, Bhaskar (1) ","(1) USC Viterbi School of Engineering, University of Southern California, Los Angeles, United States (2) USC Media Arts and Practice, University of Southern California, Los Angeles, United States ","MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 675-676,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,10.1145/3307334.3328577,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,"The air pollution level is increasing globally at an alarming rate. In the last two decades, many cities have adopted policies to control the emission of pollutants to the atmosphere as well as to promote sustainable urban developments. However, many of these initiatives have concluded that a long term success would require investing in the environmental literacy of the general population. In this demonstration paper, we present USC AiR, a mobile application that translates the air quality sensor feeds from the CCITI smart campus testbed into augmented reality visualizations for the USC community. USC AiR also allows users to report alarming air quality conditions and recommend environmental interventions such as planting trees. We believe that the integration of augmented reality for air quality monitoring enables the citizens to become more engaged with the air quality data while encouraging them to contribute to the reduction of anthropogenic air pollutants. © 2019 Copyright held by the owner/author(s).",3,Mixed reality,Air quality - Augmented reality - Data visualization - Smart city - Urban growth - Visualization,Air quality data - Air quality monitoring - General population - Immersive visualization - Mobile applications - Reality visualization - Smart Campus - Sustainable urban development,"403.1 Urban Planning and Development - 451.2 Air Pollution Control - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Influence of Interactive Questions on the Sense of Presence and Anxiety in a Virtual-reality Job-interview Simulation,"Shimizu, Shotaro (1); Jincho, Nobuyuki (1); Kikuchi, Hideaki (1) ","(1) Waseda University, 2-579-15, Mikajima, Tokorozawa-shi Saitama, Japan ",ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,p 1-5,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,10.1145/3332305.3332307,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,"Job interviews are one of the most common methods of selecting employees. However, most people experience a certain amount of anxiety during a job interview. This study aims to help people overcome this anxiety by using a virtual reality job-interview simulation system. This system utilizes voice recognition, a dialogue knowledge base, and morphological analysis to generate new questions that include nouns from the user’s answers. We expect the user’s sense of presence and anxiety to increase with interactive questions. This research used self-assessment questionnaires and psychophysiological measures to investigate two study conditions: (a) using developing questions that included part of the user’s answer (n = 10) and (b) using a fixed developing question (n = 10). The results suggested that using developing questions that included the user’s answer provided a greater sense of presence and anxiety. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",16,Virtual reality,Augmented reality - Knowledge based systems - Surveys,Anxiety - Biological signal analysis - Interactive question generation - Job interviews - Sense of presences,"723 Computer Software, Data Handling and Applications - 723.4.1 Expert Systems",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Embedding conversational agents into ar: Invisible or with a realistic human body?,"Reinhardt, Jens (1); Hillen, Luca (1); Wolf, Katrin (1) ","(1) Hamburg University of Applied Sciences, Hamburg, Germany ","TEI 2020 - Proceedings of the 14th International Conference on Tangible, Embedded, and Embodied Interaction",ACM SIGCHI,"Association for Computing Machinery, Inc",,p 299-310,6-Feb-20,"TEI 2020 - Proceedings of the 14th International Conference on Tangible, Embedded, and Embodied Interaction",2020,,,,9.78145E+12,10.1145/3374920.3374956,,"14th International Conference on Tangible, Embedded, and Embodied Interaction, TEI 2020","February 9, 2020 - February 12, 2020",,"Currently, (invisible) smart speech assistants, such as Siri, Alexa, and Cortana, are used by a constantly growing number of people. Moreover, Augmented Reality (AR) glasses are predicted to become widespread consumer devices in the future. Hence, smart assistants can easily become common applications of AR glasses, which allows for giving the assistant a visual representation as an embodied agent. While previous research on embodied agents found a user preference for a humanoid appearance, research on the uncanny valley suggests that simply designed humanoids can be favored over hyper-realistic humanoid characters. In a user study, we compared agents of simple versus more realistic appearance (seen through AR glasses) versus an invisible state-of-The-Art speech assistants (see Figure 1). Our results indicate that a more realistic visualization is preferred as it provides additional communication cues, such as eye contact and gaze, which seem to be key features when talking to a smart assistant. But if the situation requires visual attention, e.g., when being mobile or in a multitask situation, an invisible agent can be more appropriate as they do not distract the visual focus, which can be essential during AR experiences. © 2020 Association for Computing Machinery.",41,Intelligent virtual agents,Augmented reality - Behavioral research - Glass,Avatars - Consumer devices - Conversational agents - Embodied conversational agent - Number of peoples - Virtual assistants - Visual Attention - Visual representations,"723 Computer Software, Data Handling and Applications - 812.3 Glass - 971 Social Sciences",,,"Number: 01JKD1701B, Acronym: BMBF, Sponsor: Bundesministerium f&Atilde;&frac14;r Bildung und Forschung; ",This work is supported by the German Ministry of Education and Research (BMBF) within the GEVAKUB project (01JKD1701B).,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Of smarthomes, IoT plants, and implicit interaction design","Bittner, Björn (1); Aslan, Ilhan (1); Dang, Chi Tai (1); André, Elisabeth (1) ","(1) Human-Centered Multimedia Lab, Augsburg University Augsburg, Germany ","TEI 2019 - Proceedings of the 13th International Conference on Tangible, Embedded, and Embodied Interaction",ACM SIGCHI,"Association for Computing Machinery, Inc",,p 145-154,17-Mar-19,"TEI 2019 - Proceedings of the 13th International Conference on Tangible, Embedded, and Embodied Interaction",2019,,,,9.78145E+12,10.1145/3294109.3295618,,"13th International Conference on Tangible, Embedded, and Embodied Interaction, TEI 2019","March 17, 2019 - March 20, 2019",,"There seems to be a danger to carelessly replace routine tasks in homes through automation with IoT-technology. But since routines such as watering houseplants also have positive influences on inhabitants' wellbeing, they should be transformed through carefully performed designs. To this end, an attempt to use technology for augmenting a set of houseplants' nonverbal communication capabilities is presented. First, we describe in detail how implicit interactions have been designed to support inhabitants in watering their plants through meaningful interactions. Then, we report on a field study with 24 participants, comparing two alternative design implementations based on contrasting embodied interaction technologies (i.e., augmented reality and embedded computing technology). The study results highlight shortcomings of today's smartphone mediated augmented reality compared to physical interface alternatives, considering measurements of perceived attractiveness and expected effects on determinants of wellbeing, and discusses potentials of combining both modalities for future solutions. © 2019 Association of Computing Machinery.",47,Internet of things,Augmented reality,Alternative designs - Embedded computing - Embodied interaction - Experience prototyping - Implicit interaction - Non-verbal communications - Positive Computing - User study,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Sensorless Hand Guidance Using Microsoft Hololens,"Puljiz, David (1); Stöhr, Erik (1); Riesterer, Katharina S. (1); Hein, Björn (1); Kröger, Torsten (1) ","(1) Karlsruhe Institute of Technology, Karlsruhe, Germany ",ACM/IEEE International Conference on Human-Robot Interaction,ACM; ACM SIGAI; ACM SIGCHI; IEEE; IEEE Robotics and Automation Society,IEEE Computer Society,v 2019-March,p 632-633,22-Mar-19,HRI 2019 - 14th ACM/IEEE International Conference on Human-Robot Interaction,2019,,,21672148,9.78154E+12,10.1109/HRI.2019.8673145,8673145,"14th Annual ACM/IEEE International Conference on Human-Robot Interaction, HRI 2019","March 11, 2019 - March 14, 2019",,"Hand guidance of robots has proven to be a useful tool both for programming trajectories and in kinesthetic teaching. However hand guidance is usually relegated to robots possessing joint-torque sensors (JTS). Here we propose to extend hand guidance to robots lacking those sensors through the use of an Augmented Reality (AR) device, namely Microsoft's Hololens. Augmented reality devices have been envisioned as a helpful addition to ease both robot programming and increase situational awareness of humans working in close proximity to robots. We reference the robot by using a registration algorithm to match a robot model to the spatial mesh. The in-built hand tracking capabilities are then used to calculate the position of the hands relative to the robot. By decomposing the hand movements into orthogonal rotations we achieve a completely sensorless hand guidance without any need to build a dynamic model of the robot itself. We did the first tests our approach on a commonly used industrial manipulator, the KUKA KR-5. © 2019 IEEE.",5,Human robot interaction,Augmented reality - Industrial manipulators - Man machine systems - Robot programming,Close proximity - Hand movement - Hand tracking - Joint torque sensors - Kinesthetic teachings - Orthogonal rotations - Registration algorithms - Situational awareness,"723 Computer Software, Data Handling and Applications - 731.5 Robotics - 731.6 Robot Applications",,,"Number: 688117, Acronym: H2020, Sponsor: Horizon 2020 Framework Programme; ",ACKNOWLEDGMENT This work has been supported from the European Unions Horizon 2020 research and innovation programme under grant agreement No 688117 Safe human-robot interaction in logistic applications for highly flexible warehouses (SafeLog).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Deep learning based face recognition system with smart glasses,"Daescu, Ovidiu (1); Huang, Hongyao (1); Weinzierl, Maxwell (1) ","(1) Department of Computer Science, University of Texas at Dallas, Richardson; TX, United States ",ACM International Conference Proceeding Series,The Department of Computer Science and Engineering at UTA; The Human Centered Computing Laboratory (Heracleia) at UTA; The iPerform Industry-University NSF Center at UTA; The National Center for Scientific Research (NCSR)-Demokritos; The National Science Foundation (NSF),Association for Computing Machinery,,p 218-226,5-Jun-19,"Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019",2019,,,,9.78145E+12,10.1145/3316782.3316795,,"12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019","June 5, 2019 - June 7, 2019",,"Individuals with prosopagnosia have difficulty in identifying different people by their faces. Our goal is to design and develop a face recognition system with wearable glasses to recognize faces and provide identity information to users. Unlike other existing systems that run locally on glasses or cellphones, we introduce a client-server architecture system for facial identification. We designed and implemented applications both on a pair of smart glasses and a cellphone to capture images and communicate with the server. Deep Convolutional Neural Networks (CNN) were chosen to build our face recognition on the back-end system and we achieved 98.18% accuracy for face recognition. The system is designed to handle new identities and new faces without having to rebuild the model. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",19,Face recognition,Augmented reality - Client server computer systems - Convolution - Deep neural networks - Glass - Neural networks - Telephone sets - Wearable technology,Client-server architectures - Convolutional neural network - Existing systems - Face recognition systems - Facial identification - Identity information - Prosopagnosia - Wearable devices,"716.1 Information Theory and Signal Processing - 718.1 Telephone Systems and Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 812.3 Glass",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
MathBuilder: A collaborative AR math game for elementary school students,"Van Der Stappen, Almar (1); Liu, Yunjie (1); Xu, Jiangxue (1); Yu, Xiaoyu (1); Li, Jingya (1); Van Der Spek, Erik D. (1) ","(1) Eindhoven University of Technology, Eindhoven; 5612 AZ, Netherlands ",CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 731-738,17-Oct-19,CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play,2019,,,,9.78145E+12,10.1145/3341215.3356295,,"6th ACM SIGCHI Annual Symposium on Computer-Human Interaction in Play, CHI PLAY 2019","October 22, 2019 - October 25, 2019",,"Educational video games have the potential to promote students’ motivation in mathematics learning, which is often considered as one of the most difficult and important subjects for elementary school students. Augmented Reality (AR) games can enhance math learning motivation and collaboration between students-students and students-teachers in classroom settings, allowing them to learn different aspects and gain deeper understanding of the learning content. In this study, we propose a collaborative AR game for elementary school math classroom, called MathBuilder. The goal of MathBuilder is to motivate students in the learning activities and help them to collaborate and communicate more with their classmates and teachers in the classroom. In this paper, we present the design and the development of the game, discuss the potentials of the game in the future together with the result of a preliminary user study. © 2019 Copyright is held by the owner/author(s).",16,Students,Augmented reality - Human computer interaction - Interactive computer systems - Motivation - Serious games,Classroom - Classroom settings - Collaboration - Educational video games - Elementary schools - Learning Activity - Learning motivation - Mathematics learning,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 912.4 Personnel",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
'Magic flowerpot': An AR game for learning about plants,"Zarraonandia, Telmo (1); Montero, Alvaro (1); Díaz, Paloma (1); Aedo, Ignacio (1) ","(1) Computer Science Department, Universidad Carlos III de Madrid, Madrid, Spain ",CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 813-819,17-Oct-19,CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play,2019,,,,9.78145E+12,10.1145/3341215.3356290,,"6th ACM SIGCHI Annual Symposium on Computer-Human Interaction in Play, CHI PLAY 2019","October 22, 2019 - October 25, 2019",,"This paper presents an Augmented Reality (AR) pervasive game that aims to promote learning about the plants in our local environment. Players collect plants in their locale, grow them at home as they grow a real plant, and compose an AR garden with them. The game combines physical elements (i.e. a plant pot with sensors, RFID tags containing plant information) with virtual ones (i.e. AR representations of the plants). The final objective of this research is to investigate how the game experience is affected when moving from the virtual to the real world some components of the game, such as its context, its challenge or its reward. We aim to study if educational benefits can still be obtained when the game is only semi-integrated in the real world. © 2019 Copyright is held by the owner/author(s).",21,Human computer interaction,Augmented reality - Computer games - Interactive computer systems - Ubiquitous computing,Educational benefits - Educational game - Final objective - Game experience - Local environments - Pervasive game - Physical elements - Plant information,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: TIN2016-77690-R, Acronym: METI, Sponsor: Ministry of Economy, Trade and Industry; ","This work is supported by the project PACE funded by the Spanish Ministry of Economy, Industry and Competitiveness (TIN2016-77690-R).",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Poster: Sensor localization system for AR-assisted disaster relief applications,"Choi, Hong-Beom (1); Lim, Keun-Woo (2); Ko, Young-Bae (1) ","(1) Ajou University, Suwon, Korea, Republic of (2) Telecom Paristech, Paris, France ","MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 526-527,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,10.1145/3307334.3328607,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,"In this poster, we propose a sensor localization system assisted by wireless communication and augmented reality (AR) suitable for disaster relief applications. Generally, disaster environments are considered extremely hazardous and deteriorated, with unpredictable effects to human mobility and digital devices. To maximize the safety and efficiency of first responders, deployment of wireless sensors are of utmost importance, as sensor nodes can provide sensing information as well as location information. We analyze the issues and challenges that need to be tackled for high accuracy localization of sensor nodes in such environments, and then propose a system that we plan to develop in the near future. © 2019 Copyright held by the owner/author(s).",5,Emergency services,Augmented reality - Digital devices - Disaster prevention - Sensor nodes,Disaster relief - Issues and challenges - Localization of sensor nodes - Location information - Safety and efficiencies - Sensor localization - Visual odometry - Wireless communications,"722 Computer Systems and Equipment - 723 Computer Software, Data Handling and Applications - 914.1 Accidents and Accident Prevention",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Making CS learning visible: Case studies on how visibility of student work supports a community of learners in CS classrooms,"Solomon, Amber (1); Oguamanam, Vanessa (1); Guzdial, Mark (2); DiSalvo, Betsy (1) ","(1) Georgia Institute of Technology, United States (2) Univerity of Michigan, United States ","Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE",ACM SIGCSE,Association for Computing Machinery,,p 161-167,2-Jul-19,ITiCSE 2019 - Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education,2019,,1942647X,,9.78145E+12,10.1145/3304221.3319791,,"2019 ACM Conference on Innovation and Technology in Computer Science Education, ITiCSE 2019","July 15, 2019 - July 17, 2019",,"Modern learning theories emphasize the critical social aspect of learning. Computer science (CS) classrooms often have 'defensive climates' that inhibit social learning and prevent the development of a community of learners. We believe that we can improve the social context of computer science learning by expanding CS learning beyond the single student in front of a display screen. Our theory is that the single student and single display inhibits collaboration and collaborative awareness of student work. In this paper, we present two case studies where we explored ways to make student work visible to peers. The first case study involved using a studio model for learning enabled by projection-based Augmented Reality (AR), and the second case study involves using a maker-oriented curriculum to make student work visible. Findings suggest the visibility of student work in CS classrooms helped support a community of learners: students collaborated, used each other as sources of inspiration, and felt more comfortable asking for help. © 2019 Association for Computing Machinery.",23,Students,Augmented reality - Computer aided instruction - Education computing - Engineering education - Engineering research - Social aspects - Studios - Visibility,Collaborative awareness - Community of learners - Defensive climate - Design studios - Learning environments - Learning Theory - Social learning - Sources of inspirations,"723 Computer Software, Data Handling and Applications - 741.2 Vision - 901 Engineering Profession",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Demo: RemoteGL - Towards low-latency interactive cloud graphics experience for mobile devices,"Choi, Jaewon (1); Ko, JeongGil (1) ","(1) Ajou University, Korea, Republic of ","MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 693-694,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,10.1145/3307334.3328587,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,"can enable futuristic applications including many Virtual Reality, Augmented Reality, and cloud gaming applications on resource constraint mobile devices. While RGR requires a high-level of networking bandwidth for seamless servicing, emerging high-speed communication technologies such as IEEE 802.11ax and millimeter wavebased communications are expected to provide high-bandwidth and low-latency networking performance. Thus, they can potentially resolve the transmission latency constraint, which is one of the most tricky obstacles to resolve prior to applying various RGR applications in the real world. © 2019 Copyright held by the owner/author(s).",3,IEEE Standards,Augmented reality - Bandwidth - Millimeter waves - Virtual reality,Cloud gamings - High bandwidth - High-speed communications - Latency constraints - Low latency - Real-world - Resource Constraint,"711 Electromagnetic Waves - 716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Leveraging distal vibrotactile feedback for target acquisition,"Henderson, Jay (1); Avery, Jeff (1); Grisoni, Laurent (2); Lank, Edward (1, 3) ","(1) University of Waterloo, Canada (2) Université de Lille, France (3) Inria, France ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300715,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Many touch based interactions provide limited opportunities for direct tactile feedback; examples include multi-user touch displays, augmented reality based projections on passive surfaces, and mid-air input. In this paper, we consider distal feedback, through vibrotactile stimulation on a smart-watch placed on the user’s non-dominant wrist, as an alternative feedback mechanism to interaction location vibrotactile feedback, under the user’s finger. We compare the effectiveness of interaction location feedback vs. distal feedback through a Fitts’s Law task completed on a smartphone. Results show that distal and interaction location feedback both reduce errors in target acquisition and exhibit statistically comparable performance, suggesting that distal vibrotactile feedback is a suitable alternative when interaction location feedback is not readily available. © 2019 Copyright held by the owner/author(s).",36,Location,Augmented reality - Human engineering - Touch screens - Wearable computers,Feedback mechanisms - Passive surfaces - Smartwatch - Tactile feedback - Target acquisition - Targeting - Touch based interactions - Vibro-tactile feedbacks,"461.4 Ergonomics and Human Factors Engineering - 722.2 Computer Peripheral Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Evoisland immersive interactive evolutionary 3D modelling,"Ivanov, Alexander (1); Jacob, Christian (1) ","(1) University of Calgary, Canada ",GECCO 2019 Companion - Proceedings of the 2019 Genetic and Evolutionary Computation Conference Companion,ACM SIGEVO,"Association for Computing Machinery, Inc",,p 373-374,13-Jul-19,GECCO 2019 Companion - Proceedings of the 2019 Genetic and Evolutionary Computation Conference Companion,2019,,,,9.78145E+12,10.1145/3319619.3321979,,"2019 Genetic and Evolutionary Computation Conference, GECCO 2019","July 13, 2019 - July 17, 2019",,"We present EvoIsland, an interactive evolutionary augmented reality interface for parametric 3D OpenSCAD models. Our mobile prototype enables content creators to explore the full range of design possibilities encoded in a model's source code through the combination and separation of hexagonal evolutionary tiles embedded with genetic data. As these tiles are grouped into islands, localized clusters of design populations emerge for creators to explore. Interactions that take place within our EvoIsland prototype provide content creators with a novel approach for shaping evolutionary populations in an immersive environment. © 2019 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery.",5,3D modeling,Augmented reality,3D modelling - Content creators - Evolutionary population - Genetic data - Immersive - Immersive environment - Interactive evolutionary computing - Source codes,"723 Computer Software, Data Handling and Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Crowdsourcing interface feature design with Bayesian optimization,"Dudley, John J. (1); Jacques, Jason T. (1); Kristensson, Per Ola (1) ","(1) Department of Engineering, University of Cambridge, Cambridge, United Kingdom ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300482,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Designing novel interfaces is challenging. Designers typically rely on experience or subjective judgment in the absence of analytical or objective means for selecting interface parameters. We demonstrate Bayesian optimization as an efficient tool for objective interface feature refinement. Specifically, we show that crowdsourcing paired with Bayesian optimization can rapidly and effectively assist interface design across diverse deployment environments. Experiment 1 evaluates the approach on a familiar 2D interface design problem: a map search and review use case. Adding a degree of complexity, Experiment 2 extends Experiment 1 by switching the deployment environment to mobile-based virtual reality. The approach is then demonstrated as a case study for a fundamentally new and unfamiliar interaction design problem: web-based augmented reality. Finally, we show how the model generated as an outcome of the refinement process can be used for user simulation and queried to deliver various design insights. © 2019 Copyright held by the owner/author(s).",29,Crowdsourcing,Augmented reality - Human engineering - Optimization - Virtual reality,Bayesian optimization - Degree of complexity - Feature refinement - Interaction design - Interface designs - Interface parameters - Refinement process - Web-based augmented realities,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 921.5 Optimization Techniques",,,"Number: EP/R004471/1, Acronym: EPSRC, Sponsor: Engineering and Physical Sciences Research Council; ",This work was supported by EPSRC (grant EP/R004471/1) and the Trimble Fund. Supporting data for this paper is available at https://doi.org/10.17863/CAM.34781.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Rotoswype: Word-Gesture Typing using a Ring,"Gupta, Aakar (1); Ji, Cheng (1); Yeo, Hui-Shyong (2); Quigley, Aaron (2); Vogel, Daniel (1) ","(1) Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada (2) School of Computer Science, University of St. Andrews, St. Andrews, United Kingdom ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300244,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"We propose RotoSwype, a technique for word-gesture typing using the orientation of a ring worn on the index finger. RotoSwype enables one-handed text-input without encumbering the hand with a device, a desirable quality in many scenarios, including virtual or augmented reality. The method is evaluated using two arm positions: with the hand raised up with the palm parallel to the ground; and with the hand resting at the side with the palm facing the body. A five-day study finds both hand positions achieved speeds of at least 14 words-per-minute (WPM) with uncorrected error rates near 1%, outperforming previous comparable techniques. © 2019 Copyright held by the owner/author(s).",85,Human engineering,Augmented reality,Arm position - Controlled experiment - Error rate - Hand positions - Index finger - Interaction techniques - One-handed - Text input,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,"Number: #33151, Acronym: CONAE, Sponsor: Comisi&oacute;n Nacional de Actividades Espaciales; Number: 2018-05187, Acronym: NSERC, Sponsor: Natural Sciences and Engineering Research Council of Canada; Number: ER16-12-184, Acronym: ORF, Sponsor: Ontario Research Foundation; ","This work made possible by NSERC Discovery Grant #2018-05187, the Canada Foundation for Innovation Infrastructure Fund Facility for Fully Interactive Physio-digital Spaces"" (#33151)"," and Ontario Early Researcher Award #ER16-12-184.""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,
Mobi3Dsketch: 3D sketching in mobile ar,"Kwan, Kin Chung (1); Fu, Hongbo (1) ","(1) School of Creative Media, City University of Hong Kong, Hong Kong ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300406,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Mid-air 3D sketching has been mainly explored in Virtual Reality (VR) and typically requires special hardware for motion capture and immersive, stereoscopic displays. The recently developed motion tracking algorithms allow real-time tracking of mobile devices, and have enabled a few mobile applications for 3D sketching in Augmented Reality (AR). However, they are more suitable for making simple drawings only, since they do not consider special challenges with mobile AR 3D sketching, including the lack of stereo display, narrow feld of view, and the coupling of 2D input, 3D input and display. To address these issues, we present Mobi3DSketch, which integrates multiple sources of inputs with tools, mainly diferent versions of 3D snapping and planar/curves surface proxies. Our multimodal interface supports both absolute and relative drawing, allowing easy creation of 3D concept designs in situ. The efectiveness and expressiveness of Mobi3DSketch are demonstrated via a pilot study. © 2019 Association for Computing Machinery.",44,Three dimensional displays,Augmented reality - Human engineering - Stereo image processing - Virtual reality,3D sketching - Air drawing - Concept designs - Mobile applications - Mobile interaction - Motion tracking algorithms - Multi-modal interfaces - Stereoscopic display,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
AR in the gallery: The psychogeographer's table,"Franz, Juliano (1); Malloch, Joseph (1); Reilly, Derek (1); Alnusayri, Mohammed (1); Gahlon, Akshay (1) ","(1) Dalhousie University, Halifax; NS, Canada ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312898,3312898,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"In recent years, museums have embraced the use of digital technologies to add interactivity to exhibits. New tools such as wireless beacons, QR codes and markerless trackers paired with powerful smartphones are used to implement applications ranging from guides that provide supplementary material as web pages or audio to spatially precise augmented reality (AR). In this work we explore the use of head-mounted (rather than the more common hand-held) AR in a museum space. Our goal is to explore visitor behaviour when using such technology, to inform the design of a future longitudinal study. We found that visitors enjoyed their experience with head-mounted AR and learned fairly quickly how to navigate and interact with virtual content. © 2019 Copyright held by the owner/author(s).",12,Museums,Augmented reality - Human engineering - Websites,Digital heritage - Digital technologies - Interactivity - Longitudinal study - Markerless - QR codes,"402.2 Public Buildings - 461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Use of new technologies in physics studying,"Daineko, Ye.A. (1, 2); Ipalakova, M.T. (1); Tsoy, D.D. (1); Baurzhan, Zh.B. (1); Yelgondy, Ye.K. (1); Bolatov, Zh.Zh. (1); Seitnur, A.M. (1); Zhaksylyk, A. (1) ","(1) International University of Information Technologies, Almaty, Kazakhstan (2) Institute of Applied Sciences and Information Technologies, Almaty, Kazakhstan ",ACM International Conference Proceeding Series,"International Association of Researchers (IARES Inc., Canada)",Association for Computing Machinery,,,6-Jun-19,"Proceedings of the 5th International Conference on Engineering and MIS, ICEMIS 2019",2019,,,,9.78145E+12,10.1145/3330431.3330450,19,"5th International Conference on Engineering and MIS, ICEMIS 2019","June 6, 2019 - June 8, 2019",,"Modern education is impossible to imagine without information and communication technologies, which are developing at an enormous pace. Currently, the perception of students can be improved through the use of new technologies. The results of the development of the software application 'Electronic laboratory (E-lab)' are presented. The lab is implemented as an application with a set of practical tasks, laboratory works, animations and theoretical tasks to study physics using augmented and virtual reality technologies. Unity 3D cross-platform environment was chosen as the development platform. The main functionality was written in C#. Graphic models were created using Substance Painter. The article describes the development process of the presented application, its functionality, as well as the user interface. © 2019 ACM.",15,Application programs,Augmented reality - Education computing - Laboratories - Physics - User interfaces - Virtual reality,Augmented and virtual realities - Development platform - Development process - Information and Communication Technologies - Laboratory work - Modern educations - Software applications - Virtual experiments,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: -, Sponsor: Ministry of Education and Science of the Republic of Kazakhstan; ",The work was done under the funding of the Ministry of Education and Science of the Republic of Kazakhstan 2018-2020 (No. AP05135692).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
BalloonFab: Digital fabrication of large-scale balloon art,"Xie, Haoran (1); Chen, Naiyun (2); Chang, Chia-Ming (2); Peng, Yichen (1); Xie, Dazhao (1); Miyata, Kazunori (1) ","(1) Japan Advanced Institute of Science and Technology Ishikawa, Japan (2) National Chiao Tung University, Hsinchu, Taiwan ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312947,3312947,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"We propose an interactive system that allows common users to build large-scale balloon art based on a spatial augmented reality solution. The proposed system provides fabrication guidance to illustrate the differences between the depth maps of the target three-dimensional shape and the current work in progress. Instead of using color gradients for depth difference, we adopt a high contrast black and white projection of the numbers in considering balloon texture. In order to increase user immersion in the system, we propose a shaking animation for each projected number. Using the proposed system, the unskilled users in our case study were able to build large scale balloon art. © 2019 Copyright held by the owner/author(s).",7,Arts computing,Augmented reality - Balloons - Fabrication - Human engineering - Textures,Color gradients - Digital fabrication - Interactive system - Large-scale fabrication - Spatial ar - Spatial augmented realities - Three-dimensional shape - Work in progress,"461.4 Ergonomics and Human Factors Engineering - 652.5 Balloons and Gliders - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,"Number: JP17H06574, Acronym: JSPS, Sponsor: Japan Society for the Promotion of Science; ","This work is supported by JSPS KAKENHI grant JP17H06574, Japan and Higher Education Sprout Project grant 107-2200-001, Ministry of Education, Taiwan.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"This is not a tile, just a digital remix","Antunes, Elisa (1); Bidarra, José (2); Da Câmara, Alexandra Gago (2) ","(1) Centro de Investigação em Artes e Comunicação, Universidade Do Algarve, Faro, Portugal (2) Universidade Aberta Lisboa Portugal, Centro de Investigação em Artes e Comunicação, Universidade Do Algarve, Faro, Portugal ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,23-Oct-19,ARTECH 2019: Digital Media Art Ecosystems - Proceedings of the 9th International Conference on Digital and Interactive Arts,2019,Portuguese,,,9.78145E+12,10.1145/3359852.3359917,a59,"9th International Conference on Digital and Interactive Arts: Digital Media Art Ecosystems, ARTECH 2019","October 23, 2019 - October 25, 2019",,"The main goal of the conference is to promote the interest in the current digital culture and its intersection with art and technology as an important research field, and also to create a common space for discussion and exchange of new experiences. It seeks to foster greater understanding about digital arts and culture across a wide spectrum of cultural, disciplinary, and professional practices. To this end, many scholars, teachers, researchers, artists, comput-er professionals, and others who are working within the broadly defined areas of digital arts, culture and education across the world, submitted their innovative work to the conference. © 2019 ACM.",8,Arts computing,Augmented reality - Digital storage - Ecosystems,Art and technology - Common spaces - Digital culture - Digital narratives - Digital remix - Professional practices - Research fields - Wide spectrum,"454.3 Ecology and Ecosystems - 722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
I sent it: Where does slow data go to wait?,"Im, Youngbin (1); Rahimzadeh, Parisa (1); Shouse, Brett (1); Park, Shinik (2); Joe-Wong, Carlee (3); Lee, Kyunghan (2); Ha, Sangtae (1) ","(1) University of Colorado, Boulder, United States (2) Ulsan National Institute of Science and Technology, Korea, Republic of (3) Carnegie Mellon University, United States ",Proceedings of the 14th EuroSys Conference 2019,ACM Special Interest Group on Operating Systems (SIGOPS),"Association for Computing Machinery, Inc",,,25-Mar-19,Proceedings of the 14th EuroSys Conference 2019,2019,,,,9.78145E+12,10.1145/3302424.3303961,3303961,"14th European Conference on Computer Systems, EuroSys 2019","March 25, 2019 - March 28, 2019",,"Emerging applications like virtual reality (VR), augmented reality (AR), and 360-degree video aim to exploit the unprecedentedly low latencies promised by technologies like the tactile Internet and mobile 5G networks. Yet these promises are still unrealized. In order to fulfill them, it is crucial to understand where packet delays happen, which impacts protocol performance such as throughput and latency. In this work, we empirically find that sender-side protocol stack delays can cause high end-to-end latencies, though existing solutions primarily address network delays. Unfortunately, however, current latency diagnosis tools cannot even distinguish between delays on network links and delays in the end hosts. To close this gap, we present ELEMENT, a latency diagnosis framework that decomposes end-to-end TCP latency into endhost and network delays, without requiring admin privileges at the sender or receiver. We validate that ELEMENT achieves more than 90% accuracy in delay estimation compared to the ground truth in different production networks. To demonstrate ELEMENT’s potential impact on real-world applications, we implement a relatively simple user-level library that uses ELEMENT to minimize delays. For evaluation, we integrate ELEMENT with legacy TCP applications and show that it can reduce latency by up to 10 times while maintaining throughput and fairness. We finally demonstrate that ELEMENT can significantly reduce the latency of a virtual reality application that needs extremely low latencies and high throughput. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",59,5G mobile communication systems,Augmented reality - Transmission control protocol - Virtual reality,Emerging applications - End to end latencies - Latency control - Measurement tools - Potential impacts - Production network - Protocol performance - TCP latency,"723 Computer Software, Data Handling and Applications",,,"Number: 2018-0-00693, Acronym: KUT, Sponsor: Korea University of Technology and Education; Number: CNS-1525435, Acronym: NSF, Sponsor: National Stroke Foundation; Number: HR001117C0048, Acronym: DARPA, Sponsor: Defense Advanced Research Projects Agency; ","We thank our shepherd Costin Raiciu and the anonymous reviewers for their insightful comments and feedback. This work was supported by the NSF under Grant CNS-1525435, in part by the Defense Advanced Research Projects Agency (DARPA), under contract No. HR001117C0048, and in part by IITP grants funded by the Korea government (MSIT) (No. 2018-0-00693, Development of An Ultra Low-Latency User-Level Transfer Protocol and No. 2017-0-00562, UDP-based Ultra Low-Latency Transport Protocol with Mobility Support).",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Mixed reality remote collaboration combining 360 video and 3D reconstruction,"Teo, Theophilus (1); Lawrence, Louise (1); Lee, Gun A. (1); Billinghurst, Mark (1); Adcock, Matt (2) ","(1) University of South Australia, Adelaide, Australia (2) CSIRO, Canberra, Australia ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300431,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Remote Collaboration using Virtual Reality (VR) and Augmented Reality (AR) has recently become a popular way for people from different places to work together. Local workers can collaborate with remote helpers by sharing 360-degree live video or 3D virtual reconstruction of their surroundings. However, each of these techniques has benefits and drawbacks. In this paper we explore mixing 360 video and 3D reconstruction together for remote collaboration, by preserving benefits of both systems while reducing drawbacks of each. We developed a hybrid prototype and conducted user study to compare benefits and problems of using 360 or 3D alone to clarify the needs for mixing the two, and also to evaluate the prototype system. We found participants performed significantly better on collaborative search tasks in 360 and felt higher social presence, yet 3D also showed potential to complement. Participant feedback collected after trying our hybrid system provided directions for improvement. © 2019 Association for Computing Machinery.",39,Image reconstruction,Augmented reality - Human engineering - Hybrid systems - Mixed reality - Mixing - Three dimensional computer graphics - Virtual reality,360 panorama - 3D reconstruction - 3D scene reconstruction - Collaborative searches - Interaction methods - Participant feedback - Remote collaboration - Virtual reconstruction,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 802.3 Chemical Operations - 921 Mathematics",,,"Number: -, Acronym: CSIRO, Sponsor: Commonwealth Scientific and Industrial Research Organisation; ",This research is supported by a CSIRO Data61 PhD scholarship and South Australian Research Fellowship project.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
New interactive VR/AR Media: A challenge worth to address,"Quadrio, Giacomo (1); Bujari, Armir (1); Gaggi, Ombretta (1); Palazzi, Claudio E. (1) ","(1) Department of Mathematics Tullio Levi-Civita, University of Padua, Padua, Italy ",ACM International Conference Proceeding Series,The European Alliance for Innovation (EAI),Association for Computing Machinery,,p 128-129,25-Sep-19,"Proceedings of the 5th EAI International Conference on Smart Objects and Technologies for Social Good, GOODTECHS 2019",2019,,,,9.78145E+12,10.1145/3342428.3342676,,"5th EAI International Conference on Smart Objects and Technologies for Social Good, GOODTECHS 2019","September 25, 2019 - September 27, 2019",,"The advent of broadband technology has revolutionized the fruition of multimedia contents. Moreover, the cloud is now moving the computational load from domestic devices to remote servers, enabling the provisioning of new multimedia services through the Internet. To provide a proper user experience, a low E2E latency and a high bandwidth are both required. Unfortunately, this is difficult to achieve in a shared network like the Internet. My PhD project focuses on the study and analysis of modern network protocols to identify viable solutions for a suitable user experience. © 2019 ACM.",9,Multimedia services,Augmented reality - Interactive computer systems - Multimedia systems - Network protocols - Virtual reality,Broadband technologies - Computational loads - High bandwidth - Interactive multimedia - Multimedia contents - QUIC - User experience - Viable solutions,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
EdgeDroid: An experimental approach to benchmarking human-in-the-loop applications,"Olguín Moz, Manuel Osvaldo J. (1); Wang, Junjue (2); Satyanarayanan, Mahadev (2); Gross, James (1) ","(1) School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden (2) School of Computer Science, Carnegie Mellon University, Pittsburgh; PA, United States ",HotMobile 2019 - Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications,ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 93-98,22-Feb-19,HotMobile 2019 - Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications,2019,,,,9.78145E+12,10.1145/3301293.3302353,3302353,"20th International Workshop on Mobile Computing Systems and Applications, HotMobile 2019","February 27, 2019 - February 28, 2019",,"Many emerging mobile applications, including augmented reality (AR) and wearable cognitive assistance (WCA), aim to provide seamless user interaction. However, the complexity of benchmarking these human-in-the-loop applications limits reproducibility and makes performance evaluation difficult. In this paper, we present EdgeDroid, a benchmarking suite designed to reproducibly evaluate these applications. Our core idea rests on recording traces of user interaction, which are then replayed at benchmarking time in a controlled fashion based on an underlying model of human behavior. This allows for an automated system that greatly simplifies benchmarking large scale scenarios and stress testing the application. Our results show the benefits of EdgeDroid as a tool for both system designers and application developers. © 2019 Authors.",12,Benchmarking,Augmented reality - Automation - Behavioral research - Edge computing - Mobile computing,Application developers - Automated systems - Cloudlet - Cognitive assistance - Experimental approaches - Human-in-the-loop - Mobile applications - Reproducibilities,"723 Computer Software, Data Handling and Applications - 731 Automatic Control Principles and Applications - 971 Social Sciences",,,"Number: CNS-1518865, Acronym: NSF, Sponsor: National Science Foundation; ","We thank Bobby Klatzky and Dan Siewiorek for many valuable technical discussions relating to this research. We also thank our shepherd,Wenjun Hu, and the anonymous reviewers for helping us improve the paper. This research was supported in part by the National Science Foundation (NSF), grant number CNS-1518865, the VINNOVA grant MERIT (2017-05232). Additional support was provided by Intel, Vodafone, Deutsche Telekom, Verizon, Crown Castle, NTT, and the Conklin Kistler family fund. Opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view(s) of their employers or the mentioned funding sources.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Brainshare: A glimpse of social interaction for locked-in syndrome patients,"Faltaous, Sarah (1); Haas, Gabriel (2); Barrios, Liliana (3); Seiderer, Andreas (4); Rauh, Sebastian Felix (5); Chae, Han Joo (6); Schneegass, Stefan (1); Alt, Florian (7) ","(1) University of Duisburg-Essen, Essen, Germany (2) Ulm University, Ulm, Germany (3) ETH Zurich, Zurich, Switzerland (4) Augsburg University, Augsburg, Germany (5) Heilbronn University, Heilbronn, Germany (6) Seoul National University, Seoul, Korea, Republic of (7) Bundeswehr University Munich, Munich, Germany ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312754,3312754,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Locked-in syndrome (LIS) patients are partially or entirely paralyzed but fully conscious. Those patients report a high quality of life and desire to remain active in their society and families. We propose a system for enhancing social interactions of LIS patients with their families and friends with the goal of improving their overall quality of life. Our system comprises a Brain-Computer Interface (BCI), augmented-reality glasses, and a screen that shares the view of a caretaker with the patient. This setting targets both patients and caretakers: (1) it allows the patient to experience the outside world through the eyes of the caretaker and (2) it creates a way of active communication between patient and caretaker to convey needs and advice. To validate our approach, we showcased our prototype and conducted interviews that demonstrate the potential benefit for affected patients. © 2019 Copyright held by the owner/author(s).",14,Medical computing,Augmented reality - Brain computer interface - Human engineering - Locks (fasteners),Active communications - Brain computer interactions - High quality - Locked-in syndrome - Overall quality - Potential benefits - Social interactions,"461.1 Biomedical Engineering - 461.4 Ergonomics and Human Factors Engineering - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Creating a Shared Reality with Robots,"Muhammad, Faizan (1); Hassan, Amel (1); Cleaver, Andre (1); Sinapov, Jivko (1) ","(1) Department of Computer Science, Tufts University, United States ",ACM/IEEE International Conference on Human-Robot Interaction,ACM; ACM SIGAI; ACM SIGCHI; IEEE; IEEE Robotics and Automation Society,IEEE Computer Society,v 2019-March,p 614-615,22-Mar-19,HRI 2019 - 14th ACM/IEEE International Conference on Human-Robot Interaction,2019,,,21672148,9.78154E+12,10.1109/HRI.2019.8673191,8673191,"14th Annual ACM/IEEE International Conference on Human-Robot Interaction, HRI 2019","March 11, 2019 - March 14, 2019",,"This paper outlines the system design, capabilities and potential applications of an Augmented Reality (AR) framework developed for Robot Operating System (ROS) powered robots. The goal of this framework is to enable high-level human-robot collaboration and interaction. It allows the users to visualize the robot's state in intuitive modalities overlaid onto the real world and interact with AR objects as a means of communication with the robot. Thereby creating a shared environment in which humans and robots can interact and collaborate. © 2019 IEEE.",5,Human robot interaction,Augmented reality - Intelligent robots - Machine design - Man machine systems,Human-robot collaboration - Real-world - Robot operating systems (ROS),"601 Mechanical Design - 723 Computer Software, Data Handling and Applications - 731.5 Robotics - 731.6 Robot Applications",,,,"This research has been conducted with funding support from the Tufts Center for Applied Brain and Cognitive Sciences (CABCS), and PTC Inc.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Mixed reality patients monitoring application for critical care nurses,"Teng, Chia-Chi (1); Redfearn, Brady (1); Nuttall, Craig (2); Jarvis, Sabrina (2); Carr, James (1); Jensen, Jarin (1); Kanuch, Sandy (1); Peterson, Jordon (1); Taylor, David (1) ","(1) Information Technology, Brigham Young University, Provo; UT, United States (2) College of Nursing, Brigham Young University, Provo; UT, United States ",ACM International Conference Proceeding Series,University of Electronic Science and Technology of China,Association for Computing Machinery,,p 49-53,17-May-19,"Proceedings of the 2019 3rd International Conference on Medical and Health Informatics, ICMHI 2019",2019,,,,9.78145E+12,10.1145/3340037.3340050,,"3rd International Conference on Medical and Health Informatics, ICMHI 2019","May 17, 2019 - May 19, 2019",,"Recent research and development projects have demonstrated the use of virtual reality (VR) technology in the healthcare environment, although most applications are still limited to medical simulations and training applications. Whereas VR removes a user from their immediate environment, augmented and mixed reality (AR/MR) adds virtual content to a user's immediate environment. We endeavor to develop an MR application for hospital medical providers which will enable them to directly monitor any patient and their pertinent medical information from any location, at a glance. With this system, the provider could view a live-stream of patients from other locations and their vital signs to provide information for rapid medical decision making. Such application could significantly improve patient safety, allow quicker response times for emergency and critical situations, and reduce medical errors. It could also enhance the effectiveness of the medical team and allow the providers to more closely monitor their patients, improving patient care outcomes and decreasing costs. A prototype of this proposed MR application is developed with a state-of-the-art head mounted display and the result is presented below. © 2019 ACM.",17,Mixed reality,Augmented reality - Decision making - Health care - Helmet mounted displays - Medical informatics - Medical information systems - Software prototyping - Virtual reality,Head mounted displays - Healthcare environments - Healthcare information technology - Immediate environment - Medical decision making - Medical simulations - Monitoring applications - Training applications,"461.7 Health Care - 723 Computer Software, Data Handling and Applications - 723.1 Computer Programming - 903 Information Science - 912.2 Management",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Addressing bandwidth demand in full-immersive virtual reality,"Bujari, Armir (1); Palazzi, Claudio E. (1); Perale, Dilan (1) ","(1) Department of Mathematics, University of Padua, Padua, Italy ",ACM International Conference Proceeding Series,The European Alliance for Innovation (EAI),Association for Computing Machinery,,p 13-18,25-Sep-19,"Proceedings of the 5th EAI International Conference on Smart Objects and Technologies for Social Good, GOODTECHS 2019",2019,,,,9.78145E+12,10.1145/3342428.3342670,,"5th EAI International Conference on Smart Objects and Technologies for Social Good, GOODTECHS 2019","September 25, 2019 - September 27, 2019",,"Augmented and virtual reality technology are expected to become key technological players in the next wave of consumer electronics with potential applications spanning a wide variety of economic sectors. Yet, there are a number of technological hurdles which need to be addressed. Without speed and computation capability interactivity cannot work. More than often the constrained nature of the mobile device demands are complemented by cloud ones through the use of cloud offloading techniques. With cloud offloading, battery consumption and processing performance problems are translated into sensor data selection and network latency problems. In this context, we focus our study on a network function which could be deployed in a 5G virtualized architecture to alleviate bandwidth requirements of immersive application scenarios. To this end, we rely on a real dataset, providing user-centric metadata on several 4k, 360-degree videos. Different computational methods are proposed and contrasted, evidencing the goodput of the approaches. © 2019 ACM.",17,5G mobile communication systems,Augmented reality - Bandwidth - Network function virtualization - Virtual reality,Augmented and virtual realities - Bandwidth requirement - Battery consumption - Equirectangular - Immersive application - Immersive virtual reality - Network latencies - Processing performance,"716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Mobile device interaction using projector metaphor,"Ro, Hyocheol (1); Park, Yoon Jung (1); Byun, Jung-Hyun (1); Han, Tack-Don (1) ","(1) Yonsei University, Seoul, Korea, Republic of ","International Conference on Intelligent User Interfaces, Proceedings IUI",ACM Special Interest Group on Artificial Intelligence (SIGAI); ACM Special Interest Group on Computer-Human Interaction (SIGCHI),Association for Computing Machinery,,p 47-48,16-Mar-19,"Proceedings of the 24th International Conference on Intelligent User Interfaces, IUI 2019",2019,,,,9.78145E+12,10.1145/3308557.3308672,,"24th International Conference on Intelligent User Interfaces, IUI 2019","March 16, 2019 - March 20, 2019",,"Projection Augmented Reality (AR), which uses projectors to augment virtual information and objects in physical space, is one of the AR fields that have been actively researched. There have been many attempts to use mobile devices to implement the interactions available in these AR environments; however, due to mobile devices' limited computational power, they were somewhat difficult to use in real applications. We have solved this problem with the latest off-the-shelf devices and support development kit and have proven, through experimentation, that our solution can be used in actual projection AR applications. © 2019 ACM.",4,User interfaces,Augmented reality,AR application - Computational power - Intelligent User Interfaces - Mobile device interactions - Off-the-shelf devices - Real applications - Virtual information,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
On-Road Evaluation of Autonomous Driving Training,"Sportillo, Daniele (1); Paljic, Alexis (1); Ojeda, Luciano (2) ","(1) PSL Research University, Center for Robotics, MINES ParisTech Groupe PSA, Paris, France (2) Groupe PSA, Velizy-Villacoublay, France ",ACM/IEEE International Conference on Human-Robot Interaction,ACM; ACM SIGAI; ACM SIGCHI; IEEE; IEEE Robotics and Automation Society,IEEE Computer Society,v 2019-March,p 182-190,22-Mar-19,HRI 2019 - 14th ACM/IEEE International Conference on Human-Robot Interaction,2019,,,21672148,9.78154E+12,10.1109/HRI.2019.8673277,8673277,"14th Annual ACM/IEEE International Conference on Human-Robot Interaction, HRI 2019","March 11, 2019 - March 14, 2019",,"Driver interaction with increasingly automated vehicles requires prior knowledge of system capabilities, operational know-how to use novel car equipment and responsiveness to unpredictable situations. With the purpose of getting drivers ready for autonomous driving, in a between-subject study sixty inexperienced participants were trained with an on-board video tutorial, an Augmented Reality (AR) program and a Virtual Reality (VR) simulator. To evaluate the transfer of training to real driving scenarios, a test drive on public roads was conducted implementing, for the first time in these conditions, the Wizard of Oz (WoZ) protocol. Results suggest that VR and AR training can foster knowledge acquisition and improve reaction time performance in take-over requests. Moreover, participants' behavior during the test drive highlights the ecological validity of the experiment thanks to the effective implementation of the WoZ methodology. © 2019 IEEE.",36,Human robot interaction,Augmented reality - Autonomous vehicles - Human reaction time - Man machine systems - Network security - Technology transfer - Virtual reality,Automated vehicles - Autonomous driving - Driver interaction - Ecological validity - Human vehicle interactions - System capabilities - Transfer of trainings - Wizard of Oz,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,"Number: CIFRE 2015/1392, Acronym: -, Sponsor: -; ",This research was supported by the French Foundation of Technological Research under grant CIFRE 2015/1392 for the doctoral work of D. Sportillo at Groupe PSA.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Online, VR, Ar, lab, and in-situ: Comparison of research methods to evaluate smart artifacts","Voit, Alexandra (1); Mayer, Sven (1); Schwind, Valentin (1); Henze, Niels (2) ","(1) University of Stuttgart, Stuttgart, Germany (2) University of Regensburg, Regensburg, Germany ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300737,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Empirical studies are a cornerstone of HCI research. Technical progress constantly enables new study methods. Online surveys, for example, make it possible to collect feedback from remote users. Progress in augmented and virtual reality enables to collect feedback with early designs. In-situ studies enable researchers to gather feedback in natural environments. While these methods have unique advantages and disadvantages, it is unclear if and how using a specific method affects the results. Therefore, we conducted a study with 60 participants comparing five different methods (online, virtual reality, augmented reality, lab setup, and in-situ) to evaluate early prototypes of smart artifacts. We asked participants to assess four different smart artifacts using standardized questionnaires. We show that the method significantly affects the study result and discuss implications for HCI research. Finally, we highlight further directions to overcome the effect of the used methods. © 2019 Copyright held by the owner/author(s).",43,Surveys,Augmented reality - Human engineering - Surveying - Virtual reality,Augmented and virtual realities - Empirical method - Empirical studies - Natural environments - Prototype evaluation - Smart artifacts - Technical progress - User study,"405.3 Surveying - 461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,"Number: EXC 310/2, Acronym: DFG, Sponsor: Deutsche Forschungsgemeinschaft; ",This work was financially supported by the German Research Foundation (DFG) within Cluster of Excellence in Simulation Technology (EXC 310/2) at the University of Stuttgart and through project C04 of SFB/Transregio 161.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Character alive: A Tangible Reading and Writing System for Chinese Children At-risk for Dyslexia,"Fan, Min (1); Antle, Alissa N. (1); Yin, Dongxu (2); Fan, Jianyu (1); Jin, Sheng (2); Pasquier, Philippe (1) ","(1) Simon Fraser University, 250-13450 102 Ave, Surrey; B.C., Canada (2) Communication University of Zhejiang, 998 Xueyuan St, Hangzhou, Zhejiang, China ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312756,3312756,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"This paper presents Character Alive, a tangible system designed to improve early Chinese literacy acquisition for Mandarin-speaking children at-risk for dyslexia by enabling high-level interaction. Character Alive uses the multisensory training method to teach children the reading and writing of Chinese characters and words. The core design features of our system are dynamic color cues, 2D radical cards and handwriting cards with tactile cues, and multimedia content such as character animations. Character Alive was built on our previous work on designing tangible and augmented reality reading and writing systems for children at-risk for dyslexia in English. Our previous work has demonstrated that dynamic color cues can draw children's attention to key characteristics of letter-sound-correspondences, whereas the two-hand actions with tangible letters help children to better solve spelling tasks. We present the design rationale, the design and implementation of the Character Alive system, and the future plan on evaluating the system. © 2019 Copyright is held by the owner/ author(s).",13,Color,Augmented reality - Human engineering - Mergers and acquisitions - User interfaces,Children with dyslexia - Chinese acquisition - Color cues - Reading and writing acquisition - Tactile cues - Tangible user interfaces,"461.4 Ergonomics and Human Factors Engineering - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 741.1 Light/Optics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ARPeN: Mid-air object manipulation techniques for a bimanual AR system with pen & smartphone,"Wacker, Philipp (1); Nowak, Oliver (1); Voelker, Simon (1); Borchers, Jan (1) ","(1) RWTH Aachen University, Aachen; 52056, Germany ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300849,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Modeling in Augmented Reality (AR) lets users create and manipulate virtual objects in mid-air that are aligned to their real environment. We present ARPen, a bimanual input technique for AR modeling that combines a standard smartphone with a 3D-printed pen. Users sketch with the pen in mid-air, while holding their smartphone in the other hand to see the virtual pen traces in the live camera image. ARPen combines the pen’s higher 3D input precision with the rich interactive capabilities of the smartphone touchscreen. We studied subjective preferences for this bimanual input technique, such as how people hold the smartphone while drawing, and analyzed the performance of different bimanual techniques for selecting and moving virtual objects. Users preferred a bimanual technique casting a ray through the pen tip for both selection and translation. We provide initial design guidelines for this new class of bimanual AR modeling systems. © 2019 Copyright held by the owner/author(s).",44,Smartphones,3D printers - Augmented reality - Human engineering,Air objects - Bi-manual interaction - Immersive modeling - Initial design - Input techniques - Mid-air interaction - Real environments - Virtual objects,"461.4 Ergonomics and Human Factors Engineering - 718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications - 745.1.1 Printing Equipment",,,,We want to thank all our participants for taking part in the studies. This work was funded by the German Federal Ministry of Education and Research as part of the Photonics Research Germany (13N14065).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Turning your book into a game: Improving motivation through tangible interaction and diegetic feedback in an AR mathematics game for children,"Li, Jingya (1); Van Der Spek, Erik D. (1); Hu, Jun (1); Feijs, Loe (1) ","(1) Industrial Design Department, Eindhoven University of Technology, Eindhoven, Netherlands ",CHI PLAY 2019 - Proceedings of the Annual Symposium on Computer-Human Interaction in Play,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 73-85,17-Oct-19,CHI PLAY 2019 - Proceedings of the Annual Symposium on Computer-Human Interaction in Play,2019,,,,9.78145E+12,10.1145/3311350.3347174,,"6th ACM SIGCHI Annual Symposium on Computer-Human Interaction in Play, CHI PLAY 2019","October 22, 2019 - October 25, 2019",,"Augmented reality game-based learning has become an emerging trend in the field of education as it has the potential to increase children’s learning motivation for subjects such as mathematics. However, to achieve the benefits for children effectively, AR serious games need to be appropriately designed, especially in respect to their novel interactions and representation paradigms. In this paper, we report on an exploratory experiment to investigate how different interaction techniques (digital screen-touch interaction vs real-world tangible interaction) and different feedback mechanisms (non-diegetic feedback vs diegetic feedback) affect 7-8-year-old children’s motivation for mathematics learning. Our results show that diegetic feedback led to the game being considered significantly more enjoyable, as well as inducing greater feelings of competence and autonomy; screen-touch interaction versus tangible interaction did not change motivation directly, nor did we find interaction effects between the presentation and interaction modes. By analyzing the results and based on previous studies, we identify recommendations for designers to develop motivating serious AR games for children. © 2019 Association for Computing Machinery.",58,Human computer interaction,Augmented reality - Interactive computer systems - Motivation - Serious games - Touch screens,Children - Feedback mechanisms - Game design - Game-based Learning - Interaction techniques - Learning motivation - Mathematics learning - Tangible interaction,"722.2 Computer Peripheral Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 912.4 Personnel",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Mixed Reality Deictic Gesture for Multi-Modal Robot Communication,"Williams, Tom (1); Bussing, Matthew (1); Cabrol, Sebastian (1); Boyle, Elizabeth (1); Tran, Nhan (1) ","(1) Colorado School of Mines, MIRRORLab, Golden; CO, United States ",ACM/IEEE International Conference on Human-Robot Interaction,ACM; ACM SIGAI; ACM SIGCHI; IEEE; IEEE Robotics and Automation Society,IEEE Computer Society,v 2019-March,p 191-201,22-Mar-19,HRI 2019 - 14th ACM/IEEE International Conference on Human-Robot Interaction,2019,,,21672148,9.78154E+12,10.1109/HRI.2019.8673275,8673275,"14th Annual ACM/IEEE International Conference on Human-Robot Interaction, HRI 2019","March 11, 2019 - March 14, 2019",,"In previous work, researchers have repeatedly demonstrated that robots' use of deictic gestures enables effective and natural human-robot interaction. However, new technologies such as augmented reality head mounted displays enable environments in which mixed-reality becomes possible, and in such environments, physical gestures become but one category among many different types of mixed reality deictic gestures. In this paper, we present the first experimental exploration of the effectiveness of mixed reality deictic gestures beyond physical gestures. Specifically, we investigate human perception of videos simulating the display of allocentric gestures, in which robots circle their targets in users' fields of view. Our results suggest that this is an effective communication strategy, both in terms of objective accuracy and subjective perception, especially when paired with complex natural language references. © 2019 IEEE.",99,Human robot interaction,Augmented reality - Helmet mounted displays - Man machine systems - Mixed reality - Natural language processing systems,Deixis - Effective communication - Head mounted displays - Human perception - Natural language generation - Natural languages - Robot communication - Subjective perceptions,"723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 731.5 Robotics",,,"Number: CNS-1823245, Acronym: NSF, Sponsor: National Science Foundation; ",This work was supported by NSF CNS-1823245.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Virtual, Augmented, and Mixed Reality for Human-Robot Interaction (VAM-HRI)","Williams, Tom (1); Szafir, Daniel (2); Chakraborti, Tathagata (3); Phillips, Elizabeth (4) ","(1) Colorado School of Mines, Golden; CO; 80401, United States (2) University of Colorado Boulder, Boulder; CO; 80309, United States (3) IBM Research AI, Cambridge; MA; 02142, United States (4) Air Force Academy, U. S. Air Force Academy; CO; 80920, United States ",ACM/IEEE International Conference on Human-Robot Interaction,ACM; ACM SIGAI; ACM SIGCHI; IEEE; IEEE Robotics and Automation Society,IEEE Computer Society,v 2019-March,p 671-672,22-Mar-19,HRI 2019 - 14th ACM/IEEE International Conference on Human-Robot Interaction,2019,,,21672148,9.78154E+12,10.1109/HRI.2019.8673207,8673207,"14th Annual ACM/IEEE International Conference on Human-Robot Interaction, HRI 2019","March 11, 2019 - March 14, 2019",,"The 2 nd International Workshop on Virtual, Augmented, and Mixed Reality for Human-Robot Interactions (VAM-HRI) will bring together HRI, Robotics, and Mixed Reality researchers to identify challenges in mixed reality interactions between humans and robots. Topics relevant to the workshop include development of robots that can interact with humans in mixed reality, use of virtual reality for developing interactive robots, the design of new augmented reality interfaces that mediate communication between humans and robots, comparisons of the capabilities and perceptions of robots and virtual agents, and best design practices. VAM-HRI was held for the first time at HRI 2018, where it served as the first workshop of its kind at an academic AI or Robotics conference, and served as a timely call to arms to the academic community in response to the growing promise of this emerging field. VAM-HRI 2019 will follow on the success of VAM-HRI 2018, and present new opportunities for expanding this nascent research community. Website http://vam-hri.xyz/ © 2019 IEEE.",23,Human robot interaction,Augmented reality - Machine design - Man machine systems - Mixed reality - Robotics - Virtual reality,Academic community - Design practice - Interactive robot - International workshops - Reality interface - Research communities - Virtual agent,"601 Mechanical Design - 723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
On the shoulder of the giant: A multi-scale mixed reality collaboration with 360 video sharing and tangible interaction,"Piumsomboon, Thammathip (1, 2); Lee, Gun A. (1); Irlitti, Andrew (1); Ens, Barrett (3); Thomas, Bruce H. (1); Billinghurst, Mark (1) ","(1) School of ITMS, University of South Australia, Mawson Lakes; SA, Australia (2) School of Product Design, University of Canterbury, Christchurch, New Zealand (3) Immersive Analytics Lab., Monash University, Melbourne; VIC, Australia ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300458,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"We propose a multi-scale Mixed Reality (MR) collaboration between the Giant, a local Augmented Reality user, and the Miniature, a remote Virtual Reality user, in Giant-Miniature Collaboration (GMC). The Miniature is immersed in a 360-video shared by the Giant who can physically manipulate the Miniature through a tangible interface, a combined 360-camera with a 6 DOF tracker. We implemented a prototype system as a proof of concept and conducted a user study (n=24) comprising of four parts comparing: A) two types of virtual representations, B) three levels of Miniature control, C) three levels of 360-video view dependencies, and D) four 360-camera placement positions on the Giant. The results show users prefer a shoulder mounted camera view, while a view frustum with a complimentary avatar is a good visualization for the Miniature virtual representation. From the results, we give design recommendations and demonstrate an example Giant-Miniature Interaction. © 2019 Association for Computing Machinery.",76,Mixed reality,Augmented reality - Human engineering - Interactive computer graphics - Interactive computer systems - User interfaces,Live panorama sharing - Multi-scale - Remote collaboration - Tangible user interfaces - Wearable interfaces,"461.4 Ergonomics and Human Factors Engineering - 722.2 Computer Peripheral Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Validating the Accuracy of Imaged-Based Research into the Uncanny Valley: An Experimental Proposal,"Beiboer, Julian (1); Sandoval, Eduardo B. (1) ","(1) UNSW Arts and Design, Australia ",ACM/IEEE International Conference on Human-Robot Interaction,ACM; ACM SIGAI; ACM SIGCHI; IEEE; IEEE Robotics and Automation Society,IEEE Computer Society,v 2019-March,p 608-609,22-Mar-19,HRI 2019 - 14th ACM/IEEE International Conference on Human-Robot Interaction,2019,,,21672148,9.78154E+12,10.1109/HRI.2019.8673261,8673261,"14th Annual ACM/IEEE International Conference on Human-Robot Interaction, HRI 2019","March 11, 2019 - March 14, 2019",,"The uncanny valley phenomenon has be researched for the past 15 years, attempting to prove its validity with limited success. Researchers have been trying to recreate Masahiro Mori's hypothesised function [1] through a variety of experiments using images of real robots and images created from morphing humans and robots. Although some of these experiments have provided results supporting Mori's hypothesis, there is no solid confirmation of their legitimacy when it comes to real human-robot interaction. This paper examines the methods and subsequent results of studies to draw conclusions regarding the validity of experimental data into Mori's hypothesis. These conclusions lead us to propose an Augmented Reality(AR) experiment designed to verify the results of previous experimentation. © 2019 IEEE.",17,Human robot interaction,Anthropomorphic robots - Augmented reality - Landforms - Man machine systems,androids - Humanoid robot - Morphing - Real robot - Uncanny valley,"481.1 Geology - 723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An exploratory study of augmented embodiment for computational thinking,"Chung, Cheng-Yu (1); Hsiao, I-Han (1) ","(1) Arizona State University, United States ","International Conference on Intelligent User Interfaces, Proceedings IUI",ACM Special Interest Group on Artificial Intelligence (SIGAI); ACM Special Interest Group on Computer-Human Interaction (SIGCHI),Association for Computing Machinery,,p 37-38,16-Mar-19,"Proceedings of the 24th International Conference on Intelligent User Interfaces, IUI 2019",2019,,,,9.78145E+12,10.1145/3308557.3308676,,"24th International Conference on Intelligent User Interfaces, IUI 2019","March 16, 2019 - March 20, 2019",,"The contiguity of physical and digital content of embodied learning has been shown to increase student's engagement in educational contexts. Applications with various kinds of physical interactions have been deployed to enhance the learning experiences in many engineering domains. However, even though computer science education (CSE) is one of zestful topics in the recent years, there are few studies focusing on the embodiment of CSE materials, by which the abstract and intangible concepts could be transformed into an intuitive affordance that utilizes sensorimotor experiences during the learning process. We propose an augmented embodiment mobile app designed for computational thinking (CT), specifically the debugging practices and abstraction concept, that makes use of gestures and augmented reality for learners to interact with the content. We examine the logic by the design framework for embodied learning and discuss potential extensions of multimodal analytics in such an application. Our preliminary user study in a middle school shows students' engagement in the application, however, it also reflected several design issues that need to be solved in the next iteration. The future plan of data analysis and experiments is also discussed. © 2019 ACM.",5,User interfaces,Augmented reality - Computation theory - Education computing - Mobile computing - Students,Computational thinkings - Computer Science Education - Educational context - Embodied learning - Learning experiences - Multi-modal interfaces - Physical interactions - Students' engagements,"721.1 Computer Theory, Includes Formal Logic, Automata Theory, Switching Theory, Programming Theory - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ANGELA: A novel approach of graphic notation based on the metaphor of road signs to facilitate the learning of programming,"Schez-Sobrino, Santiago (1); García, María A. (1); Gómez, Cristian (1); Vallejo, David (1); Molina, Ana I. (1); Lacave, Carmen (1); Glez-Morcillo, Carlos (1); Albusac, Javier A. (1); Redondo, Miguel A. (1) ","(1) Escuela Superior de Informática, University of Castilla-La Mancha, Ciudad Real, Spain ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 822-829,16-Oct-19,Proceedings - TEEM 2019: 7th International Conference on Technological Ecosystems for Enhancing Multiculturality,2019,,,,9.78145E+12,10.1145/3362789.3362871,,"7th International Conference on Technological Ecosystems for Enhancing Multiculturality, TEEM 2019","October 16, 2019 - October 18, 2019",,"Programming is a field that influences other disciplines in a transversal way, so its learning is necessary considering the emergence of new jobs that will require programming knowledge in the future. However, programming raises certain difficulties during its learning, especially in understanding programming concepts due to the high level of abstraction required. This level of abstraction can be reduced by using graphic representations that motivate the student and facilitate the understanding of certain programming concepts that arise at the beginning of the learning process. Therefore, this paper introduces ANGELA, a graphic notation based on the metaphor of roads and traffic signs that is meant to complement the learning process of beginner students who are starting to program by visualizing programs. These visualizations can be automatically generated from the source code of the programs, thanks to the modular and scalable design of the notation, and used by teachers to explain programming concepts during classes. The proposal has been evaluated with students in order to validate if the notation is appropriate to represent the concepts it tries to abstract from and if it results easy for the students to understand. Additionally, some use cases are presented in real-world scenarios in order to demonstrate the flexibility of the proposal. © 2019 ACM.",33,Traffic signs,Abstracting - Augmented reality - Ecosystems - Learning systems - Students - Visualization,Algorithm visualization - Automatically generated - Graphic representation - High level of abstraction - Learning-of-programming - Metaphors - Program visualization - Programming learning,"432.4 Highway Traffic Control - 454.3 Ecology and Ecosystems - 723 Computer Software, Data Handling and Applications - 903.1 Information Sources and Analysis",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
VR/AR/MR for everyone!,"Curley, Jordyn (1); Phillips, Stephen (2); Canning, John (3); McCarthy, Michael (4); Elvis, A.U. (5); Conlogue, Samuel (1) ","(1) Infusion Studios, DBA Czarnowski, Portland; ME, United States (2) Theia Interactive, Chico; CA, United States (3) Digital Domain, Los Angeles, CA, United States (4) Super Symmetric Studio, United States (5) Blue Sky Studios, Greenwich; CT, United States ","ACM SIGGRAPH 2019 Panels, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Panels, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3306212.3328111,,"ACM SIGGRAPH 2019 Panels - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"The attendees at the SIGGRAPH Conference run the gambit from indie contractors to multi-million-dollar studios. After attending the conference for the past 7 years, I kept hearing the same thing from many small studios and/or individuals. They feel a lot of the emerging technology that is premiered at SIGGRAPH seems out of reach or impractical for them to utilize. Our panel works to challenge this assumption. By bringing in industry professionals from various disciplines and levels, we can show how anyone from an independent contractor to a large studio can implement new tools and technologies. Specifically, our talk will focus on encounters with AR/VR/MR and how we worked it into our pipeline. Whether someone is out there making the next big movie or trying to pitch to a client in a conference room setting, they will find attending this panel useful. We will talk about the practical applications of AR/VR/MR and how we have explored these technologies over the years. © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6312-9/19/07.",,Interactive computer graphics,Argon - Audition - Augmented reality - Contractors - Mixed reality - Studios - Virtual reality,Conference rooms - Emerging technologies - Future - Industry professionals - Real time - Tools and technologies,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 804 Chemical Products Generally - 912 Industrial Engineering and Management",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Stochastic optimization for green multimedia services in dense 5G networks,"Cao, Tengfei (1); Xu, Changqiao (1); Wang, Mu (1); Jiang, Zhongbai (1); Chen, Xingyan (1); Zhong, Lujie (2); Grieco, Luigi Alfredo (3) ","(1) State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing; 100876, China (2) Information Engineering College, Capital Normal University, Beijing; 100089, China (3) Department of Electrical and Information Engineering, Politecnico di Bari, v. Orabona, Bari; 4-70125, Italy ","ACM Transactions on Multimedia Computing, Communications and Applications",,Association for Computing Machinery,"v 15, n 3",,Sep-19,,2019,,15516857,15516865,,10.1145/3328996,79,,,,"The manyfold capacity magnification promised by dense 5G networks will make possible the provisioning of broadband multimedia services, including virtual reality, augmented reality, and mobile immersive video, to name a few. These new applications will coexist with classic ones and contribute to the exponential growth of multimedia services in mobile networks. At the same time, the different requirements of past and old services pose new challenges to the effective usage of 5G resources. In response to these challenges, a novel Stochastic Optimization framework for Green Multimedia Services named SOGMS is proposed herein that targets the maximization of system throughput and the minimization of energy consumption in data delivery. In particular, Lyapunov optimization is leveraged to face this optimization objective, which is formulated and decomposed into three tractable subproblems. For each subproblem, a distinct algorithm is conceived, namely quality of experience based admission control, cooperative resource allocation, and multimedia services scheduling. Finally, extensive simulations are carried out to evaluate the proposedmethod against stateof-art solutions in dense 5G networks. © 2019 Association for Computing Machinery.",45,5G mobile communication systems,Access control - Augmented reality - Energy utilization - Multimedia services - Multimedia systems - Optimization - Quality control - Quality of service - Queueing networks - Resource allocation - Stochastic systems - Virtual reality,Broadband multimedia services - Exponential growth - Extensive simulations - G-networks - Green multimedia - Quality of experience (QoE) - Stochastic optimizations - Stochastic optimizing,"525.3 Energy Utilization - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 912.2 Management - 913.3 Quality Assurance and Control - 921.5 Optimization Techniques - 961 Systems Science",,,"Number: CX2018208, Acronym: -, Sponsor: -; Number: 61871048, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; Number: -, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; Number: z2016081, Acronym: -, Sponsor: -; Number: -, Acronym: BUPT, Sponsor: Beijing University of Posts and Telecommunications; ","This work was supported by the National Natural Science Foundation of China (NSFC) under grants 61871048, 61872253, and 61762074; by BUPT Excellent Ph.D. Students Foundation grant CX2018208; and by Chunhui Plan of Ministry of Education of China grant z2016081. Authors&rsquo; addresses: T. Cao, C. Xu (corresponding author), M. Wang, Z. Jiang, and X. Chen, State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China; emails: {caotf, cqxu, wangmu, zbjiang, chenxingyan}@bupt.edu.cn; L. Zhong, Information Engineering College, Capital Normal University, Beijing, 100089, China; email: zhonglj@cnu.edu.com; L. A. Grieco, Department of Electrical and Information Engineering, Politecnico di Bari, v. Orabona 4-70125, Bari, Italy; email: a.grieco@poliba.it. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. &copy; 2019 Association for Computing Machinery. 1551-6857/2019/09-ART79 $15.00 https://doi.org/10.1145/3328996",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Handling work complexity with ar/deep learning,"Dhiman, Hitesh (1); Buttner, Sebastian (1, 2); Rocker, Carsten (1, 3); Reisch, Raphael (4) ","(1) Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany (2) Human-Centered Information Systems, Clausthal University of Technology, Clausthal-Zellerfeld, Germany (3) Fraunhofer IOSB-INA, Lemgo, Germany (4) Resolto Informatik GmbH, Herford, Germany ",ACM International Conference Proceeding Series,Curtin University; Edith Cowan University (ECU); Human Factors and Ergonomics Society of Australia (HFESA); Perth Convention Bureau; The University of Western Australia (UWA); UX Machines Pty Ltd,Association for Computing Machinery,,p 518-522,2-Dec-19,"Proceedings of the 31st Australian Conference on Human-Computer-Interaction, OzCHI 2019",2019,,,,9.78145E+12,10.1145/3369457.3370919,,"31st Australian Conference on Human-Computer-Interaction, OzCHI 2019","December 2, 2019 - December 5, 2019",,"Complexity is a fundamental part of product design and manufacturing today, owing to increased demands for customization and advances in digital design techniques. Assembling and repairing such an enormous variety of components means that workers are cognitively challenged, take longer to search for the relevant information and are prone to making mistakes. Although in recent years deep learning approaches to object recognition have seen rapid advances, the combined potential of deep learning and augmented reality in the industrial domain remains relatively under explored. In this paper we introduce AR-ProMO, a combined hardware/software solution that provides a generalizable assistance system for identifying mistakes during product assembly and repair. © 2019 Association for Computing Machinery.",29,Deep learning,Augmented reality - Human computer interaction - Object recognition - Product design - Repair,Assistance system - Digital design techniques - Hardware/software - Learning approach - Product assembly - Work complexity,"723 Computer Software, Data Handling and Applications - 913.1 Production Engineering - 913.5 Maintenance",,,"Number: ZF4036809ED7, Acronym: -, Sponsor: -; ","This project is funded by the German Federal Ministry of Economic Affairs and Energy under the Central Innovation Programme for small and medium-sized enterprises (SMEs)"""," grant number ZF4036809ED7.""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,
AR in production: Development of UI patterns,"Koreng, Regina (1) ","(1) Institut für Medientechnik Fachgebiet Medienproduktion Technische Universität Ilmenau Ilmenau, Deutschland, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 655-659,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,German,,,9.78145E+12,10.1145/3340764.3344886,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"The use of Augmented Reality (AR) in the production environment is still not widespread. The high heterogeneity of the user interfaces in standard Head-Mounted Displays (HMD) poses a challenge for developers to find an ergonomically high-quality solution for specific tasks within the production environment. The aim of the study is to develop a catalogue of sample solutions, so-called patterns, which are tailored to the generic tasks of these special target groups, such as quality inspectors or line managers. This is done on the basis of extensive task analyses and comparative empirical evaluations of common user interfaces in HMDs. From these, possible patterns were extracted in which alternative layouts were combined with the input modes 'focus' and 'gestures'. The paper describes the results of an evaluation with AR-experienced usability engineers. © 2019 Association for Computing Machinery.",13,User interfaces,Augmented reality - Helmet mounted displays - Job analysis,Eingabemodus - Empirical evaluations - Head mounted displays - High-quality solutions - Pattern - Production environments - Produktion - Usability,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Improving vocational training in the philippines using AR,"Dayagdag, Carlwin V. (1); Catanghal, Ricardo A. (2); Palaoag, Thelma D. (3) ","(1) Romblon State University Odiongan, Romblon, Philippines (2) University of Antique, Antique, Philippines (3) University of the Cordilleras, Baguio City, Philippines ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 253-257,16-Mar-19,"Proceedings of the 8th International Conference on Informatics, Environment, Energy and Applications, IEEA 2019",2019,,,,9.78145E+12,10.1145/3323716.3323755,,"8th International Conference on Informatics, Environment, Energy and Applications, IEEA 2019","March 16, 2019 - March 19, 2019",,"Currently, Augmented Reality Training is already being used by different industries such as medicine, military, manufacturing, automotive and aerospace. However, many Academic and vocational training institutions, particularly in the Philippines, are not yet utilizing this technology because of expensive AR HMD and smart-glass devices. To shorten the gap in realizing the benefits of this wearable technology between industry and academic and training institution, the paper introduced AR4Juan Technical Vocational Training Solution. The main contributions of this paper are as follows: 1) The researchers introduced cost-effective hands-free AR4Juan based training solution for k-12 technical vocational training, and 2) Shows how A4Juan Mobile application could address the trainee difficulties identified in the current training methods utilized by Academic and Technical Vocational institutions in the Philippines. AR4Juan training solution could revolutionize the training approach and realize AR based hands-free training benefits accessible to everyone. The next phase of the study is to evaluate AR4Juan training solution on its degree of usefulness, ease of use, user enjoyment and engagement and ease of access. In addition, the ARBox1 and ARBox2 headsets will be evaluated to identify which headset is more suitable for hands-free AR-based training. Other researchers could use the AR4Juan training solution and evaluate its usefulness. AR4Juan utilization and evaluation are essential to improve AR-based training delivery in academic and technical vocational institutions continuously. © 2019 Association for Computing Machinery.",13,Wearable technology,Augmented reality - Cost effectiveness - mHealth - Smartphones,Cost effective - Hands-free - Mobile applications - Smart glass - Training institutions - Training methods - Training solutions - Vocational training,"718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications - 911.2 Industrial Economics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An AR/TUI-supported Debugging Teaching Environment,"Resnyansky, Dmitry (1); Billinghurst, Mark (1); Dey, Arindam (2) ","(1) School of Information Technology and Mathematical, Sciences University of South Australia, Adelaide; SA, Australia (2) School of Information Technology and Electrical Engineering, University of Queensland, Brisbane; QLD, Australia ",ACM International Conference Proceeding Series,Curtin University; Edith Cowan University (ECU); Human Factors and Ergonomics Society of Australia (HFESA); Perth Convention Bureau; The University of Western Australia (UWA); UX Machines Pty Ltd,Association for Computing Machinery,,p 590-594,2-Dec-19,"Proceedings of the 31st Australian Conference on Human-Computer-Interaction, OzCHI 2019",2019,,,,9.78145E+12,10.1145/3369457.3369538,,"31st Australian Conference on Human-Computer-Interaction, OzCHI 2019","December 2, 2019 - December 5, 2019",,"This paper presents research on the potential application of Tangible and Augmented Reality (AR) technology to computer science education and the teaching of programming in tertiary settings. An approach to an AR-supported debugging-Teaching prototype is outlined, focusing on the design of an AR workspace that uses physical markers to interact with content (code). We describe a prototype which has been designed to actively scaffold the student's development of the two primary abilities necessary for effective debugging: (1) the ability to read not just the code syntax, but to understand the overall program structure behind the code; and (2) the ability to independently recall and apply the new knowledge to produce new, working code structures. © 2019 Association for Computing Machinery.",41,Human computer interaction,Augmented reality - Codes (symbols) - Computer debugging - Education computing - Engineering education - Program debugging - Scaffolds - User interfaces,Code structure - Computer Science Education - Program structures - Tangible user interfaces - Teaching prototypes - Teaching-of-programming - Tertiary education,"405.1 Construction Equipment - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 901.2 Education",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A principled formulation of integrating objects in Monocular SLAM,"Pokale, Aniket (1); Das, Dipanjan (2); Aggarwal, Aditya (1); Bhowmick, Brojeshwar (2); Krishna, K. Madhava (1) ","(1) International Institute of Information Technology, Hyderabad, India (2) Tata Consultancy Services, India ",ACM International Conference Proceeding Series,"Indian Institute of Technology, Madras (IITM)",Association for Computing Machinery,,,2-Jul-19,"Proceedings of the Advances in Robotics 2019, AIR 2019",2019,,,,9.78145E+12,10.1145/3352593.3352664,a54,"2019 Conference on Advances in Robotics, AIR 2019","July 2, 2019 - July 6, 2019",,"Monocular SLAM is a well-studied problem and has shown significant progress in recent years, but still, challenges remain in creating a rich semantic description of the scene. Feature-based visual SLAMs are vulnerable to erroneous pose estimates due to insufficient tracking of mapped points or motion induced errors such as in large or in-place rotations. We present a new SLAM framework in which we use monocular edge based SLAM [1], along with category level models, to localize objects in the scene as well as improve the camera trajectory. In monocular SLAM systems, the camera track tends to break in conditions with abrupt motion which leads to reduction in the number of 2D point correspondences. In order to tackle this problem, we propose the first most principled formulation of its kind which integrates object category models in the core SLAM back-end to jointly optimize for the camera trajectory, object poses along with its shape and 3D structure. We show that our joint optimization is able to recover a better camera trajectory in such cases, as compared to Edge SLAM. Moreover, this method gives a better visualization incorporating object representations in the scene along with the 3D structure of the base SLAM system, which makes it useful for augmented reality (AR) applications. © 2019 Association for Computing Machinery. All rights reserved.",34,Vision,Augmented reality - Cameras - Motion tracking - Semantics - SLAM robotics - Three dimensional computer graphics - Trajectories,Camera trajectories - Edge SLAM - Joint optimization - Object representations - Object SLAM - Point correspondence - Semantic descriptions - Shape,"723 Computer Software, Data Handling and Applications - 742.2 Photographic Equipment",,,,"The authors acknowledge the support and funding from Kohli Center for Intelligent Systems (KCIS) IIIT Hyderabad, and Tata Consultancy Services (TCS) Innovation Labs India for this work. We also acknowledge the help of Yogesh Sharma, Parv Parkhiya, Junaid Ahmed Ansari, Gourav Kumar, Soumyadip Maity for their timely assistance.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Healing cracks in cyberspace: Towards best practice,"Thorne, Chris (1) ","(1) Systemic Pty Ltd, Australia ",ACM International Conference Proceeding Series,Curtin University; Edith Cowan University (ECU); Human Factors and Ergonomics Society of Australia (HFESA); Perth Convention Bureau; The University of Western Australia (UWA); UX Machines Pty Ltd,Association for Computing Machinery,,p 491-502,2-Dec-19,"Proceedings of the 31st Australian Conference on Human-Computer-Interaction, OzCHI 2019",2019,,,,9.78145E+12,10.1145/3369457.3369540,,"31st Australian Conference on Human-Computer-Interaction, OzCHI 2019","December 2, 2019 - December 5, 2019",,"As we move firmly into the digital age, navigation through cyberspace has become an increasingly ubiquitous form of human-computer interaction. Whether the application we use is geospatial, flight simulation, virtual reality, augmented reality, simulation or game; we rely on sophisticated computation to paint data into views we can understand. 3D images and video with moving perspectives allow rapid navigation and assimilation of information. However, despite all the sophistication of modern technology, we have inadvertently programmed random cracks into cyberspace leading to positional jitter. Positional jitter can present as: random motion, rendering errors, physics errors and imprecise interaction. This unintended numerical error, and its mitigation, has been a focus of research and development over that past 2 decades. A review of mitigation methods has revealed differences in the quality, complexity and performance of implementations and some ad-hoc approaches to designing for sufficient quality. To help move research and development towards a consensus solution, this paper reviews and evaluates different approaches. New metrics to estimate error and quality are presented. A simple and efficient method to minimising positional jitter, that will benefit scientific and engineering calculations sensitive to error and achieve the best performance and quality for general applications, is recommended. © 2019 Association for Computing Machinery.",47,Human computer interaction,Augmented reality - Computers - Digital arithmetic - Flight simulators - Image quality - Jitter - Navigation - Random errors - Virtual reality,Accuracy - Cyberspaces - Floating origin - Floating points - Interaction,"721.1 Computer Theory, Includes Formal Logic, Automata Theory, Switching Theory, Programming Theory - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Gesture recognition system using optical muscle deformation sensors,"Hosono, Satoshi (1); Nishimura, Shoji (1); Iwasaki, Ken (2); Tamaki, Emi (3) ","(1) Waseda Univ., Tokyo, Japan (2) H2L, Inc., Tokyo, Japan (3) Waseda Univ., H2L, Inc., Tokyo, Japan ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 12-15,13-Apr-19,"Proceedings of the 2nd International Conference on Electronics, Communications and Control Engineering, ICECC 2019",2019,,,,9.78145E+12,10.1145/3324033.3324037,,"2nd International Conference on Electronics, Communications and Control Engineering, ICECC 2019","April 13, 2019 - April 16, 2019",,"Due to the spread of VR(Virtual Reality)/AR(Augmented Reality) applications, gesture input method will be required. In this research, a gesture recognition system is suggested using the optical muscle deformation sensors. Our gesture recognition system adapts machine learning with 8 channel optical muscle deformation sensors on the forearm which doesn’t disturb the movement of the hand. In our experiment, significant differences were found in t-test. It was found that SVM can recognize gesture with higher accuracy more than Logistic Regression. In addition, we conducted an experiment to distinguish the state of bending each finger joint. As a result, it was found that the open hand gesture is erroneously recognized as PIP bent gesture. © 2019 Association for Computing Machinery.",14,Gesture recognition,Augmented reality - Deformation - Muscle - Palmprint recognition - Virtual reality,Finger joints - Gesture input - Gesture recognition system - Hand gesture - Human activity recognition - Information interfaces - Logistic regressions - Muscle deformation,"461.2 Biological Materials and Tissue Engineering - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Hybrid CNNs: A rotation equivariant framework for high resolution spherical images,"Yu, Wei (2); Zha, Daren (1); Mu, Nan (1); Fu, Tianshu (1) ","(1) Institute of Information Engineering, Chinese Academy of Sciences, China (2) School of Cyber Security, University of Chinese Academy of Sciences, No.89 Minzhuang Road, Haidian District, Beijing, China ",ACM International Conference Proceeding Series,,Association for Computing Machinery,v Part F147955,p 38-42,2019,,2019,,,,9.78145E+12,10.1145/3316551.3316573,,"3rd International Conference on Digital Signal Processing, ICDSP 2019","February 24, 2019 - February 26, 2019",,"With the prevalence of virtual reality, augmented reality and autonomous robots, the high resolution spherical images they produced make the standard convolutional neural networks (CNNs), which have been proven powerful on perspective images, non-trivial. The classic solution to utilize CNNs on spherical images is to project the spherical images onto plane and learning the planar images using conventional CNNs. But the distortion generated by the projection of spherical images to planar images invalidates the projection based models. Besides, these models are not robust to rotations which are the basic transformation of spherical images. Another type of solution based on spherical harmonics recently proposed by Cohen et al. [1] is rotation equivariant, but can't handle high resolution spherical images with its expensive computational cost. To process high resolution spherical images, we proposed the Hybrid CNNs. Our framework is both computationally efficient and rotation equivariant with two kinds of convolution operations defined in this paper. We compared our method with several baseline models in two classification tasks. The experimental results demonstrate the computational efficiency and rotation equivariance of the Hybrid CNNs. © 2019 Association for Computing Machinery.",12,Spheres,Augmented reality - Computational efficiency - Convolution - Digital signal processing - Neural networks - Rotation - Virtual reality,Basic transformations - Classification tasks - Computational costs - Computationally efficient - Convolutional neural network - Perspective image - Spherical harmonics - Spherical images,"716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications - 931.1 Mechanics",,,"Number: -, Acronym: -, Sponsor: Haixi Institute, Chinese Academy of Sciences; ","This work was funded by Institute of Information Engineering of Chinese Academy of Sciences. Some of the results in this paper have been derived using the HEALPix (Gorski et al., 2005) package.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Motion statistic based local homography transformation estimation for mismatch removal,"Du, Songlin (1); Ikenaga, Takeshi (1) ","(1) Graduate School of Information, Production and Systems, Waseda University, Japan ",ACM International Conference Proceeding Series,Nanyang Technological University; University of Tsukuba,Association for Computing Machinery,,p 47-50,27-Jul-19,AIVR 2019 - 2019 3rd International Conference on Artificial Intelligence and Virtual Reality,2019,,,,9.78145E+12,10.1145/3348488.3348496,,"3rd International Conference on Artificial Intelligence and Virtual Reality, AIVR 2019","July 27, 2019 - July 29, 2019",,"Accurately establishing pixel-level correspondence between images taken from same objects is an essential problem in many computer vision applications, such as 3D reconstruction, simultaneous localization and mapping (SLAM), and augmented reality (AR). Existing local feature descriptor based image matching approaches are unable to avoid mismatches which cause negative effects to the above mentioned applications. This paper proposes a motion statistic based local homography transformation estimation method for removing mismatches. The proposed method estimates local homography transformations between the grids in a pair of images and then classifies each match as correct or incorrect by checking whether it is consisting with the corresponding local homography transformation or not. Experimental results on the widely used Oxford affine image dataset show that the proposed approach finds out more potential correct matches than the existing state-of-the-art method. © 2019 Association for Computing Machinery.",17,Virtual reality,Augmented reality - Image matching - Robotics,3D reconstruction - Computer vision applications - Essential problems - Homography transformation - Local feature descriptor - Motion statistics - Simultaneous localization and mapping - State-of-the-art methods,"723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,"Number: 2019C-581, Acronym: -, Sponsor: Waseda University; ",This work was supported by Waseda University Grant for Special Research Projects (2019C-581).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Research on effective teaching in the vision of virtual reality,"Shi, Pu (1) ","(1) School of Education, Tianjin University, 135 Yaguan Road, Jinnan District, Tianjin; 300350, China ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 44-47,25-Oct-19,ICDTE 2019 - 2019 the 3rd International Conference on Digital Technology in Education,2019,,,,9.78145E+12,10.1145/3369199.3369240,,"3rd International Conference on Digital Technology in Education, ICDTE 2019","October 25, 2019 - October 27, 2019",,"Since human beings entered the industrial society, 'the concept of efficiency is regarded as the noblest moral concept.' This highest moral concept' is also used to solve the contradiction between knowledge proliferation and inefficient teaching, effective teaching. Therefore, as an important concept throughout the teaching reform. Pursuing effectiveness has become an inherent requirement of teaching development and has guided teaching reform. In recent years, the rapid development of virtual reality technology has amazed the world, and this technology has been used in many industries to promote change and development, and the education field is also among them. Unlike other fields, virtual reality technology is rarely used in education. Even though a few domestic and foreign schools and educational institutions have tried to use virtual reality technology to assist teaching, this technology has not been able to study due to its shallow research and consideration. Play the expected ideal effect in education and teaching. Therefore, this paper starts from the frontier concept of virtual reality technology, combines the connotation and essence of effective teaching, analyzes the 'virtual reality + augmented reality' teaching mode that can be used in current school classroom teaching, and further explains its operation outline and rationality. In order to provide a reasonable teaching mode under the vision of virtual reality for the school classroom. © 2019 Association for Computing Machinery",7,E-learning,Augmented reality - Educational technology - Engineering education - Virtual reality,Classroom teaching - Educational institutions - Effective teaching - Industrial societies - Inherent requirements - Teaching modes - Teaching reforms - Virtual reality technology,"723 Computer Software, Data Handling and Applications - 901.2 Education",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Designing AR enhanced art exhibitions: A methodology and a case study,"Pittarello, Fabio (1) ","(1) Università Ca' Foscari Venezia Mestre, Venezia, Italy ",ACM International Conference Proceeding Series,"Dipartimento dei Beni Culturali: Archeologia, Storia dell'Arte, del Cinema e della Musica; Dipartimento di Psicologia Generali (DPG); Human Inspired Technology Research Centre (HIT); Universita Ca'Foscari Venezia, Dipartimento di Scienze Ambientali, Informatica e Statistica; Universita degli Studi di Padova",Association for Computing Machinery,,,23-Sep-19,CHItaly 2019 - Proceedings of the 13th Biannual Conference of the Italian SIGCHI Chapter Designing the Next Interaction,2019,,,,9.78145E+12,10.1145/3351995.3352052,a19,"13th Biannual Conference of the Italian SIGCHI Chapter Designing the Next Interaction, CHItaly 2019","September 23, 2019 - September 25, 2019",,"This paper describes a methodology for the creation of augmented reality experiences for artistic exhibitions, targeted at offering a smooth authoring path to designers with limited computer science skills. The work describes how to design a smooth and coherent experience for the visitors, in spite of the limitations of the current authoring environments, providing a cognitive and emotional impact without overwhelming the original exhibition concept. The methodology was experimented in occasion of the world famous Biennale Art Exhibition in Venice and showed to be effective in the creation of a meaningful user experience. © 2019 Association for Computing Machinery.",17,Exhibitions,Augmented reality,Authoring environments - Design Methodology - User experience,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Future of the electrical engineering education on the AR and VR basis,"Ivan, Kholodilin (1, 2); Alexander, Nesterov (2) ","(1) Beijing Institute of Technology, Beijing; 100081, China (2) South Ural State University, Chelyabinsk; 454080, Russia ",ACM International Conference Proceeding Series,Northwestern Polytechnical University (NWPU); Universidad Nacional Autonoma de Mexico (UNAM); Wuhan University,Association for Computing Machinery,,p 113-117,29-Oct-19,"VSIP 2019 - Proceedings of 2019 International Conference on Video, Signal and Image Processing",2019,,,,9.78145E+12,10.1145/3369318.3369337,,"2019 International Conference on Video, Signal and Image Processing, VSIP 2019","October 29, 2019 - October 31, 2019",,"Augmented Reality (AR) and Virtual Reality (VR) can serve as an efficient tool in Information Technology (IT), helping students to cope with the tasks they used to deal in an interesting and the same time productive way. The aim of this study is to show that educational process might look different: students could be really involved in it and have the ability to carry out various type of experiments and simulations in the safety and entertaining way. This paper reveals current situation of the education, presents modification of the already existed system based on the laboratory complex and proposed ideas of the electrical engineering education future on the AR and VR technologies basis. Paper shows certain achievements in the field of virtual education and technologies according to which it becomes possible to realize these expected future ideas. Experiment was conducted in order to show the possibility of data transfer between hardware and software. Moreover, the main advantages of the AR and VR technologies are listed as well. The analysis suggests that these technologies are of particular importance nowadays and only by constant development and showing students' interest towards AR and VR significant changes and improvements might be achieved in the educational field. © 2019 Association for Computing Machinery.",15,Students,Augmented reality - Data transfer - Distance education - Engineering education - Image processing - Video signal processing - Virtual reality,Current situation - Educational process - Hardware and software - Students' interests - Virtual education - Virtual laboratories - VR technology,"716.4 Television Systems and Equipment - 723 Computer Software, Data Handling and Applications - 901.2 Education",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An effective microlearning approach using living book to promote vocational students' computational thinking,"Leela, Soralak (1); Chookaew, Sasithorn (2); Nilsook, Prachyanun (1) ","(1) Information and Communication Technology for Education, Faculty of Technical Education, King Mongkut's University of Technology North Bangkok, Thailand (2) Department of Teacher Training in Mechanical Engineering, Faculty of Technical Education, King Mongkut's University of Technology North Bangkok, Thailand ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 25-29,25-Oct-19,ICDTE 2019 - 2019 the 3rd International Conference on Digital Technology in Education,2019,,,,9.78145E+12,10.1145/3369199.3369200,,"3rd International Conference on Digital Technology in Education, ICDTE 2019","October 25, 2019 - October 27, 2019",,"The purpose of this study was to describe the effectiveness of using living books (mobile leaning and augmented reality) within the context of microlearning approach to promote vocational students computational thinking. The participants were 97 vocational students that included experimental group (n=52) and control group (n=45) vocational college who enrolled in the basic of mathematics career in 2nd semester of academic year 2018. The instruments of this research include learning materials: mobile learning and AR book, the lesson plan of surface area and volume topic in basic mathematics career subject, pre and post-tests, and the computational thinking assessment. The results showed that the students learning achievement of experimental group employed microlearning approach using living books, the control group employed traditional learning was statistically significant at level of .01. In addition, the result of students' computational thinking in the experimental group using of microlearning living book have been the computational thinking at the high level. © 2019 Association for Computing Machinery",12,Students,Augmented reality - E-learning - Educational technology,Computational thinkings - Experimental groups - Learning achievement - Learning materials - Living book - Micro-learning - Traditional learning - Vocational colleges,"723 Computer Software, Data Handling and Applications - 901.2 Education",,,"Number: -, Acronym: SBA, Sponsor: U.S. Small Business Administration; ","Thank you to the head of academic affairs and the director of Keowalin Business and Administration Technological College to provide support about location, participants, and advice on the development of learning management processes until successful results in research.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A qualitative comparison between augmented and virtual reality collaboration with handheld devices,"Müller, Jens (1); Zagermann, Johannes (1); Wieland, Jonathan (1); Pfeil, Ulrike (1); Reiterer, Harald (1) ","(1) HCI Group, University of Konstanz, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 399-410,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,10.1145/3340764.3340773,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"Handheld Augmented Reality (AR) displays offer a see-through option to create the illusion of virtual objects being integrated into the viewer’s physical environment. Some AR display technologies also allow for the deactivation of the see-through option, turning AR tablets into Virtual Reality (VR) devices that integrate the virtual objects into an exclusively virtual environment. Both display configurations are typically available on handheld devices, raising the question of their influence on users’ experience during collaborative activities. In two experiments, we studied how the different display configurations influence user experience, workload, and team performance of co-located and distributed collaborators during a spatial referencing task. A mixed-methods approach revealed that participants’ opinions were polarized towards the two display configurations, regardless of the spatial distribution of collaboration. Based on our findings, we identify critical aspects to be addressed in future research to better understand and support co-located and distributed collaboration using AR and VR displays. © 2019 Association for Computing Machinery.",54,Display devices,Augmented reality - Hand held computers - Virtual reality,Augmented and virtual realities - Co-located collaboration - Collaborative activities - Display configurations - Distributed collaboration - Hand held device - Handheld augmented realities - Physical environments,"722.2 Computer Peripheral Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: DFG, Sponsor: Deutsche Forschungsgemeinschaft; Number: TRR 161, Acronym: DFG, Sponsor: Deutsche Forschungsgemeinschaft; ","Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) ? Projekt-ID 251654672 ? TRR 161 (Project C01).",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A mobile application system for community health workers - A review,"Emmanuel, Gahizi (1); Hungilo, Gilbert Gutabaga (1); Rahardjo Emanuel, Andi Wahju (1) ","(1) Magister Teknik Informatika, Universitas Atma Jaya Yogyakarta, Indonesia ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 106-110,19-Apr-19,ICCAI 2019 - 2019 5th International Conference on Computing and Artificial Intelligence,2019,,,,9.78145E+12,10.1145/3330482.3330485,,"5th International Conference on Computing and Artificial Intelligence, ICCAI 2019","April 19, 2019 - April 22, 2019",,"Community Health workers (CHWs) are the foundation of public health services aimed to connect the gap between communities, health and social service system, and it is done by navigating the health and human services system and educating communities on disease prevention. Unfortunately, the way of sharing and accessing information for delivering the services is often very unreliable by using manual system for reporting which can cause error and falsification. Furthermore, the Staff which performs these duties often they do not have knowledge about disease and health system training or education. To address this need, a mobile application System for CHWs is needed, which enables community health workers to automatically send a report of monthly activities without using any manual input form. Making use of the digital device (the smartphone, PDAs, and The Augmented Reality Personal Digital Assistant .The mobile application will automatically allow submit a report, transfer knowledge, sharing information and receiving training by using the user interface which will have the features like social media. Also the electronic file for entering information will be filled automatically. The system will be recording and uploaded to a central server for use by CHWs supervisor and the health manager official. This article provides ICTs with a regard to Mobile Health System and the probable of field which are lacking. Its absence is root of challenges faced by CHWs, the solutions to challenges is to design technological (Mobile Health System) which create durable, imperishable answers for tending to the world's wellbeing need. © 2019 Association for Computing Machinery.",42,Electronic document exchange,Artificial intelligence - Augmented reality - Digital devices - Information dissemination - mHealth - Mobile computing - Personal digital assistants - Personnel training - Public health - User interfaces,Community Health Workers - Disease prevention - Health and human services - Mobile applications - Mobile health systems - Public health services - Sharing information - Social,"461.6 Medicine and Pharmacology - 722 Computer Systems and Equipment - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 903.2 Information Dissemination - 912.4 Personnel",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An AR based edge maintenance architecture and maintenance knowledge push algorithm for communication networks,"Li, Ruide (1); Gao, Guohua (1); Liang, Yingjie (1); Zhang, Xin (1); Liao, Yongqiang (2) ","(1) Jiangmen Power Supply Bureau of Guangdong Power Grid Co., Ltd., No. 152, Jianshe 2nd Road, Pengjiang District, Jiangmen City, Guangdong; 529000, China (2) Guangdong Electric Power Information Communication Co., Ltd., Baili Pavilion, Guangfa Garden, No. 498 Huanshi East Road, Yuexiu District, Guangzhou; 510000, China ",ACM International Conference Proceeding Series,Shenzhen University; Sun Yat-Sen University,Association for Computing Machinery,,p 165-168,10-May-19,ICBDC 2019 - Proceedings of 2019 4th International Conference on Big Data and Computing,2019,,,,9.78145E+12,10.1145/3335484.3335532,,"4th International Conference on Big Data and Computing, ICBDC 2019","May 10, 2019 - May 12, 2019",,"Maintenance is an important aspect of the entire life cycle of communication network devices. The existing maintenance environment of the communication network is complex, with various types of equipment components and complicated structures. The quality of maintenance is greatly affected by the ability and experience of on-site maintenance personnel, and the maintenance cost is high, and the efficiency is low. In order to decrease the difficulty of on-site maintenance and improve maintenance efficiency and quality, we proposed an augmented reality (AR) based edge maintenance architecture. Moreover, we propose a context model-based maintenance knowledge push algorithm (CMKP) to help on-site maintenance personnel obtain auxiliary information. This will provide on-site maintenance personnel with intelligent assistance and improve overall maintenance efficiency. © 2019 Association for Computing Machinery",8,Maintenance,Augmented reality - Big data - Edge computing - Efficiency - Internet of things - Life cycle - Network architecture - Personnel - Telecommunication networks,Auxiliary information - Complicated structures - Entire life cycles - Intelligent assistances - Maintenance cost - Maintenance efficiency - Maintenance personnel - Smart wearables,"723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 912.4 Personnel - 913.1 Production Engineering - 913.5 Maintenance",,,,"This work is supported by the Science and Technology Project of Guangdong Power Grid Co., Ltd:",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Industry focused in data collection: How industry 4.0 is handled by big data,"Oliveira, Miguel (1); Afonso, Daniel (1) ","(1) School of Design, Management and Production, Technologies Northern Aveiro, University of Aveiro Oliveira de Azeméis, Aveiro, Portugal ",ACM International Conference Proceeding Series,National University of Singapore; The Hong Kong Polytechnic University,Association for Computing Machinery,,p 12-18,19-Jul-19,"Proceedings of the 2019 2nd International Conference on Data Science and Information Technology, DSIT 2019",2019,,,,9.78145E+12,10.1145/3352411.3352414,,"2nd International Conference on Data Science and Information Technology, DSIT 2019","July 19, 2019 - July 21, 2019",,"The paper aims to organize and structure data collected and associated to technologies that powers the abroad concept of Industry 4.0. It starts with the historic evolution of industry, separated by date landmarks and approaches the last transition between 3.0 to 4.0. Apart from the differences between industry models, production data stats show a huge and important transformation in the amount of data related to manufacturing and how that knowledge is processed. The paper also aims to put on debate the lack of solutions regarding the knowledge extraction of data from machines and systems, needed for data analytics. Approaches with cyber-physical systems, machine learning, virtual environments, Industrial IoT 1 and augmented reality, in an industrial scale, are some of the strategies to power the reading and interpretation of data, in order to promote industrial efficiency. Real context industrial applications are taken into account in order to state the importance of collected data in the efficiency of a production process. Exploring technologies and concepts to improve digital twins systems, perception and perceived systems as well as maintenance processes are some of the explored implemented strategies that make Industry 4.0. Some possible strategies are presented, as well as the transition for Industry 5.0. © 2019 Association for Computing Machinery.",31,Metadata,Augmented reality - Automation - Big data - Data Analytics - Efficiency - Embedded systems - Industry 4.0 - Virtual reality,Data collection - Implemented strategy - Industrial scale - Interpretation of data - Knowledge extraction - Maintenance process - Production data - Production process,"723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 731 Automatic Control Principles and Applications - 913.1 Production Engineering",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Alternative interaction techniques for drone-based mission definition: From desktop UI to wearable AR,"Vaquero-Melchor, Diego (1); Bernardos, Ana M. (1) ","(1) Information Processing and Telecommunications Center, Universidad Politécnica de Madrid, Madrid, Spain ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,10.1145/3365610.3368420,3368420,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"In this paper we address the adaptation of applications for conventional platforms (web, tablets and smartphones) to Augmented Reality (AR) settings. We do this on a specific use case, a tool to help users to define drone missions, which has been implemented with equivalent functionalities both for web-enabled devices and AR wearable devices (in particular, HoloLens). First, a brief description of the application workings is presented as well as the creation process for the mission and the manipulation of its components. Then, we present a preliminary validation with 8 users to analyze the degree of acceptance of the AR version, taking the web user interface as a baseline. Despite the technical limitations of the AR version (mainly related to the device visualization constraints and weight) and the reduction in efficiency (tasks need more time to be completed), several users have expressed their preference for this version. However, with the problem of precision in AR Head Mounted Display context always present, the need to establish new interaction techniques is once more highlighted. © 2019 Copyright held by the owner/author(s).",6,Drones,Augmented reality - Helmet mounted displays - User interfaces - Wearable computers,Head mounted displays - Interaction techniques - Interface adaptation - Mission definition - Technical limitations - User validation - Web user interface - Web-enabled devices,"722.2 Computer Peripheral Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A workflow for real-time visualization and data analysis of gesture using motion capture,"Jégo, Jean-François (1); Meyrueis, Vincent (2); Boutet, Dominique (3) ","(1) INREV-AIAC Laboratory, Université Paris 8, France (2) LCPI Laboratory ENSAM, France (3) DYLIS Laboratory, Université de Rouen, France ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,10-Oct-19,MOCO 2019 - 6th International Conference on Movement and Computing,2019,,,,9.78145E+12,10.1145/3347122.3359598,a24,"6th International Conference for Movement and Computing, MOCO 2019","October 10, 2019 - October 12, 2019",,"In this paper, we investigate new ways to understand and to analyze human gesture in a research context applied on co-verbal gesture across language. The research project focuses on the quality of the movement and consider the gesture 'pulse of effort.' We propose a workflow for real-time gesture analysis to visualize gesture kinematics features (Velocity, Acceleration, Jerk) from heterogeneous data (Video, Motion Capture and Gesture Annotations) at the same time base. The tools designed here provide immersive and interactive explorations of data: users can test hypotheses and embody gesture visualization and descriptors adopting different Frames of Reference using augmented reality. We have conducted an evaluation protocol in the field of linguistics that compares 496 annotated gestures to benchmark the workflow. © 2019 Copyright is held by the owner/author(s).",13,Data visualization,Augmented reality - Flow visualization - Visualization,Embodiment - Gesture - Motion capture - Real time - Workflow,"631.1 Fluid Flow, General - 723 Computer Software, Data Handling and Applications",,,"Number: 14-48-00067II, Acronym: RSF, Sponsor: Russian Science Foundation; ",We wish to thank all the participants. The research was carried out at Moscow State Linguistic University and supported by Russian Science Foundation (project No. 14-48-00067II).,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The mobile exergames design model to encourage physical activity for sedentary generation Z,"Hashim, Hasdina Lynn (1); Kamaruddin, Azrina (1); Jantan, Azrul Hazri (1) ","(1) Universiti Putra Malaysia, Serdang, Malaysia ",ACM International Conference Proceeding Series,mozilla,Association for Computing Machinery,,p 137-141,1-Apr-19,Proceedings of CHIuXiD 2019 - 5th International Conference on HCI and UX: Empowering Digital Transformation,2019,,,,9.78145E+12,10.1145/3328243.3328260,,"5th International Conference on Human-Computer Interaction (HCI) and User Experience (UX): Empowering Digital Transformation, CHIuXiD 2019","April 1, 2019 - April 9, 2019",,"With the advancement of HCI technologies, user’s bodily movement can now be placed in the centre of experience. Exergames, a combination of exercise and games, allows users to use their body movement as input devices to reach games’ goals. A sedentary lifestyle, one where little physical activity is done, is common nowadays as individuals become dependent on technological aids to assist in running their daily actions. Physical activities can be attained using exergames as it helps to encourage some fitness for individuals through physically entertaining games during leisure times. The latest smartphones are now powerful enough to run game smoothly, displaying engaging contents with augmented reality immersion to players whilst exercising. This paper focuses on a proposed design model for a mobile exergame containing components that are beneficial to encourage productive behavioural change from sedentary to active for sedentary GenZ through planned exercises. © 2019 Association for Computing Machinery.",19,Human computer interaction,Augmented reality,Behavioural changes - Bodily movement - Body movements - Design modeling - Generation Z - Physical ability - Physical activity - Sedentary lifestyle,"723 Computer Software, Data Handling and Applications",,,"Number: GP/2018/9649800, Acronym: -, Sponsor: Universiti Putra Malaysia; ","We thank the Research Management Centre (RMC), Universiti Putra Malaysia (UPM), Malaysia for funding this paper through its ?Geran Putra? grant initiative program (GP/2018/9649800).",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Development of learning support equipment for sign language and fingerspelling by mixed reality,"Hirabayashi, Natsuhiko (1); Fujikawa, Nami (1); Yoshimura, Ryohei (1); Fujisawa, Yoshinori (1) ","(1) National Institute of Technology, Nagano College, Nagano, Japan ",ACM International Conference Proceeding Series,International Association for Computer and Information Science (ACIS),Association for Computing Machinery,,,29-May-19,"Proceedings - 7th International Conference on Applied Computing and Information Technology, ACIT 2019",2019,,,,9.78145E+12,10.1145/3325291.3325373,3325373,"7th International Conference on Applied Computing and Information Technology, ACIT 2019","May 29, 2019 - May 31, 2019",,"Our purpose of this study is to increase efficiency of learning sign language and fingerspelling, the visual language. In general, illustrations and videos are used as learning materials to study these languages. However, the learning materials are drawn from the view point of someone to talk to. Therefore, learners need to imagine finger shape that looked from the person while they study. Understanding the motion is easier if learners can watch it in three dimensions. Therefore, taking lessons by a teacher who has already mastered it will be the best way. However, the method is not appropriate to study by themselves. Then, we devise a new method to increase the efficiency of learning by increasing learning opportunities by developing equipment that learners can study alone. We propose learning support equipment using glasses-shaped wearable device and mixed reality. By using mixed reality, learners can watch sign language and fingerspelling motion in three dimensions as if learners looked at these languages in the real world. In the previous studies, the equipment was developed by using augmented reality. AR marker was used for the operation. With this equipment, learners have controlled movement and rotation of 3D model. In this study, we develop the equipment used mixed reality by selecting the holographic button with gesture. Therefore, learners can operate the equipment without using the other device and this make them possible to study more intuitive. With glasses-shaped wearable device, learners can also study sign language and fingerspelling mimicking the gesture with both hands. As we build sign language learning support contents, learners can study sign language and fingerspelling with them. However, the present equipment is short of 3D animation of sign language. Therefore, we are planning to make 3D animation by capturing the motions that hearing-impaired person use. We had a trial test for hearing-impaired person and a sign language interpreter in order to get real impression. As a result, we got an opinion that some 3D animations are not accurate and complicated to understand. Also, we got an opinion that 3D model direction is not clear after the rotation. According to these opinions, we corrected as follows: the correction of 3D animation, adding function of displaying the track of the finger, and the function of selecting the holographic reset button for the 3D model direction. © 2019 Association for Computing Machinery.",7,Learning systems,3D modeling - Animation - Audition - Augmented reality - Efficiency - Glass - Holography - Mixed reality - Visual languages - Watches - Wearable computers,Fingerspelling - Hearing-impaired persons - Learning materials - Learning opportunity - Learning support - Sign language - Three dimensions - Wearable devices,"461.4 Ergonomics and Human Factors Engineering - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.1.1 Computer Programming Languages - 743 Holography - 746 Imaging Techniques - 812.3 Glass - 913.1 Production Engineering - 943.3 Special Purpose Instruments",,,"Number: 18K11595, Acronym: JSPS, Sponsor: Japan Society for the Promotion of Science; ",This research was supported by JSPS KAKENHI Grant Number 18K11595.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Masked RCNn+ fast boosted tree classifier: A combination of Deep learning technology for neural networks and classifier for pedestrian detection,"Salam, Nader (1); Ali, Abdul (1) ","(1) Dept. of. CSE, ICET, KTU University, Kerala, India ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,15-Jun-19,"Proceedings of the 3rd International Conference on Advanced Informatics for Computing Research, ICAICR 2019",2019,,,,9.78145E+12,10.1145/3339311.3339317,a6,"3rd International Conference on Advanced Informatics for Computing Research, ICAICR 2019","June 15, 2019 - June 16, 2019",,"The most intriguing advancements brought by deep learning and neural networks is in the field of computer vision. We associate any problem that has an image or camera input to encompass problems within computer vision. Self-driving cars, MRI analysis, Mars exploration rovers, facial recognition systems, object detection and augmented reality are just a few breakthroughs in the field. In this paper we will take a look at ways to improve pedestrian detection by using Convolutional Neural Network (CNN) along with Region Proposal Network (RPN), Masked Region Based Convolutional Neural Network (RCNN) and Fast Boosted Tree Classifier. Our approach effectively recognizes objects in a picture while at the same time generating an excellent segmentation mask for each instance. The technique, called Mask R-CNN, expands Faster R-CNN by including a branch for predicting an item mask in parallel with the current branch for bouncing box recognitionn. © 2019 Association for Computing Machinery.",33,Deep learning,Augmented reality - Computer vision - Convolution - Face recognition - Forestry - Martian surface analysis - Neural networks - Object detection - Object recognition,Convolutional neural network - Facial recognition systems - Learning technology - Mars Exploration Rover - Pedestrian detection - Region Proposal Network (RPN) - Segmentation masks - Tree classifiers,"716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: FAI, Sponsor: Food Allergy Initiative; ","We would like to acknowledge that this work is a combination of the ideas presented by the FAIR team[30] and the method JiaXiang Zhao, Jun Li,YingDong Ma [27]",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A usability study with healthcare professionals of a customizable framework for reminiscence and music based cognitive activities for people with dementia,"Ferreira, Luis (1, 2); Cavaco, Sofia (1); Bermúdez i Badia, Sergi (3) ","(1) NOVA LINCS, Department of Computer Science, Faculdade de Ciências e Tecnologia, Universidade NOVA de Lisboa, Caparica; 2829-516, Portugal (2) Madeira Interactive Technologies Institute, Funchal, Portugal (3) Faculdade de Ciências Exatas e da Engenharia Madeira Interactive Technologies Institute, Universidade da Madeira, Funchal, Portugal ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,28-Nov-19,"Proceedings of the 23rd Pan-Hellenic Conference of Informatics, PCI 2019",2019,,,,9.78145E+12,10.1145/3368640.3368654,,"23rd Pan-Hellenic Conference of Informatics, PCI 2019","November 28, 2019 - November 29, 2019",,"The possibility of using serious games to stimulate people with dementia (PwD) has gained much attention in recent years. However, most of such games are not adapted to individual needs of such population in terms of the design of technology and its content. Thus, the desired therapeutic outcomes may not be achieved. Alternatively, more traditional approaches, such as the usage of music and reminiscence, have been shown to be able to lead to positive outcomes. Here, we propose a framework for serious games that allows healthcare professionals to customize music and reminiscence-based activities to stimulate PwD. It runs on an augmented reality setup, but also on PC, interactive table and tablet. Results from a usability study show that participants (1) were efficient in using the framework, (2) therapists are very interested in using it for stimulation purposes in PwD and (3) the usage of the framework was adequate in terms of effort and workload for PwD. Future deployments will be discussed in this article regarding the usage of the framework. © 2019 Association for Computing Machinery.",34,Serious games,Augmented reality - mHealth - Neurodegenerative diseases,Customization - Dementia - Music - Reminiscence - Technological Versatility - Usability testing,"461.6 Medicine and Pharmacology - 723 Computer Software, Data Handling and Applications",,,"Number: PEest/UID/CEC/04516/2019, Acronym: -, Sponsor: -; Number: M1420-09-5369-FSE-000001, Acronym: -, Sponsor: -; ","We want to thank Yuri Almeida and Eduardo Gomes for their technical support during the development phase of the platform. Also, we would like to thank the health professionals of Casa de Sa&uacute;de S&atilde;o Jo&atilde;o de Deus for their feedback during the usability study of Musiquence. This project is supported by the Portuguese Foundation for Science and Technology under project NOVA-LINCS (PEest/UID/CEC/04516/2019), supported by ARDITI under the scope of project M1420-09-5369-FSE-000001 Ph.D. Studentship and supported by project Larsys UID/EEA/50009/2019.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Analysis and Design of a Latency Control Protocol for Multi-Path Data Delivery with Pre-Defined QoS Guarantees,"Chiariotti, Federico (1); Kucera, Stepan (2); Zanella, Andrea (1); Claussen, Holger (2) ","(1) Department of Information Engineering, University of Padua, Padua; 35131, Italy (2) Nokia Bell Laboratories, Dublin 15, Ireland ",IEEE/ACM Transactions on Networking,,Institute of Electrical and Electronics Engineers Inc.,"v 27, n 3",p 1165-1178,Jun-19,,2019,,10636692,15582566,,10.1109/TNET.2019.2911122,8723311,,,,"As the capacity and reliability of mobile networks increases, so does the demand for more responsive end-to-end services: applications such as augmented reality, live video conferencing, and smart or autonomous vehicles require reliable, throughput-intensive end-to-end communications with strict delay constraints. Only consistently reliable delivery of data flows well within human interactivity deadlines will enable a truly immersive user experience. To enable data delivery within pre-defined deadlines, controlled on demand by an application or its user, we propose and demonstrate a novel transport-layer protocol for explicit latency control called latency-controlled end-to-end aggregation protocol (LEAP). The LEAP splits a data flow with quality of service (QoS) constraints into multiple subflows that are delivered over multiple parallel links (e.g., Wi-Fi and LTE in a standard smartphone, WiGig, and 5G in the near future). The subflow data rates are set based on a novel proactive forecasting of the achievable channel capacity, subject to application-specific QoS constraints. Cross-path encoding and redundancy adaptation are then used to deliberately balance the trade-off between maximum throughput, required delay, and minimum reliability as function of application/user-specific input parameters. When compared to leading state-of-the-art transport protocols in live network experiments, LEAP exhibits a superior capacity to reliably provide a high and stable throughput with bounded latency, both in wired and wireless scenarios. The LEAP is also the first protocol to allow applications to explicitly set their priorities, giving them the freedom to set the operating point in the trade-off between throughput, latency, and reliability. © 1993-2012 IEEE.",54,Quality of service,5G mobile communication systems - Augmented reality - Data transfer - Economic and social effects - Multipath propagation - Redundancy - Vehicle to vehicle communications - Video conferencing,Application specific - End-to-End communication - End-to-end service - Maximum through-put - Quality of Service constraints - Transport layer protocols - Transport protocols - Wired and wireless,"711 Electromagnetic Waves - 716 Telecommunication; Radar, Radio and Television - 716.4 Television Systems and Equipment - 723 Computer Software, Data Handling and Applications - 971 Social Sciences",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
School-driven mobiquitous invisible paths management for smart territories (Jmagine APPLICATION),"Galli, Greg (2); Mosbah, Yasmin (1); Felin, Rémi (1); Montorsi, Benoît (1); Miranda, Serge (1, 3) ","(1) MBDS Innovation Lab., MIAGE, Université Côte d'Azur, France (2) Tokidev, France (3) LSIS Research Lab, Université d'Aix Marseille, France ","TESCA 2019 - Proceedings of the 2019 1st ACM International Workshop on Technology Enablers and Innovative Applications for Smart Cities and Communities, co-located with the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, ACM BuildSys 2019",AMRITA; Association for Computing Machinery (ACM); UCIrvine; Universite Nice Sophia Antipolis,"Association for Computing Machinery, Inc",,p 54-57,13-Nov-19,"TESCA 2019 - Proceedings of the 2019 1st ACM International Workshop on Technology Enablers and Innovative Applications for Smart Cities and Communities, co-located with the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, ACM BuildSys 2019",2020,,,,9.78145E+12,10.1145/3364544.3364832,,"1st ACM International Workshop on Technology Enablers and Innovative Applications for Smart Cities and Communities, TESCA 2019, co-located with the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, ACM BuildSys 2019","November 13, 2019 - November 14, 2019",,"In this paper, we present the Jmagine APPLICATION for schools and smart territories under development at MBDS innovation laboratory of University of Côte d'Azur (UCA) and deployment in Southern France and abroad (Cambodia, Haïti). Jmagine is an Open Source platform that offers users the ability to follow (from the mobile application) or create (from the web platform) invisible paths in a smart territory. Jmagine provides a web platform to allow teachers to easily build customized invisible paths with creative multidisciplinary content developed by their school children in a very straightforward way. The underlying idea of invisible paths stems from the Song Lines (Bruce Chatwin) of invisible paths of native Australian people on their territories ! Jmagine application implemented this idea with schools. An invisible path is a set of Points of Interest (POI) in a smart city, a smart museum or a smart territory. In a given cultural-rich territory different routes could co-exist making possible to follow one and skip to another one at any time. This innovative application brings together POI and makes it possible to gather enriching and relevant information proposed by creative school children. The concept is to make the smart city and smart territory an open-air museum where, the smartphone acts as a guide and provides multimedia augmented reality. The content of the platform's routes is continuously updated by school children under the guidance and the responsibility of their teachers. Several location-based technologies were used in Jmagine implementations such as image recognition, near-field communication (NFC), LED light beams (Li-Fi) (in Grasse Museum of perfume) or QR codes among others. © 2019 Copyright held by the owner/author(s).",13,Near field communication,Augmented reality - Energy efficiency - Image recognition - Intelligent buildings - Museums - Smart city - Social networking (online) - Visible light communication - Web services,Cloud platforms - Location-based technologies - Mobile app - Mobile applications - Open source platforms - Points of Interest(POI) - QR codes - The near field communication (NFC),"402 Buildings and Towers - 402.2 Public Buildings - 525.2 Energy Conservation - 717.1 Optical Communication Systems - 722.3 Data Communication, Equipment and Techniques - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Holographic near-eye displays based on overlap-add stereograms,"Padmanaban, N.; Yifan Peng; Wetzstein, G. ",,ACM Transactions on Graphics,,"ACM, USA","v 38, n 6",214 (13 pp.),Nov. 2019,,,,0730-0301,,,10.1145/3355089.3356517,,,,,"Holographic near-eye displays are a key enabling technology for virtual and augmented reality (VR/AR) applications. Holographic stereograms (HS) are a method of encoding a light field into a hologram, which enables them to natively support view-dependent lighting effects. However, existing HS algorithms require the choice of a hogel size, forcing a tradeoff between spatial and angular resolution. Based on the fact that the short-time Fourier transform (STFT) connects a hologram to its observable light field, we develop the overlap-add stereogram (OLAS) as the correct method of 'inverting' the light field into a hologram via the STFT. The OLAS makes more efficient use of the information contained within the light field than previous HS algorithms, exhibiting better image quality at a range of distances and hogel sizes. Most remarkably, the OLAS does not degrade spatial resolution with increasing hogel size, overcoming the spatio-angular resolution tradeoff that previous HS algorithms face. Importantly, the optimal hogel size of previous methods typically varies with the depth of every object in a scene, making the OLAS not only a hogel size-invariant method, but also nearly scene independent. We demonstrate the performance of the OLAS both in simulation and on a prototype near-eye display system, showing focusing capabilities and view-dependent effects.",,,augmented reality - Fourier transforms - holography - image reconstruction - image representation - image resolution - optimisation - rendering (computer graphics) - stereo image processing - three-dimensional displays,holographic near-eye displays - stereogram - holographic stereograms - hologram - view-dependent lighting effects - spatial resolution - STFT - observable light field - OLAS - correct method - spatio-angular resolution tradeoff - HS algorithms - optimal hogel size - hogel size-invariant method - prototype near-eye display system - view-dependent effects,"B7260 Display technology - B0260 Optimisation techniques - B0290X Integral transforms in numerical analysis - B4350 Holography - B6135 Optical, image and video signal processing - C5540D Computer displays - C6130B Graphics techniques - C6130V Virtual reality - C1180 Optimisation techniques - C4188 Integral transforms in numerical analysis - C5260B Computer vision and image processing techniques",G03H - G06F3/14 - G06T - H04N5/66 - H04N11/00 - H04N13/00 - H04N13/30,Practical (PRA); Theoretical or Mathematical (THR),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Local light field fusion: practical view synthesis with prescriptive sampling guidelines,"Mildenhall, B.; Srinivasan, P.P.; Ortiz-Cayon, R.; Kalantari, N.K.; Ramamoorthi, R.; Ren Ng; Kar, A. ",,ACM Transactions on Graphics,,"ACM, USA","v 38, n 4",29 (14 pp.),Jul-19,,,,0730-0301,,,10.1145/3306346.3322980,,,,,"We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000X fewer views. We demonstrate our approach's practicality with an augmented reality smart-phone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.",,,augmented reality - image representation - image sampling - learning (artificial intelligence) - neural nets - rendering (computer graphics),local light field fusion - practical view synthesis - prescriptive sampling guidelines - practical learning solution - robust deep learning solution - world scenes - intractably dense view sampling - sample views - sampled view - multiplane image scene representation - plenoptic sampling theory - Nyquist rate view sampling - mobile platforms - MPI scene representation,"B6135 Optical, image and video signal processing - C5260B Computer vision and image processing techniques - C5290 Neural computing techniques - C6130B Graphics techniques - C6130V Virtual reality",G06T - G06N20/00,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Predicting perceived naturalness of human animations based on generative movement primitive models,"Knopp, B. (1); Velychko, D. (1); Dreibrodt, J. (1); Endres, D. (1) ","(1) Univ. of Marburg, Marburg, Germany ",ACM Transactions on Applied Perception,,"ACM, USA","v 16, n 3",15 (18 pp.),Sept. 2019,,,,1544-3558,,,10.1145/3355401,,,,,"We compared the perceptual validity of human avatar walking animations driven by six different representations of human movement using a graphics Turing test. All six representations are based on movement primitives (MPs), which are predictive models of full-body movement that differ in their complexity and prediction mechanism. Assuming that humans are experts at perceiving biological movement from noisy sensory signals, it follows that these percepts should be describable by a suitably constructed Bayesian ideal observer model. We build such models from MPs and investigate if the perceived naturalness of human animations are predictable from approximate Bayesian model scores of the MPs. We found that certain MP-based representations are capable of producing movements that are perceptually indistinguishable from natural movements. Furthermore, approximate Bayesian model scores of these representations can be used to predict perceived naturalness. In particular, we could show that movement dynamics are more important for perceived naturalness of human animations than single frame poses. This indicates that perception of human animations is highly sensitive to their temporal coherence. More generally, our results add evidence for a shared MP-representation of action and perception. Even though the motivation of our work is primarily drawn from neuroscience, we expect that our results will be applicable in virtual and augmented reality settings, when perceptually plausible human avatar movements are required.",,,augmented reality - avatars - Bayes methods - computer animation - motion control - virtual reality - visual perception,perceived naturalness - human animations - generative movement primitive models - human avatar walking animations - human movement - movement primitives - predictive models - full-body movement - prediction mechanism - biological movement - percepts - suitably constructed Bayesian ideal observer model - approximate Bayesian model scores - MP-based representations - natural movements - movement dynamics - shared MP-representation - perceptually plausible human avatar movements,"B6135 Optical, image and video signal processing - C6130V Virtual reality - C3120C Spatial variables control - C5260B Computer vision and image processing techniques - C6130B Graphics techniques",G05D3/00 - G06T - G06T13/00,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
ISAR: An authoring system for interactive tabletops,"Hodaie, Zardosht (1); Haladjian, Juan (1); Bruegge, Bernd (1) ","(1) Technical University of Munich, Munich, Germany ",ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 355-360,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,10.1145/3343055.3360751,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"Despite more than three decades of research in augmented reality and shown positive effect of AR in educational settings, we still don't witness spread of this technology in the schools. Complex technology and limited educational content are among the reasons for this absence. Authoring systems can play a positive role in introduction of AR into the school settings. In this paper we introduce ISAR, a domain-independent authoring system for a camera-projector based interactive tabletop. ISAR allows teachers to create learning content and define interactions for the tabletop themselves, without the need for programming, and hence reduces the entrance barrier of educational practitioners for experimenting with augmented reality and tangible interactions. © 2019 Copyright is held by the owner/author(s).",14,Interactive devices,Augmented reality,Authoring systems - Content creation - Domain independents - Educational contents - Educational settings - End user development - Interactive tabletop - Tangible interaction,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
FRISP: Framework for registering interactive spatial projection,"Byun, Jung-Hyun (1); Ro, Hyocheol (1); Han, Tack-Don (1) ","(1) Department of Computer Science, Yonsei University, Seoul, Korea, Republic of ","International Conference on Intelligent User Interfaces, Proceedings IUI",ACM Special Interest Group on Artificial Intelligence (SIGAI); Specialist Interest Group in Computer-Human Interaction of the ACM (SIGCHI),Association for Computing Machinery,,p 93-94,17-Mar-20,Proceedings of the 25th International Conference on Intelligent User Interfaces Companion. IUI 2020,2020,,,,9.78145E+12,10.1145/3379336.3381458,,"25th International Conference on Intelligent User Interfaces, IUI 2020","March 17, 2020 - March 20, 2020",,"Projection-based augmented reality (AR) is a promising medium for realizing pervasive computing environment, and yet the problem of determining projection-suitable regions and where to project remains. To tackle this problem, we introduce FRISP, a projectionbased augmented reality (AR) Framework designed for Registering Interactive Spatial Projection. The FRISP framework utilizes a pantilt projection-camera (pro-cam) system for capturing geometry and projection mapping. The framework scans and analyzes the geometric properties of a room, in order to determine projection-suitable regions and generate multi-window layouts. Once the multi-windows are registered to the real world, they can be interacted with by the users. The users can assign various widgets or applications to the multi-windows, which are then finally augmented onto the real world and can serve as a base units for realizing the pervasive AR environment. © 2020 International Conference on Intelligent User Interfaces, Proceedings IUI. All rights reserved.",9,User interfaces,Augmented reality - Mapping - Ubiquitous computing,Geometric properties - Multi-Windows - OR applications - Pervasive computing environment - Projection camera - Real-world,"405.3 Surveying - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: -, Acronym: MSIP, Sponsor: Ministry of Science, ICT and Future Planning; Number: -, Acronym: NRF, Sponsor: National Research Foundation of Korea; ",This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No.NRF-2018R1A2A1A05078628).,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Immersive insights: A hybrid analytics system for collaborative exploratory data analysis,"Cavallo, Marco (1); Dholakia, Mishal (1); Havlena, Matous (1); Ocheltree, Kenneth (1); Podlaseck, Mark (1) ",(1) IBM Research ,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364242,3364242,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"In the past few years, augmented reality (AR) and virtual reality (VR) technologies have experienced terrific improvements in both accessibility and hardware capabilities, encouraging the application of these devices across various domains. While researchers have demonstrated the possible advantages of AR and VR for certain data science tasks, it is still unclear how these technologies would perform in the context of exploratory data analysis (EDA) at large. In particular, we believe it is important to better understand which level of immersion EDA would concretely benefit from, and to quantify the contribution of AR and VR with respect to standard analysis workflows. In this work, we leverage a Dataspace reconfigurable hybrid reality environment to study how data scientists might perform EDA in a co-located, collaborative context. Specifically, we propose the design and implementation of Immersive Insights, a hybrid analytics system combining high-resolution displays, table projections, and augmented reality (AR) visualizations of the data. We conducted a two-part user study with twelve data scientists, in which we evaluated how different levels of data immersion affect the EDA process and compared the performance of Immersive Insights with a state-of-the-art, non-immersive data analysis system. © 2019 Association for Computing Machinery.",81,Mixed reality,Augmented reality - Data handling - Data visualization - Information analysis - Visualization,Clustering - Data space - Exploratory data analysis - Hybrid Reality - Virtuality continuum,"723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 903.1 Information Sources and Analysis",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Rare shop: Real & augmented retail bookshop experience using projection mapping,"You, Yuhui (1); Cheng, Kelvin (1) ","(1) Rakuten Institute of Technology, Rakuten Inc., 1-14-1 Tamagawa, Setagaya, Tokyo, Japan ",ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 415-420,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,10.1145/3343055.3360761,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"Augmented reality (AR) technology has been used to integrate online product information to the physical shopping experience by relying on hardware outputs such as head-mounted displays or mobile devices, which interferes with the customers natural in-store shopping experience. Different from traditional AR, spatial augmented reality (SAR) could potentially offer an unmediated augmented physical experience. In this paper, we present a proof-of-concept prototype that augments existing physical shops with online personalized product information without disrupting the customer's natural shopping behavior. We envision a real and augmented book shop experience, using SAR to provide immersive digital and online information overlaid onto physical products, breaking down the boundary between physical space and digital information at retail shops. Moreover, we design unmediated interaction that reacts to the relative spatial and temporal context of the users, based on user's natural interactions with the physical products, without the need for learning special interactions/gestures. We also discuss the design challenges and implications for this in-store shopping experience. © 2019 Copyright is held by the owner/author(s).",12,Product design,Augmented reality - Helmet mounted displays - Sales - Space-based radar,Digital information - Head mounted displays - Natural interactions - On-line information - Online product information - Personalized products - Physical products - Spatial augmented realities,"716.2 Radar Systems and Equipment - 723 Computer Software, Data Handling and Applications - 913.1 Production Engineering",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Extending AR interaction through 3D printed tangible interfaces in an urban planning context,"Narazani, Marla (1); Eghtebas, Chloe (2); Klinker, Gudrun (2); Jenney, Sarah L. (3); Mühlhaus, Michael (3); Petzold, Frank (3) ","(1) Faculty of Computer Science, Technical University of Munich, Munich, Germany (2) Research Group Augmented Reality, Technical University of Munich, Munich, Germany (3) Architectural Informatics, Technical University of Munich, Munich, Germany ",UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 116-118,14-Oct-19,UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332167.3356891,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Embedding conductive material into 3D printed objects enables non-interactive objects to become tangible without the need to attach additional components. We present a novel use for such touch-sensitive objects in an augmented reality (AR) setting and explore the use of gestures for enabling different types of interaction with digital and physical content. In our demonstration, the setting is an urban planning scenario. The multi-material 3D printed buildings consist of thin layers of white plastic filament and a conductive wireframe to enable touch gestures. Attendees can either interact with the physical model or with the mobile AR interface for selecting, adding or deleting buildings. © 2019 Copyright is held by the owner/author(s).",14,3D printers,Augmented reality - Conductive materials - Plastic filaments - Urban planning - User interfaces,3-D printing - Capacitive sensing - Gestures - Interactive objects - Multi materials - Physical model - Tangible interfaces - Tangible user interfaces,"403.1 Urban Planning and Development - 708.2 Conducting Materials - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 745.1.1 Printing Equipment - 817.1 Polymer Products",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
This land AR: An Australian music and sound XR installation a transmedia storytelling approach,"Matthias, Philip (1); Billinghurst, Mark (2); See, Zi Siang (1) ","(1) School of Creative Industries, University of Newcastle, Newcastle; NSW, Australia (2) Emphatic Computing Lab, University of South Australia, Adelaide; SA, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365740,a69,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"This demonstration presents a development of an Augmented Reality (AR) Indigenous music and sound installation, an extended reality (XR) interactive audible experiential approach for augmenting audible elements in a public exhibition setting. It is a transmedia initiative as part of a music project, This Land. The project connected contributors and musicians, involving traditional to contemporary vocal and instrumental sounds. This Land project embraces cultural and social perspectives and related contemporary discourses within the Australia context. As augmented reality was being explored as an on-going study for the project, a number of conventional printed wall design (posters and photograph exhibits) were enhanced with augmented musical and sound elements. This Land project commenced as artistic performative event built around many years of collaboration between staff and students from the School of Creative Industries and the Wollotuka Institute at University of Newcastle (UON). Its vision embraces issues of Indigenisation, decolonisation, reciprocity and language revitalisation. A portable version of This Land AR will be used for the demonstration where users could experience features of the prototype system in the public setting. © 2019 Association for Computing Machinery.",2,Mixed reality,Augmented reality - Image enhancement - Interactive computer graphics,Creative industries - Music - Prototype system - Social perspective - Sound installation - Storytelling approaches - University of Newcastle - User experience,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Container deployment strategy for edge networking,"Wong, Walter (1); Zavodovski, Aleksandr (1); Zhou, Pengyuan (1); Kangasharju, Jussi (1) ","(1) University of Helsinki, Helsinki, Finland ","MECC 2019 - Proceedings of the 2019 4th Workshop on Middleware for Edge Clouds and Cloudlets, Part of Middleware 2019",ACM,"Association for Computing Machinery, Inc",,p 1-6,9-Dec-19,"MECC 2019 - Proceedings of the 2019 4th Workshop on Middleware for Edge Clouds and Cloudlets, Part of Middleware 2019",2019,,,,9.78145E+12,10.1145/3366614.3368101,,"4th Workshop on Middleware for Edge Clouds and Cloudlets, MECC 2019 - Part of Middleware 2019","December 9, 2019 - December 13, 2019",,"Edge computing paradigm has been proposed to support latency-sensitive applications such as Augmented Reality (AR)/ Virtual Reality(VR) and online gaming, by placing computing resources close to where they are most demanded, at the edge of the network. Many solutions have proposed to deploy virtual resources as close as possible to the consumers using virtual machines and containers. However, the most popular container orchestration tools, e.g., Docker Swarm and Kubernetes, do not take into account the locality aspect during deployment, resulting in poor location choices at the edge of the network. In this paper, we propose an edge deployment strategy to tackle the lack of locality awareness of the container orchestrator. In this strategy, the orchestrator collects information about latency and the real-time resource consumption from the current container deployments, providing a bird’s-eye view of the most demanded locations and the best places for deployment to cover the largest number of clients. We evaluated the proposed model using 16 AWS regions across the globe and compared to the standard deployment strategies. The experimental results show our edge strategy reduces the average latency between serving container to the clients by up to 4 times compared to the standard deployment algorithms. © 2019 Association for Computing Machinery.",17,Containers,Augmented reality - Edge computing - Middleware - Scheduling - Virtual reality,Computing paradigm - Computing resource - Deployment - Deployment algorithms - Deployment strategy - Locality awareness - Resource consumption - Sensitive application,"723 Computer Software, Data Handling and Applications - 723.1 Computer Programming - 912.2 Management",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Editing text in the wild,"Wu, Liang (1); Han, Junyu (2); Zhang, Chengquan (2); Liu, Jingtuo (2); Bai, Xiang (1); Liu, Jiaming (2); Ding, Errui (2) ","(1) Huazhong University of Science and Technology, China (2) Department of Computer Vision Technology, (VIS), Baidu Inc, China ",MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,ACM SIGMM,"Association for Computing Machinery, Inc",,p 1500-1508,15-Oct-19,MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,2019,,,,9.78145E+12,10.1145/3343031.3350929,,"27th ACM International Conference on Multimedia, MM 2019","October 21, 2019 - October 25, 2019",,"In this paper, we are interested in editing text in natural images, which aims to replace or modify a word in the source image with another one while maintaining its realistic look. This task is challenging, as the styles of both background and text need to be preserved so that the edited image is visually indistinguishable from the source image. Specifically, we propose an end-to-end trainable style retention network (SRNet) that consists of three modules: text conversion module, background inpainting module and fusion module. The text conversion module changes the text content of the source image into the target text while keeping the original text style. The background inpainting module erases the original text, and fills the text region with appropriate texture. The fusion module combines the information from the two former modules, and generates the edited text images. To our knowledge, this work is the first attempt to edit text in natural images at the word level. Both visual effects and quantitative results on synthetic and real-world dataset (ICDAR 2013) fully confirm the importance and necessity of modular decomposition. We also conduct extensive experiments to validate the usefulness of our method in various real-world applications such as text image synthesis, augmented reality (AR) translation, information hiding, etc. © 2019 Association for Computing Machinery.",41,Textures,Augmented reality,Background inpainting - Information hiding - Modular decomposition - Quantitative result - Text conversion - Text editing - Text Erasure - Visual effects,"723 Computer Software, Data Handling and Applications",,,"Number: 61733007, Acronym: -, Sponsor: -; Number: 2017QYTD08, Acronym: -, Sponsor: -; ","This work is supported by NSFC 61733007, to Dr. Xiang Bai by the National Program for Support of Top-notch Young Professionals and the Program for HUST Academic Frontier Youth Team 2017QYTD08. We sincerely thank Zhen Zhu and Tengteng Huang for their valuable discussions and continuous help to this paper.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Visual link routing in immersive visualisation,"Prouzeau, Arnaud (1); Lhuillier, Antoine (2); Ens, Barrett (1); Weiskopf, Daniel (2); Dwyer, Tim (1) ","(1) Monash University, Melbourne, Australia (2) University of Stuttgart, Stuttgart, Germany ",ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 241-253,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,10.1145/3343055.3359709,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"In immersive display environments, such as virtual or augmented reality, we can make explicit the connections between data points in visualisations and their context in the world, or in other visualisations. This paper considers the requirements and design space for drawing such links in order to minimise occlusion and clutter. A novel possibility in immersive environments is to optimise the link layout with respect to a particular point of view. In collaborative scenarios there is the need to do this for multiple points of view. We present an algorithm to achieve such link layouts and demonstrate its applicability in a variety of practical use cases. Copyright © 2019 Association of Computing Machinery.",57,Visualization,Augmented reality - Information systems - Virtual reality,Immersive - Immersive display - Immersive environment - Immersive visualisation - Information visualization - Link Routing - Multiple points - Visual Links,"723 Computer Software, Data Handling and Applications - 903.2 Information Dissemination",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
EXEC: Elastic extensible edge cloud,"Zavodovski, Aleksandr (1); Mohan, Nitinder (1); Bayhan, Suzan (2); Wong, Walter (1); Kangasharju, Jussi (1) ","(1) University of Helsinki, Finland (2) TU Berlin, Germany ","EdgeSys 2019 - Proceedings of the 2nd ACM International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2019",ACM SIGOPS,"Association for Computing Machinery, Inc",,p 24-29,25-Mar-19,"EdgeSys 2019 - Proceedings of the 2nd ACM International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2019",2019,,,,9.78145E+12,10.1145/3301418.3313941,,"2nd ACM International Workshop on Edge Systems, Analytics and Networking, EdgeSys 2019, Part of EuroSys 2019","March 25, 2019 - March 25, 2019",,"Edge computing (EC) extends the centralized cloud computing paradigm by bringing computation into close proximity to the end-users, to the edge of the network, and is a key enabler for applications requiring low latency such as augmented reality or content delivery. To make EC pervasive, the following challenges must be tackled: how to satisfy the growing demand for edge computing facilities, how to discover the nearby edge servers, and how to securely access them? In this paper, we present ExEC, an open framework where edge providers can offer their capacity and be discovered by application providers and end-users. ExEC aims at the unification of interaction between edge and cloud providers so that cloud providers can utilize services of third-party edge providers, and any willing entity can easily become an edge provider. In ExEC, the unfolding of initially cloud-deployed application towards edge happens without administrative intervention, since ExEC discovers available edge providers on the fly and monitors incoming end-user traffic, determining the near-optimal placement of edge services. ExEC is a set of loosely coupled components and common practices, allowing for custom implementations needed to embrace the diverse needs of specific EC scenarios. ExEC leverages only existing protocols and requires no modifications to the deployed infrastructure. Using real-world topology data and experiments on cloud platforms, we demonstrate the feasibility of ExEC and present results on its expected performance. © 2019 Association for Computing Machinery.",40,Edge computing,Augmented reality,Application providers - Close proximity - Cloud platforms - Cloud providers - Computing facilities - Content delivery - Deployed applications - Loosely coupled,"723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: -, Sponsor: Academy of Finland; Number: 317086, Acronym: -, Sponsor: -; Number: 314167, Acronym: -, Sponsor: -; Number: 313477, Acronym: -, Sponsor: -; ","This work was supported by the Academy of Finland in the BCDC (314167), AIDA (317086), and WMD (313477) projects.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Interactive virtual-reality fire extinguisher with haptic feedback,"Seo, Sang-Woo (1); Kwon, Seungjoon (1); Hassan, Waseem (2); Jeon, Seokhee (2) ","(1) Creative Content Research Division ETRI, Daejeon, Korea, Republic of (2) Department of Computer Science and Engineering, Kyung Hee University, Yongin, Korea, Republic of ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364725,3364725,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"We present an interactive virtual-reality (VR) fire extinguisher that provides both realistic viewing using a head-mounted display (HMD) and kinesthetic experiences using a pneumatic muscle and vibrotactile transducer. The VR fire extinguisher is designed to train people to use a fire extinguisher skillfully in real fire situations. We seamlessly integrate three technologies: VR, object motion tracking, and haptic feedback. A fire scene is immersed in the HMD, and a motion tracker is used to replicate a real designed object into the virtual environment to realize augmented reality. In addition, when the handle of the fire extinguisher is squeezed to release the extinguishing agent, the haptic device generates both vibrotactile and air flow tactile feedback signals, providing the same experience as that obtained while using a real fire extinguisher. © 2019 Copyright held by the owner/author(s).",4,Fires,Augmented reality - Fire extinguishers - Helmet mounted displays - Motion analysis - Virtual reality,Extinguishing agent - Firefighting - Haptic feedbacks - Head mounted displays - Interactive virtual reality - Object motion tracking - Pneumatic muscle - Tactile feedback,"723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 914.2 Fires and Fire Protection",,,,This research was supported by a grant (2019-MOIS34-001) from Preventive Safety Service Technology Development Program funded by Korean Ministry of Interior and Safety (MOIS).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Dealing with clutter in augmented museum environments,"Zhao, Wanqi (1); Stevenson, Duncan (1); Gardner, Henry (1); Adcock, Matt (2) ","(1) Australian National University, Australia (2) CSIRO, Australian National University, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365683,a21,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"Augmented Reality (AR) can be used in museum and exhibition spaces to extend the available information space. However, AR scenes in such settings can become cluttered when exhibits are displayed close to one another. To investigate this problem, we have implemented and evaluated four AR headset interaction techniques for the Microsoft HoloLens that are based on the idea of Focus+Context (F+C) visualisation [Kalkofen et al. 2007]. These four techniques were made up of all combinations of interaction and response dimensions where the interaction was triggered by either 'walk' (approaching an exhibit) or 'gaze' (scanning/looking at an exhibit) and the AR holograms responded dynamically in either a 'scale' or 'frame' representation. We measured the efficiency and accuracy of these four techniques in a user study that examined their performance in an abstracted exhibition setting when undertaking two different tasks ('seeking' and 'counting'). The results of this study indicated that the 'scale' representation was more effective at reducing clutter than the 'frame' representation, and that there was a user preference for the 'gaze-scale' technique. © 2019 Association for Computing Machinery.",25,Exhibitions,Augmented reality - Clutter (information theory) - Interactive computer graphics - Museums - Radar clutter - Virtual reality - Visualization,Focus + context - Information spaces - Interaction techniques - MicroSoft - User interaction - User study,"402.2 Public Buildings - 716.1 Information Theory and Signal Processing - 716.2 Radar Systems and Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
EAGER: Edge-Aided imaGe undERstanding system,"He, Jianzhong (1); Liu, Xiaobin (1); Zhang, Shiliang (1) ","(1) School of Electronic Engineering and Computer Science, Peking University, Beijing; 100871, China ",ICMR 2019 - Proceedings of the 2019 ACM International Conference on Multimedia Retrieval,ACM SIGMM,"Association for Computing Machinery, Inc",,p 408-412,5-Jun-19,ICMR 2019 - Proceedings of the 2019 ACM International Conference on Multimedia Retrieval,2019,,,,9.78145E+12,10.1145/3323873.3326925,,"2019 ACM International Conference on Multimedia Retrieval, ICMR 2019","June 10, 2019 - June 13, 2019",,"Image understanding is a fundamental task for many multimedia and computer vision applications, such as self-driving, multimedia retrieval, and augmented reality, etc. In this paper, we demonstrate that edge detection could aid image understanding tasks such as semantic segmentation, optical flow estimation, and object proposal generation. Based on our recent research efforts on edge detection, we develop a robust and efficient Edge-Aided imaGe undERstanding system named as EAGER. EAGER is built on a compact and efficient edge detection module, which is constructed with a bi-directional cascade network, multi-scale feature enhancement, and layer-specific training supervision, respectively. Based on detected edges, EAGER achieves accurate semantic segment, optical flow estimation, as well as object bounding-box proposal generation for user-uploaded images and videos. © 2019 Association for Computing Machinery.",24,Image segmentation,Augmented reality - Edge detection - Image understanding - Optical flows - Semantics,Computer vision applications - Image understanding systems - Multi-scale features - Multimedia Retrieval - Object proposal - Optical flow estimation - Recent researches - Semantic segmentation,"723 Computer Software, Data Handling and Applications - 741.1 Light/Optics",,,,"This work is supported in part by Beijing Natural Science Foundation under Grant No. JQ18012, in part by Natural Science Foundation of China under Grant No. 61620106009, 61572050, 91538111.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A compact retinal scan near-eye display,"Akutsu, Katsuyuki (1); Seino, Susumu (1); Ogawa, Yusuke (1); Ohki, Kenji (1); Takahashi, Atsushi (1); Ueda, Daisuke (1); Ogawa, Ryo (1); Imamura, Teppei (1); Yoshikaie, Akira (1) ","(1) Sony Corporation, Japan ","ACM SIGGRAPH 2019 Emerging Technologies, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Emerging Technologies, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3305367.3327977,,"ACM SIGGRAPH 2019 Emerging Technologies - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"A compact full color laser beam scanning (LBS)-based Augmented Reality (AR) near-eye display has been developed. By the unique relay optical system adopting a novel holographic grating to compensate the color dispersion of holographic image combiner in front of the eye, we have achieved high resolution (1280x720p), large field of view (47degree diagonal), high transparency (over 85%) and hand-held miniaturization. The prototype implemented only two connectors of USB Type-C and HDMI Type-D to supply power and video signal of HDMI interface respectively from small control box using two cables. © held by the owner/author(s).",2,Holographic displays,Augmented reality - Holographic optical elements - Holography - Interactive computer graphics - Laser beams - Optical systems,Color dispersion - High resolution - High transparency - Holographic images - Large field of views - Relay optical system - Retinal scans - Video signal,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 741.3 Optical Devices and Systems - 743 Holography - 744.8 Laser Beam Interactions - 746 Imaging Techniques",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Spatially accurate generative music with AR drawing,"Yoo, Kyungjin (1); Schwelling, Eli (1) ","(1) University of Maryland, United States ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3365048,3365048,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Recent experiments in semi-automatically generating ambient music have yielded emotionally affecting results, leading scientists and musicians alike to develop and experiment with computational systems for creating audible art with varying degrees of success. Most of these systems are based either in analogue technology such as classic tape-reel recording systems or digital systems like virtual synthesizers triggered by a combination of developer-defined values and random number generation. In this paper, I outline the conceptual reasoning behind and development of one such generative music system which uses a simple but versatile virtual synthesizer to generate sound and sequences of repeating randomly generated notes drawn by the user in augmented reality to formulate the patterns and spatial origin of each sound contributing to the entire generative piece. © 2019 Copyright held by the owner/author(s).",,Random number generation,Augmented reality - Education - Flow visualization - Museums - Virtual reality,Analogue technology - Computational system - Digital system - Generative musics - Interactive - Music - Recording systems,"402.2 Public Buildings - 631.1 Fluid Flow, General - 723 Computer Software, Data Handling and Applications - 922.2 Mathematical Statistics",,,"Number: -, Acronym: UMD, Sponsor: University of Maryland; ",The authors would like to acknowledge the University of Maryland's FIRE program for the opportunity to pursue this research.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
3D point cloud geometry compression on deep learning,"Huang, Tianxin (1, 2); Liu, Yong (1, 2) ","(1) State Key Laboratory of Industrial Control Technology, Zhejiang University, China (2) NetEase Fuxi AI Lab ",MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,ACM SIGMM,"Association for Computing Machinery, Inc",,p 890-898,15-Oct-19,MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,2019,,,,9.78145E+12,10.1145/3343031.3351061,,"27th ACM International Conference on Multimedia, MM 2019","October 21, 2019 - October 25, 2019",,"3D point cloud presentation has been widely used in computer vision, automatic driving, augmented reality, smart cities and virtual reality. 3D point cloud compression method with higher compression ratio and tiny loss is the key to improve data transportation efficiency. In this paper, we propose a new 3D point cloud geometry compression method based on deep learning, also an auto-encoder performing better than other networks in detail reconstruction. It can reach much higher compression ratio than the state-of-art while keeping tolerable loss. It also supports parallel compressing multiple models by GPU, which can improve processing efficiency greatly. The compression process is composed of two parts. Firstly, Raw data is compressed into codeword by extracting feature of raw model with encoder. Then, the codeword is further compressed with sparse coding. Decompression process is implemented in reverse order. Codeword is recovered and fed into decoder to reconstruct point cloud. Detail reconstruction ability is improved by a hierarchical structure in our decoder. Latter outputs are grown from former fuzzier outputs. In this way, details are added to former output by latter layers step by step to make a more precise prediction. We compare our method with PCL compression and Draco compression on ShapeNet40 part dataset. Our method may be the first deep learning-based point cloud compression algorithm. The experiments demonstrate it is superior to former common compression algorithms with large compression ratio, which can also reserve original shapes with tiny loss. © 2019 Association for Computing Machinery.",26,Deep learning,Augmented reality - Automobile drivers - Data compression ratio - Decoding - Efficiency - Geometry - Image reconstruction - Signal encoding - Virtual reality,3D point cloud - Auto encoders - Compression algorithms - Compression process - Geometry compression - Hierarchical structures - Higher compression ratios - Transportation efficiency,"432 Highway Transportation - 716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 913.1 Production Engineering - 921 Mathematics",,,"Number: U1509210?, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; ",This work is supported by the National Natural Science Foundation of China under Grant U1509210?61836015.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Effects of age and motivation for visiting on AR museum experiences,"Park, Narae (1); Hong, Yohan (1); Pak, Hyunjeong (1); Nam, Jung Who (1); Kim, Kyoungsu (2); Gil, Kyungwon (2); Pyo, Junbom (1); Lee, Kyoobin (3) ","(1) Korea Culture Technology Institute, Korea, Republic of (2) VIRNECT (3) Gwangju Institute of Science and Technology, Korea, Republic of ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364711,3364711,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Augmented reality(AR) provides a unique viewing experience at museums where people understand abstract history through physical artifacts. Although AR usage in museum settings has been increasing, it is not well understood how AR viewing experience differs in different groups of visitors, which can be problematic considering that museums are places visited by diverse groups of people. In this study, we evaluate the differences in AR experiences according to the characteristics of the visitors. The results show the effect of AR usage in museum settings with visitors' different age groups and motivations for visiting. © 2019 Copyright held by the owner/author(s).",4,Museums,Augmented reality - Motivation - Virtual reality,Age groups - Physical artifacts - User characteristics,"402.2 Public Buildings - 723 Computer Software, Data Handling and Applications - 912.4 Personnel",,,"Number: -, Acronym: KOCCA, Sponsor: Korea Creative Content Agency; Number: -, Acronym: KOCCA, Sponsor: Korea Creative Content Agency; Number: -, Acronym: MCST, Sponsor: Ministry of Culture, Sports and Tourism; ","This research is supported by Ministry of Culture, Sport and Tourism(MCST) and Korea Creative Content Agency(KOCCA) in the Culture Technology(CT) Research ? Development Program 2019 through the Korea Culture Technology Institute(KCTI), Gwangju Institute of Science and Technology(GIST). Thanks to the National Museum of Korea and their curator, Daehwan Kim, for assistance on conducting this experiment.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
InfoLED: Augmenting LED indicator lights for device positioning and communication,"Yang, Jackie (1); Landay, James A. (1) ","(1) Stanford University, Stanford; CA, United States ",UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 175-187,17-Oct-19,UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332165.3347954,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Augmented Reality (AR) has the potential to expand our capability for interacting with and comprehending our surrounding environment. However, current AR devices treat electronic appliances no different than common non-interactive objects, which substantially limits the functionality of AR. We present InfoLED, a positioning and communication system based on indicator lights that enables appliances to transmit their location, device IDs, and status information to the AR client without changing their visual design. By leveraging human insensitivity to high-frequency brightness flickering, InfoLED transmits all of that information without disturbing the original function as an indicator light. We envision InfoLED being used in three categories of application: malfunctioning device diagnosis, appliances control, and multi-appliance configuration. We conducted three user studies, measuring the performance of the InfoLED system, the human readability of the patterns and colors displayed on the InfoLED, and users' overall preference for InfoLED. The study results showed that InfoLED can work properly from a distance of up to 7 meters in indoor conditions and it did not interfere with our participants' ability to comprehend the high-level patterns and colors of the indicator light. Overall, study subjects prefer InfoLED to an ArUco 2D barcode-based baseline system and reported less cognitive load when using our system. Copyright © 2019 Association of Computing Machinery.",45,User interfaces,Augmented reality - Internet of things - Light - Light emitting diodes - Visible light communication,Baseline systems - Electronic appliances - High frequency HF - Human readability - Indoor conditions - Interactive objects - Status informations - Surrounding environment,"714.2 Semiconductor Devices and Integrated Circuits - 717.1 Optical Communication Systems - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 741.1 Light/Optics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Dynamic occlusion handling for real-time AR applications,"Jorge, Joaquim (1); Dos Anjos, Rafael Kuffner (1); Silva, Ricardo (2) ","(1) INESC-ID, Técnico, U. Lisboa, Lisboa, Portugal (2) Instituto Superior Técnico, U. Lisboa, Lisboa, Portugal ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365700,a15,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"Augmented reality (AR) allows computer generated graphics to be overlaid in images or video captured by a camera in real time. This technology is often used to enhance perception by providing extra information or simply by enriching the experience of the user. AR offers a significant potential in many applications such as industrial, medical, education and entertainment. However, for AR to achieve the maximum potential and become fully accepted, the real and virtual objects within the user's environment must become seamlessly integrated. Three main types of problems arise when we try to achieve this effect: Illumination issues, tracking difficulties and occlusion troubles. In this work we present an algorithm to handle AR occlusions in real time. Our approach uses raw depth information of the scene to realize a rough foreground/background segmentation.We use this information, as well as details from color data to estimate a blending coefficient and combine the virtual objects with the real objects into a single image. After experimenting with different scenes we show that our approach is able to produce consistent and aesthetically pleasing occlusions between virtual and real objects, with a low computational cost. Furthermore, we explore different alternatives to improving the quality of the final results while overcoming limitations of previous methods. © 2019 Association for Computing Machinery.",46,Virtual reality,Augmented reality - Interactive computer graphics,Alpha matting - Computational costs - Computer generated graphics - Depth information - Foreground/background - Occlusion handling - Real time - Virtual objects,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Transparent AR processing acceleration at the edge,"Trinelli, Marco (1); Gallo, Massimo (1); Rifai, Myriana (1); Pianese, Fabio (1) ","(1) Nokia Bell Labs, United States ","EdgeSys 2019 - Proceedings of the 2nd ACM International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2019",ACM SIGOPS,"Association for Computing Machinery, Inc",,p 30-35,25-Mar-19,"EdgeSys 2019 - Proceedings of the 2nd ACM International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2019",2019,,,,9.78145E+12,10.1145/3301418.3313942,,"2nd ACM International Workshop on Edge Systems, Analytics and Networking, EdgeSys 2019, Part of EuroSys 2019","March 25, 2019 - March 25, 2019",,"Mobile devices are increasingly capable of supporting advanced functionalities but still face fundamental resource limitations. While the development of custom accelerators for compute-intensive functions is progressing, precious battery life and quality vs. latency trade-offs are limiting the potential of applications relying on processing real-time, computational-intensive functions, such as Augmented Reality. Transparent network support for on-the-fly media processing at the edge can significantly extend the capabilities of mobile devices without the need for API changes. In this paper we introduce NEAR, a framework for transparent live video processing and augmentation at the network edge, along with its architecture and preliminary performance evaluation in an object detection use case. © 2019 Association for Computing Machinery.",22,Video signal processing,Augmented reality - Economic and social effects - Edge computing - Learning systems - Object detection,Battery life - ITS architecture - Live video - Media processing - Network edges - On the flies - Resource limitations - Transparent network,"716.4 Television Systems and Equipment - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 971 Social Sciences",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
VideoPipe: Building video stream processing pipelines at the edge,"Salehe, Mohammad (1); Hu, Zhiming (2); Mortazavi, Seyed Hossein (1); Capes, Tim (2); Mohomed, Iqbal (2) ","(1) University of Toronto, Toronto, Canada (2) Samsung AI Center, Toronto, Canada ","Middleware Industry 2019 - Proceedings of the 2019 20th International Middleware Conference Industrial Track, Part of Middleware 2019",ACM,"Association for Computing Machinery, Inc",,p 43-49,9-Dec-19,"Middleware Industry 2019 - Proceedings of the 2019 20th International Middleware Conference Industrial Track, Part of Middleware 2019",2019,,,,9.78145E+12,10.1145/3366626.3368131,,"20th International Middleware Conference Industrial Track, Middleware Industry 2019 - Part of Middleware 2019","December 9, 2019 - December 13, 2019",,"Real-time video processing in the home, with the benefits of low latency and strong privacy guarantees, enables virtual reality (VR) applications, augmented reality (AR) applications and other next-gen interactive applications. However, processing video feeds with computationally expensive machine learning algorithms may be impractical on a single device due to resource limitations. Fortunately, there are ubiquitous underutilized heterogeneous edge devices in the home. In this paper, we propose VideoPipe, a system that bridges the gap and runs flexible video processing pipelines on multiple devices. Towards this end, with inspirations from Function-as-a-Service (FaaS) architecture, we have unified the runtime environments of the edge devices. We do this by introducing modules, which are the basic units of a video processing pipeline and can be executed on any device. With the uniform design of input and output interfaces, we can easily connect any of the edge devices to form a video processing pipeline. Moreover, as some devices support containers, we further design and implement stateless services for more computationally expensive tasks such as object detection, pose detection and image classification. As they are stateless, they can be shared across pipelines and can be scaled easily if necessary. To evaluate the performance of our system, we design and implement a fitness application on three devices connected through Wi-Fi. We also implement a gesture-based Internet of Things (IoT) control application. Experimental results show the the promises of VideoPipe for efficient video analytics on the edge. © 2019 Association for Computing Machinery.",33,Pipeline processing systems,Augmented reality - Edge computing - Internet of things - Learning algorithms - Machine learning - Middleware - Object detection - Pipe linings - Pipelines - Video streaming - Virtual reality,Control applications - Design and implements - Expensive machines - Interactive applications - Internet of Things (IOT) - Real-time video processing - Resource limitations - Runtime environments,"619.1 Pipe, Piping and Pipelines - 619.1.1 Pipe Accessories - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Technologies for social augmentations in user-embodied virtual reality,"Roth, Daniel (1); Bente, Gary (2); Kullmann, Peter (1); Mal, David (1); Purps, Chris Felix (1); Vogeley, Kai (3); Latoschik, Marc Erich (1) ","(1) University of Würzburg, HCI Group, Würzburg, Germany (2) Michigan State University, East Lansing, United States (3) University Hospital Cologne, Cologne, Germany ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364269,3364269,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Technologies for Virtual, Mixed, and Augmented Reality (VR, MR, and AR) allow to artificially augment social interactions and thus to go beyond what is possible in real life. Motivations for the use of social augmentations are manifold, for example, to synthesize behavior when sensory input is missing, to provide additional affordances in shared environments, or to support inclusion and training of individuals with social communication disorders. We review and categorize augmentation approaches and propose a software architecture based on four data layers. Three components further handle the status analysis, the modification, and the blending of behaviors. We present a prototype (injectX) that supports behavior tracking (body motion, eye gaze, and facial expressions from the lower face), status analysis, decision-making, augmentation, and behavior blending in immersive interactions. Along with a critical reflection, we consider further technical and ethical aspects. © 2019 Copyright held by the owner/author(s).",75,Virtual reality,Artificial intelligence - Augmented reality - Blending - Decision making,Augmented social interactions - Behavior tracking - Critical reflections - Ethical aspects - Facial Expressions - Social communications - Social interactions - Three component,"723 Computer Software, Data Handling and Applications - 723.4 Artificial Intelligence - 802.3 Chemical Operations - 912.2 Management",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
VARiable HMD: Optical see-through HMD for AR and VR,"Yamada, Wataru (1, 2); Manabe, Hiroyuki (1); Ikeda, Daizo (1); Rekimoto, Jun (2) ","(1) Research Labs, NTT DOCOMO, Kanagawa, Japan (2) University of Tokyo, Tokyo, Japan ",UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 131-133,14-Oct-19,UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332167.3356896,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Virtual reality (VR) and augmented reality (AR) with head-mounted displays (HMDs) are rapidly becoming popular. Unfortunately, optical see-through HMDs for AR are not compatible with VR and users must prepare the other type of HMD to experience VR. We present a new method for optical see-through HMDs that enables switching between VR and AR by changing the degree of transparency. The proposed method controls liquid crystals to change the transparency in an enclosure constructed of polarizing plates to achieve the switch. The proposed method does not require the use of expensive modules and is applicable to various HMDs including those that are smartphone based and retinal projection based. We construct a prototype implementing the proposed method and evaluate its performance and unique characteristics. © 2019 Copyright is held by the owner/author(s).",8,Helmet mounted displays,Augmented reality - Liquid crystals - Mixed reality - Smartphones - Street traffic control - Transparency - User interfaces,Head mounted displays - Optical see-though - Optical see-through,"406.2 Roads and Streets - 718.1 Telephone Systems and Equipment - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 741.1 Light/Optics",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Evaluating the impact of point marking precision on situated modeling performance,"Soares, Leonardo Pavanatto (1); Bowman, Doug A. (1); Pinho, Márcio Sarroglia (2) ","(1) Center for HCI, Virginia Tech, Blacksburg; VA, United States (2) School of Technology, PUCRS, Porto Alegre, RS, Brazil ",Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,10.1145/3357251.3357586,a10,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,"Three-dimensional modeling in augmented reality allows the user to create or modify the geometry of virtual content registered to the real world. One way of correctly placing the model is by creating points over real-world features and designing the model derived from those points.We investigate the impact of using point marking techniques with different levels of precision on the performance of situated modeling, considering accuracy, and ease of use. Results from a formal user study indicate that high-precision point marking techniques are needed to ensure the accuracy of the model, while ease of use is affected primarily by perceptual issues. In domains where correctness of the model is critical for user understanding and judgment, higher precision is needed to ensure the usefulness of the application. © 2019 Association for Computing Machinery.",21,3D modeling,Augmented reality,Ease-of-use - High-precision - Model performance - Real-world - Three-dimensional model - User study,"723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: CAPES, Sponsor: Coordena&ccedil;&atilde;o de Aperfei&ccedil;oamento de Pessoal de N&iacute;vel Superior; Number: -, Acronym: CAPES, Sponsor: Coordena&ccedil;&atilde;o de Aperfei&ccedil;oamento de Pessoal de N&iacute;vel Superior; Number: -, Acronym: ONR, Sponsor: Office of Naval Research; ",Our research is partially funded by (a) the Coordena&ccedil;&atilde;o de Aper-fei&ccedil;oamento de Pessoal de Nivel Superior &ndash; Brasil (CAPES) &ndash; Finance Code 001 and (b) the Office of Naval Research.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A left-hand advantage: Motor asymmetry in touchless input,"Habibi, Pantea (1); Chattopadhyay, Debaleena (1) ","(1) Department of Computer Science, University of Illinois, Chicago, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312974,3312974,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Touchless gesture is a common input type when interacting with large displays or virtual and augmented reality applications. In touchless input, users may alternate between hands or use bimanual gestures. But touchless performance in nondominant hands is little explored-even though cognitive science and neuroscience studies show cerebral hemispheric specialization causes performance differences between dominant and nondominant hands in lateralized individuals. Drawing on theories that account for between-hand differences in rapid-aimed movements, we characterize motor asymmetry in touchless input. Results from a controlled study (n = 20, right-handed) show freehand touchless input produces significantly smaller between-hand performance differences than a mouse in pointing and dragging. We briefly discuss the HCI implications of motor asymmetry in an input type. © 2019 Copyright held by the owner/author(s).",10,Computation theory,Augmented reality - Human engineering - Mammals,Cognitive science - Hemispheric specialization - Large displays - Mid-air gestures - Nondominant hand - Touchless - User performance - Virtual and augmented reality,"461.4 Ergonomics and Human Factors Engineering - 721.1 Computer Theory, Includes Formal Logic, Automata Theory, Switching Theory, Programming Theory - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: ERL, Sponsor: Electronics Research Laboratory, Volkswagen of America; ",We thank the Electronic Visualization Laboratory for the apparatus used in the experiments.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Audible panorama: Automatic spatial audio generation for panorama imagery,"Huang, Haikun (1); Solah, Michael (1); Li, Dingzeyu (2); Yu, Lap-Fai (3) ","(1) University of Massachusetts, Boston, United States (2) Adobe Research, Columbia University, United States (3) George Mason University, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300851,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"As 360° cameras and virtual reality headsets become more popular, panorama images have become increasingly ubiquitous. While sounds are essential in delivering immersive and interactive user experiences, most panorama images, however, do not come with native audio. In this paper, we propose an automatic algorithm to augment static panorama images through realistic audio assignment. We accomplish this goal through object detection, scene classifcation, object depth estimation, and audio source placement. We built an audio fle database composed of over 500 audio fles to facilitate this process. We designed and conducted a user study to verify the efcacy of various components in our pipeline. We run our method on a large variety of panorama images of indoor and outdoor scenes. By analyzing the statistics, we learned the relative importance of these components, which can be used in prioritizing for power-sensitive time-critical tasks like mobile augmented reality (AR) applications. © 2019 Association for Computing Machinery.",36,Audio acoustics,Augmented reality - Human engineering - Object detection - Object recognition - Virtual reality,Automatic algorithms - Depth Estimation - Immersive media - Mobile augmented reality - Panorama images - Spatial audio - User experience - Virtual-reality headsets,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 751.1 Acoustic Waves",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Lake: A digital wizard of Oz prototyping tool,"Finke, Andrew (1) ","(1) Northwestern University, Evanston; IL, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3308455,3308455,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Mobile app designers aim to develop the best mobile software interfaces in the least amount of time, and rely on testing ideas with prototypes in lieu of building costly, fully functioning applications. Yet, designers cannot effectively prototype some complex app experiences, including augmented reality applications like Pokémon GO, because existing tools lack the needed features, or because prototyping in them is too time intensive to be feasible. To solve this problem, we introduce Lake, a mobile application prototyping tool that enables the creation of complex mobile applications with the same ease as paper prototyping. By leveraging the Wizard of Oz technique used in paper prototyping in our digital medium, we enable designers to prototype at the same low cost as paper, but at a much higher fidelity. Through a pilot study (N=6), we find that designers are able to gather organic in-context feedback from complex prototypes made with Lake. © 2019 Copyright held by the owner/author(s).",9,Software prototyping,Application programs - Augmented reality - Human engineering - Lakes - Mobile computing - Software testing,Augmented reality applications - Mobile applications - Mobile apps - Mobile softwares - Paper prototyping - Pilot studies - Prototyping tools - Wizard of Oz,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,"We thank Haoqi Zhang, Kapil Garg, Meg Grasse, Garrett Hedman, Sarah Lim, and the rest of the Design, Technology, and Research community for their support. We also thank Ian Baird, Jenny Chen, Grace Chin, Bill Dudney, Ben Fearnley, Craig Federighi, Matthew Firlik, Peter Hajas, Tim Isted, and Cheryl Thomas for their mentorship and guidance. This work was supported by the National Science Foundation under Grant 1464315, and an Undergraduate Research Grant from Northwestern University.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Designing a mobile game that develops emotional resiliency in Indian country,"Vigil-Hayes, Morgan (1); Collier, Ann Futterman (1); Castillo, Giovanni (1); Blackhorse, Davona (1); Awbery, Nikole (1); Abrahim, John-Paul (1) ","(1) Northern Arizona University, Flagstaff; AZ, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312790,3312790,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Communities in Indian Country experience severe behavioral health inequities [11, 12]. Based on recent research investigating scalable behavioral health interventions and therapeutic best practices for Native American (NA) communities, we propose ARORA, a social and emotional learning intervention delivered over a networked mobile game that uses geosocial gaming mechanisms enhanced with augmented reality technology. Focusing on the Navajo community, we take a community-based participatory research approach to include NA psychologists, community health workers, and educators as co-designers of the intervention activities and gaming mechanisms. Critical questions involve operation of the application across low-infrastructure landscapes as well scalability of design practices to be inclusive of the many diverse NA cultural communities in Indian Country. © 2019 Copyright held by the owner/author(s).",12,Behavioral research,Augmented reality - Health - Human engineering - mHealth,Augmented reality technology - Behavioral health - Community Health Workers - Community-based participatory research - Critical questions - Gamification - Indigenous - Intervention activities,"461.4 Ergonomics and Human Factors Engineering - 461.6 Medicine and Pharmacology - 723 Computer Software, Data Handling and Applications",,,,Supported by NIH/NIMHD RCMI U54MD012388 (Baldwin/Stearns-MPI).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Accelerated volume rendering with Chebyshev distance maps,"Deakin, Lachlan (1); Knackstedt, Mark (1) ","(1) Applied Mathematics Australian National University, Canberra; ACT, Australia ","SIGGRAPH Asia 2019 Technical Briefs, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 25-28,17-Nov-19,"SIGGRAPH Asia 2019 Technical Briefs, SA 2019",2019,,,,9.78145E+12,10.1145/3355088.3365164,,"SIGGRAPH Asia 2019 Technical Briefs - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,Volume rendering has useful applications with emerging technologies such as virtual and augmented reality. The high frame rate targets of these technologies poses a problem for volume rendering because of its very high computational complexity compared with conventional surface rendering. We developed an efficient empty space skipping algorithm for accelerating volume rendering. A distance map is generated which indicates the Chebyshev distance to the nearest occupied region (with non-transparent voxels) within a volume. The distance map is used to efficiently skip empty regions while volume ray casting. We show improved performance over state-of-the-art empty space skipping techniques. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.,11,Volume rendering,Augmented reality - Interactive computer graphics,Distance map - Emerging technologies - Empty space skipping - Ray casting - State of the art - Surface rendering - Virtual and augmented reality - Volume ray casting,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,This work is funded by the Australian Government Research Training Program (AGRTP) with additional support from the Australian National Laboratory for X-ray Micro Computed Tomography.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Predicting perceived naturalness of human animations based on generative movement primitive models,"Knopp, Benjamin (1); Velychko, Dmytro (1); Dreibrodt, Johannes (1); Endres, Dominik (1) ","(1) Department of Psychology, University of Marburg, Gutenbergstraße 18, Marburg; 35039, Germany ",ACM Transactions on Applied Perception,,Association for Computing Machinery,"v 16, n 3",,Sep-19,,2019,,15443558,15443965,,10.1145/3355401,15,,,,"We compared the perceptual validity of human avatar walking animations driven by six different representations of human movement using a graphics Turing test. All six representations are based on movement primitives (MPs), which are predictive models of full-body movement that differ in their complexity and prediction mechanism. Assuming that humans are experts at perceiving biological movement from noisy sensory signals, it follows that these percepts should be describable by a suitably constructed Bayesian ideal observer model. We build such models from MPs and investigate if the perceived naturalness of human animations are predictable from approximate Bayesian model scores of the MPs. We found that certain MP-based representations are capable of producing movements that are perceptually indistinguishable from natural movements. Furthermore, approximate Bayesian model scores of these representations can be used to predict perceived naturalness. In particular, we could show that movement dynamics are more important for perceived naturalness of human animations than single frame poses. This indicates that perception of human animations is highly sensitive to their temporal coherence. More generally, our results add evidence for a shared MP-representation of action and perception. Even though the motivation of our work is primarily drawn from neuroscience, we expect that our results will be applicable in virtual and augmented reality settings, when perceptually plausible human avatar movements are required. © 2019 Copyright held by the owner/author(s).",45,Virtual reality,Augmented reality - Bayesian networks - Dynamical systems - Forecasting - Sensory perception,Approximate Bayesian - Dynamical model - Human animation - Ideal observer models - Movement primitives - Prediction mechanisms - Psychophysics - Virtual and augmented reality,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 921.4 Combinatorial Mathematics, Includes Graph Theory, Set Theory",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Sensorless Hand Guidance Using Microsoft Hololens,"Puljiz, D. (1); Stohr, E. (1); Riesterer, K.S. (1); Hein, B. (1); Kroger, T. (1) ","(1) Karlsruhe Inst. of Technol., Karlsruhe, Germany ",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE Robot. & Autom. Soc.,"IEEE, Piscataway, NJ, USA",,632-3,2019,,,,,,978-1-5386-8555-6,10.1109/HRI.2019.8673145,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,"Daegu, South Korea","Hand guidance of robots has proven to be a useful tool both for programming trajectories and in kinesthetic teaching. However hand guidance is usually relegated to robots possessing joint-torque sensors (JTS). Here we propose to extend hand guidance to robots lacking those sensors through the use of an Augmented Reality (AR) device, namely Microsoft's Hololens. Augmented reality devices have been envisioned as a helpful addition to ease both robot programming and increase situational awareness of humans working in close proximity to robots. We reference the robot by using a registration algorithm to match a robot model to the spatial mesh. The in-built hand tracking capabilities are then used to calculate the position of the hands relative to the robot. By decomposing the hand movements into orthogonal rotations we achieve a completely sensorless hand guidance without any need to build a dynamic model of the robot itself. We did the first tests our approach on a commonly used industrial manipulator, the KUKA KR-5.",5,,augmented reality - control engineering computing - educational robots - manipulator dynamics - mobile robots - robot programming,Microsoft Hololens - joint-torque sensors - robot programming - robot model - hand tracking capabilities - hand movements - augmented reality device - sensorless hand guidance - programming trajectories - kinesthetic teaching,C0110 Control education and training - C6110 Systems analysis and programming - C7810C Computer-aided instruction - C3390C Mobile robots - C3390M Manipulators - C6130V Virtual reality - C7420 Control engineering computing - E0250 Education and training - E2230 Robot and manipulator mechanics,B25J - G05B15/00 - G05D1/00 - G06F9/44 - G09B5/00,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Creating a Shared Reality with Robots,"Muhammad, F. (1); Hassan, A. (1); Cleaver, A. (1); Sinapov, J. (1) ","(1) Dept. of Comput. Sci., Tufts Univ., Medford, MA, United States ",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE Robot. & Autom. Soc.,"IEEE, Piscataway, NJ, USA",,614-15,2019,,,,,,978-1-5386-8555-6,10.1109/HRI.2019.8673191,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,"Daegu, South Korea","This paper outlines the system design, capabilities and potential applications of an Augmented Reality (AR) framework developed for Robot Operating System (ROS) powered robots. The goal of this framework is to enable high-level human-robot collaboration and interaction. It allows the users to visualize the robot's state in intuitive modalities overlaid onto the real world and interact with AR objects as a means of communication with the robot. Thereby creating a shared environment in which humans and robots can interact and collaborate.",5,,augmented reality - control engineering computing - human-robot interaction - operating systems (computers),system design - high-level human-robot collaboration - shared environment - augmented reality framework - robot operating system powered robots - shared reality - human-robot interaction,C3390 Robotics - C6130V Virtual reality - C6150J Operating systems - C7420 Control engineering computing - C6180R Human-robot interaction,B25J13/00 - G05B15/00 - G06F9/46,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
On-road Evaluation of Autonomous Driving Training,"Sportillo, D. (1); Paljic, A. (1); Ojeda, L. (2) ","(1) Center for Robot., PSL Res. Univ., Paris, France (2) Groupe PSA, Velizy-Villacoublay, France ",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE Robot. & Autom. Soc.,"IEEE, Piscataway, NJ, USA",,182-90,2019,,,,,,978-1-5386-8555-6,10.1109/HRI.2019.8673277,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,"Daegu, South Korea","Driver interaction with increasingly automated vehicles requires prior knowledge of system capabilities, operational know-how to use novel car equipment and responsiveness to unpredictable situations. With the purpose of getting drivers ready for autonomous driving, in a between-subject study sixty inexperienced participants were trained with an on-board video tutorial, an Augmented Reality (AR) program and a Virtual Reality (VR) simulator. To evaluate the transfer of training to real driving scenarios, a test drive on public roads was conducted implementing, for the first time in these conditions, the Wizard of Oz (WoZ) protocol. Results suggest that VR and AR training can foster knowledge acquisition and improve reaction time performance in take-over requests. Moreover, participants' behavior during the test drive highlights the ecological validity of the experiment thanks to the effective implementation of the WoZ methodology.",36,,augmented reality - computer based training - traffic engineering computing,driving scenarios - public roads - knowledge acquisition - road evaluation - autonomous driving training - driver interaction - car equipment - on-board video tutorial - VR training - between-subject study - virtual reality simulator - augmented reality program - WoZ protocol - Wizard of Oz - AR training,C7810C Computer-aided instruction - C6130V Virtual reality - C7445 Traffic engineering computing,G09B5/00,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Mixed Reality Deictic Gesture for Multi-Modal Robot Communication,"Williams, T. (1); Bussing, M. (1); Cabrol, S. (1); Boyle, E. (1); Tran, N. (1) ","(1) MIRRORLab, Colorado Sch. of Mines, Golden, CO, United States ",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE Robot. & Autom. Soc.,"IEEE, Piscataway, NJ, USA",,191-201,2019,,,,,,978-1-5386-8555-6,10.1109/HRI.2019.8673275,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,"Daegu, South Korea","In previous work, researchers have repeatedly demonstrated that robots' use of deictic gestures enables effective and natural human-robot interaction. However, new technologies such as augmented reality head mounted displays enable environments in which mixed-reality becomes possible, and in such environments, physical gestures become but one category among many different types of mixed reality deictic gestures. In this paper, we present the first experimental exploration of the effectiveness of mixed reality deictic gestures beyond physical gestures. Specifically, we investigate human perception of videos simulating the display of allocentric gestures, in which robots circle their targets in users' fields of view. Our results suggest that this is an effective communication strategy, both in terms of objective accuracy and subjective perception, especially when paired with complex natural language references.",99,,augmented reality - gesture recognition - helmet mounted displays - human-robot interaction,mixed reality deictic gesture - multimodal robot communication - human-robot interaction - physical gestures - allocentric gestures - augmented reality head mounted displays,B6135E Image recognition - C3390 Robotics - C5260B Computer vision and image processing techniques - C6130V Virtual reality,G06K9/00 - G06T,Bibliography (BIB); Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Validating the Accuracy of Imaged-Based Research into the Uncanny Valley: An Experimental Proposal,"Beiboer, J. (1); Sandoval, E.B. (1) ","(1) UNSW Arts & Design, Sydney, NSW, Australia ",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE Robot. & Autom. Soc.,"IEEE, Piscataway, NJ, USA",,608-9,2019,,,,,,978-1-5386-8555-6,10.1109/HRI.2019.8673261,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,"Daegu, South Korea","The uncanny valley phenomenon has be researched for the past 15 years, attempting to prove its validity with limited success. Researchers have been trying to recreate Masahiro Mori's hypothesised function [1] through a variety of experiments using images of real robots and images created from morphing humans and robots. Although some of these experiments have provided results supporting Mori's hypothesis, there is no solid confirmation of their legitimacy when it comes to real human-robot interaction. This paper examines the methods and subsequent results of studies to draw conclusions regarding the validity of experimental data into Mori's hypothesis. These conclusions lead us to propose an Augmented Reality(AR) experiment designed to verify the results of previous experimentation.",17,,augmented reality - human-robot interaction - image processing,human-robot interaction - imaged-based research - experimental proposal - uncanny valley phenomenon - Masahiro Mori's hypothesised function - robots - augmented reality,"B6135 Optical, image and video signal processing - C3390 Robotics - C5260B Computer vision and image processing techniques - C6130V Virtual reality - C6180 User interfaces",G06T,Practical (PRA); Experimental (EXP),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Distilling the essence of raw video to reduce memory usage and energy at edge devices,"Zhang, Haibo (1); Zhao, Shulin (1); Pattnaik, Ashutosh (1); Kandemir, Mahmut T. (1); Sivasubramaniam, Anand (1); Das, Chita R. (1) ","(1) Pennsylvania State University, United States ","Proceedings of the Annual International Symposium on Microarchitecture, MICRO",AMD; arm; et al.; HUAWEI; IBM; Microsoft,IEEE Computer Society,,p 657-669,12-Oct-19,"MICRO 2019 - 52nd Annual IEEE/ACM International Symposium on Microarchitecture, Proceedings",2019,,10724451,,9.78145E+12,10.1145/3352460.3358298,,"52nd Annual IEEE/ACM International Symposium on Microarchitecture, MICRO 2019","October 12, 2019 - October 16, 2019",,"Video broadcast and streaming are among the most widely used applications for edge devices. Roughly 82% of the mobile internet traffic is made up of video data. This is likely to worsen with the advent of 5G that will open up new opportunities for high resolution videos, virtual and augmented reality-based applications. The raw video data produced and consumed by edge devices is considerably higher than what is transmitted out of them. This leads to huge memory bandwidth and energy requirements from such edge devices. Therefore, optimizing the memory bandwidth and energy consumption needs is imperative for further improvements in energy efficiency of such edge devices. In this paper, we propose two mechanisms for on-the-fly compression and approximation of raw video data that is generated by the image sensors. The first mechanism, MidVB, performs lossless compression of the video frames coming out of the sensors and stores the compressed format into the memory. The second mechanism, Distill, builds on top of MidVB and further reduces memory consumption by approximating the video frame data. On an average, across 20 raw videos, MidVB and Distill are able to reduce the memory bandwidth by 43% and 72%, respectively, over the raw representation. They outperform a well known memory saving mechanism by 7% and 36%, respectively. Furthermore, MidVB and Distill reduce the energy consumption by 40% and 67%, respectively, over the baseline. © 2019 Association for Computing Machinery.",90,Image compression,5G mobile communication systems - Augmented reality - Bandwidth - Computer architecture - Data storage equipment - Edge computing - Energy efficiency - Energy utilization - System-on-chip - Video recording - Video streaming,Energy requirements - High resolution - Lossless compression - Memory bandwidths - Memory consumption - Mobile Internet - Video broadcasts - Virtual and augmented reality,"525.2 Energy Conservation - 525.3 Energy Utilization - 714.2 Semiconductor Devices and Integrated Circuits - 716.1 Information Theory and Signal Processing - 716.4 Television Systems and Equipment - 722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications",,,"Number: 1526750, Acronym: NSF, Sponsor: National Sleep Foundation; ","This research is supported in part by NSF grants #1763681, #1629915, #1629129, #1317560, #1526750, and a DARPA/SRC JUMP grant. We would also like to thank Jack Sampson for his feedback on this paper.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Audio-augmented museum experiences with gaze tracking,"Yang, Jing (1); Chan, Cheuk Yu (1) ","(1) Department of Computer Science, ETH Zurich, Switzerland ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,10.1145/3365610.3368415,3368415,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"In this work, we enrich landscape and genre paintings by spatializing sounds for the drawn objects and scenes, which expands visitors’ perception of the paintings and immerses them in the depicted scenarios. Plus, we personalize such spatial audio perception based on visitors’ viewing behavior by applying gaze tracking. Through a preliminary user study with 14 participants, we observed that the gaze tracking-based audio augmentation helped people better focus on the areas of interest in the paintings, and enhanced their overall viewing experience. © 2019 Copyright held by the owner/author(s).",32,Eye tracking,Augmented reality - Museums,Audio augmented reality - Gaze tracking - Perception-based - Spatial audio - User experience - User study,"402.2 Public Buildings - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The MUSETECH model: a comprehensive evaluation framework for museum technology,"Damala, A. (1); Ruthven, I. (2); Hornecker, E. (3) ","(1) Digital Humanities Dept., Univ. Paris 8, Paris, France (2) Dept. of Comput. & Inf. Sci., Univ. of Strathclyde, Glasgow, United Kingdom (3) Comput. Sci. & Media, Bauhaus Univ., Weimar, Germany ",Journal on Computing and Cultural Heritage,,"ACM, USA","v 12, n 1",7 (22 pp.),Feb. 2019,,,,1556-4673,,,10.1145/3297717,,,,,"Digital technologies are being introduced in museums and other informal learning environments alongside more traditional interpretive and communication media. An increasing number of studies has proved the potential of digitally mediated cultural heritage experiences. However, there is still a lot of controversy as to the advantages and disadvantages of introducing the digital into museum settings, primarily related to the risks and investment in terms of time and human and financial resources required. This work introduces the MUSETECH model, a comprehensive framework for evaluating museum technology before and after its introduction into a museum setting. One of the unique features of our framework is to consider the evaluation of digital technologies from three different perspectives: the cultural heritage professional, cultural heritage institution, and museum visitor. The framework benefited from an extensive review of the current state of the art and from inputs from cultural heritage professionals, designers, and engineers. MUSETECH can be used as a tool for reflection before, during, and after introducing novel digital media resources. The model covers technologies as diverse as mobile museum guides, Augmented and Virtual Reality applications, hands-on museum interactives, edutainment applications, digitally mediated tangible and embodied experiences, or online approaches used for museum education and learning.",,,augmented reality - computer aided instruction - history - humanities - interactive systems - mobile computing - museums,hands-on museum interactives - museum education - MUSETECH model - comprehensive evaluation framework - museum technology - digital technologies - informal learning environments - communication media - digitally mediated cultural heritage experiences - museum setting - financial resources - comprehensive framework - cultural heritage institution - museum visitor - cultural heritage professionals - novel digital media resources - mobile museum guides - traditional interpretive media - augmented reality applications - virtual reality applications,"C7820 Humanities computing - C6190V Mobile, ubiquitous and pervasive computing - C6130V Virtual reality - C7810C Computer-aided instruction",G06F9/44 - G09B5/00,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Fostering Positive Affects in Software Development Environments Using Extended Reality,"Mehra, R. (1); Sharma, V.S. (1); Kaulgud, V. (1); Podder, S. (1) ","(1) Accenture Labs., Bangalore, India ",2019 IEEE/ACM 4th International Workshop on Emotion Awareness in Software Engineering (SEmotion). Proceedings,,"IEEE, Piscataway, NJ, USA",,42-5,2019,,,,,,978-1-7281-2280-9,10.1109/SEmotion.2019.00016,,2019 IEEE/ACM 4th International Workshop on Emotion Awareness in Software Engineering (SEmotion),28-May-19,"Montreal, QC, Canada","Recent research on affects has been aimed at establishing the hypothesis that a developer's productivity has a positive correlation with positive affects, such as her happiness quotient. Moreover, studies show that the software development environment itself has a huge role to play in how a developer feels during her day to day activities. In this paper, we present our early work regarding an Extended Reality based approach that virtually transforms a developer's extended and immediate environment, in a way that induces positive affects. This is aimed at assisting the developer to perform better during development related activities like programming and debugging. Salient features of the approach include a sense of presence, uniqueness and easy customization. As part of our initial research efforts, we showcase a couple of Augmented Reality based prototype implementations capable of augmenting virtual and interactive pets and scenic content in the real world. Finally, we also discuss some important future directions that we are investigating further as part of our research.",26,,augmented reality - software engineering,positive correlation - software development environment - extended reality based approach - positive affects - augmented reality based prototype implementations - augmenting virtual interactive pets,C6110B Software engineering techniques - C6130V Virtual reality,G06F9/44,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Practical urban localization for mobile AR,"Xu, Tiantu (1); Wang, Guohui (2); Lin, Felix Xiaozhu (1) ","(1) Purdue ECE, United States (2) ByteDance ",HotMobile 2020 - Proceedings of the 21st International Workshop on Mobile Computing Systems and Applications,ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 27-32,3-Mar-20,HotMobile 2020 - Proceedings of the 21st International Workshop on Mobile Computing Systems and Applications,2020,,,,9.78145E+12,10.1145/3376897.3377855,,"21st International Workshop on Mobile Computing Systems and Applications, HotMobile 2020","March 3, 2020 - March 4, 2020",,"Emerging mobile apps render AR effects based on the places of interest (POI) that a user is currently in. To obtain the needed POI labels and a smartphone's camera position and orientation, such apps demand inexpensive localization. Yet, existing localization solutions either work poorly in urban areas or require expensive data collection. To this end, we advocate for an inexpensive, practical localization pipeline by integrating commodity vision operators. To instantiate the pipeline, we propose a system with three key designs: the cloud indexes image features as a forest rather than a monolithic tree; smartphones incrementally prefetch image features for on-device matching rather than uploading features to the cloud; smartphones tune the camera positioning algorithm dynamically based on its physical environment. Our preliminary results show that these designs can reduce the cost of image data collection by up to three orders of magnitude, reduce user-perceived delays, and scale to diverse AR resource demands and environments. © 2020 Association for Computing Machinery.",38,Data acquisition,Augmented reality - Cameras - Mobile computing - Pipelines - Smartphones,Camera positions - Image data collection - Mobile augmented reality - Physical environments - Positioning algorithms - Resource demands - Three orders of magnitude - Urban Localization,"619.1 Pipe, Piping and Pipelines - 718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 742.2 Photographic Equipment",,,"Number: 1846102, Acronym: NSF, Sponsor: National Science Foundation; Number: -, Acronym: -, Sponsor: Google; ","The authors from Purdue were supported in part by NSF Award 1846102, 1718702, and a Google Faculty Award.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Learning-based estimation of 6-DoF camera poses from partial observation of large objects for mobile AR,"Lomaliza, Jean-Pierre (1); Park, Hanhoon (1) ","(1) Department of Electronic Engineering, Pukyong National University, Busan, Korea, Republic of ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364718,3364718,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"We propose a method that estimates 6-DoF camera pose from a partially visible large object, by exploiting information of its subparts that are detected using a state-of-the-art convolutional neural network (CNN). The trained CNN outputs two-dimensional bounding boxes around subparts and associated classes. Information from detection is then fed to a deep neural network that regresses to camera's 6-DoF poses. Experimental results show that the proposed method is more robust to occlusions than conventional learning-based methods. © 2019 Copyright held by the owner/author(s).",8,Deep neural networks,Augmented reality - Cameras - Deep learning - Neural networks - Object oriented programming - Virtual reality,Bounding box - Convolutional neural network - Large object - Learning-based methods - Mobile augmented reality - Partial observation - Pose estimation - State of the art,"723 Computer Software, Data Handling and Applications - 723.1 Computer Programming - 742.2 Photographic Equipment",,,"Number: NRF-2018R1D1A1B07045650, Acronym: MOE, Sponsor: Ministry of Education; Number: -, Acronym: NRF, Sponsor: National Research Foundation of Korea; ",This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF-2018R1D1A1B07045650),,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Parallel adaptive frameless rendering with NVIDIA OPTIX,"Hsiao, Chung-Che (1); Watson, Benjamin (1) ","(1) North Carolina State University, United States ","SIGGRAPH Asia 2019 Posters, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,17-Nov-19,"SIGGRAPH Asia 2019 Posters, SA 2019",2019,,,,9.78145E+12,10.1145/3355056.3364569,3364569,"SIGGRAPH Asia 2019 Posters - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"In virtual reality (VR) or augmented reality (AR) systems, latency is one of the most important causes of simulator sickness. Latency is difficult to limit in traditional renderers, which sample time rigidly with a series of frames, each representing a single moment in time, depicted with a fixed amount of latency. Previous researchers proposed adaptive frameless rendering (AFR), which removes frames to sample space and time flexibly, and reduce latency. However, their prototype was neither parallel nor interactive. We implement AFR in NVIDIA OptiX, a concurrent, real–time ray tracing API taking advantage of NVIDIA GPUs, including their latest RTX ray tracing components. With proper tuning, our prototype prioritizes temporal detail when scenes are dynamic (producing rapidly updated, blurry imagery), and spatial detail when scenes are static (producing more slowly updated, sharp imagery). The result is parallel, interactive, low-latency imagery that should reduce simulator sickness. © 2019 Copyright held by the owner/author(s).",6,Rendering (computer graphics),Augmented reality - Diseases - Interactive computer graphics - Program processors - Ray tracing - Virtual reality,Adaptive frameless rendering - Augmented reality systems - Low latency - NVIDIA OptiX - Sample space - Simulator sickness - Tracing components,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 741.1 Light/Optics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A field study to collect expert knowledge for the development of AR HuD navigation concepts,"Schneider, Matthias (1); Schluesener, Tim (2); Bruder, Anna (2); Henze, Niels (3); Necker, Marc (2); Wolff, Christian (3) ","(1) Media Informatics Group, University of Regensburg, GER Daimler AG, Stuttgart, Germany (2) Daimler AG, Stuttgart, Germany (3) Media Informatics Group, University of Regensburg, Germany ","Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",cerence; Helmholtz Instituut; here; Rijkswaterstaat - Ministry of Infrastructure and Water Management; Uber ARG; Utrecht University - Faculty of Social and Behavioral Sciences,"Association for Computing Machinery, Inc",,p 358-362,21-Sep-19,"Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",2019,,,,9.78145E+12,10.1145/3349263.3351339,,"11th ACM International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019","September 21, 2019 - September 25, 2019",,"A promising advancement of conventional head-up displays in vehicles is the implementation of augmented reality. By projecting the content onto the vehicle’s windshield, information can be displayed in a contact analogue way in the real world. Two major challenges for concept developers are to reduce masking caused by augmented reality content and to create concepts that are suitable for the limited field of view. To approach these challenges, we designed two contact analogue navigation concepts and evaluated them in a field study with a prototype car that contained a complete AR HUD testing environment. The subjects were experts in interaction design, AR, HUD and sales. First results of the experts’ suggestions for improvements are given in this extended abstract. © 2019 Copyright is held by the owner/author(s).",11,Head-up displays,Air navigation - Augmented reality - Navigation - Speech intelligibility - User interfaces,Augmented reality content - Expert knowledge - Extended abstracts - Field of views - Field studies - Interaction design - Real-world - Testing environment,"431.5 Air Navigation and Traffic Control - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 751.5 Speech",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Lookingood: Enhancing performance capture with real-time neural re-rendering,"Martin-Brualla, Ricardo (1); Pandey, Rohit (1); Yang, Shuoran (1); Pid-Lypenskyi, Pavel (1); Taylor, Jonathan (1); Valentin, Julien (1); Khamis, Sameh (1); Davidson, Philip (1); Tkach, Anastasia (1); Lincoln, Peter (1); Kowdle, Adarsh (1); Rhemann, Christoph (1); Goldman, Dan B. (1); Keskin, Cem (1); Seitz, Steve (1); Izadi, Shahram (1); Fanello, Sean (1) ","(1) Google Inc., United States ","SIGGRAPH Asia 2018 Technical Papers, SIGGRAPH Asia 2018",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,4-Dec-18,"SIGGRAPH Asia 2018 Technical Papers, SIGGRAPH Asia 2018",2019,,,,9.78145E+12,10.1145/3272127.3275099,255,"SIGGRAPH Asia 2018 Technical Papers - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH Asia 2018","December 4, 2018 - December 7, 2018",,"Motivated by augmented and virtual reality applications such as telepresence, there has been a recent focus in real-time performance capture of humans under motion. However, given the real-time constraint, these systems often suffer from artifacts in geometry and texture such as holes and noise in the final rendering, poor lighting, and low-resolution textures. We take the novel approach to augment such real-time performance capture systems with a deep architecture that takes a rendering from an arbitrary viewpoint, and jointly performs completion, super resolution, and denoising of the imagery in real-time. We call this approach neural (re-)rendering, and our live system 'LookinGood'. Our deep architecture is trained to produce high resolution and high quality images from a coarse rendering in real-time. First, we propose a self-supervised training method that does not require manual ground-truth annotation. We contribute a specialized reconstruction error that uses semantic information to focus on relevant parts of the subject, e.g. the face. We also introduce a salient reweighing scheme of the loss function that is able to discard outliers. We specifically design the system for virtual and augmented reality headsets where the consistency between the left and right eye plays a crucial role in the final user experience. Finally, we generate temporally stable results by explicitly minimizing the difference between two consecutive frames. We tested the proposed system in two different scenarios: one involving a single RGB-D sensor, and upper body reconstruction of an actor, the second consisting of full body 360 capture. Through extensive experimentation, we demonstrate how our system generalizes across unseen sequences and subjects. The supplementary video is available at http://youtu.be/Md3tdAKoLGU. © 2018 Copyright held by the owner/author(s).",74,Rendering (computer graphics),Augmented reality - Image denoising - Image enhancement - Interactive computer graphics - Optical resolving power - Real time systems - Semantics - Textures - Virtual reality - Visual communication,Augmented and virtual realities - Low-resolution textures - Real time constraints - Real time performance - Reconstruction error - Semantic information - Super resolution - Virtual and augmented reality,"717.1 Optical Communication Systems - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 741.1 Light/Optics",,,,"We thank Jason Lawrence, Harris Nover, and Supreeth Achar for continuous feedback and support regarding this work.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Towards augmenting IVR communication with physiological sensing data,"George, Ceenu (1); Hassib, Mariam (2) ","(1) LMU, Munich, Germany (2) Bundeswehr University of Munich, LMU, Munich, Germany ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3313082,3313082,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Immersive Virtual Reality (IVR) does not afford social cues for communication, such as sweaty palms to indicate stress, as users can only see an avatar of their collaborator. Prior work has shown that this data is necessary for successful collaboration, which is why we propose to augment IVR communication by (1) real-time capturing of physiological senses and (2) leveraging the unlimited virtual space to display these. We present the results of a focus group (N=7) and a preliminary study (N=32) that investigate how this data may be visualized in a playful interaction and the effects they have on the performances of the collaborators. © 2019 Copyright held by the owner/author(s).",17,Physiology,Augmented reality - Helmet mounted displays - Human engineering - Virtual reality,Collaboration - Focus groups - Immersive virtual reality - Physiological sensing - Physiological sensors - Playful interactions - Virtual and augmented reality - Virtual spaces,"461.4 Ergonomics and Human Factors Engineering - 461.9 Biology - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Should I interrupt or not? Understanding interruptions in head-mounted display settings,"George, Ceenu (1); Janssen, Philipp (1); Heuss, David (1); Alt, Florian (2) ","(1) LMU Munich, Germany (2) Bundeswehr University Munich, Germany ",DIS 2019 - Proceedings of the 2019 ACM Designing Interactive Systems Conference,Adobe; Google; Sketch; UC San Diego's Design Lab; Virginia Tech,"Association for Computing Machinery, Inc",,p 497-510,18-Jun-19,DIS 2019 - Proceedings of the 2019 ACM Designing Interactive Systems Conference,2019,,,,9.78145E+12,10.1145/3322276.3322363,,"2019 ACM Conference on Designing Interactive Systems, DIS 2019","June 23, 2019 - June 28, 2019",,"Head-mounted displays (HMDs) are being used for VR and AR applications and increasingly permeate our everyday life. At the same time, a detailed understanding of interruptions in settings where people wearing an HMD (HMD user) and people not wearing an HMD (bystander) is missing. We investigate (a) whether bystanders are capable of identifying when HMD users switch tasks by observing their gestures, and hence exploit opportune moments for interruptions, and (b) which strategies bystanders employ. In a lab study (N=64) we found that bystanders are able to successfully identify both task switches (83%) and tasks (77%) within only a few seconds of the task switch. Furthermore, we identified interruption strategies of bystanders. From our results we derive implications meant to support designers and practitioners in building HMD applications that are used in a co-located collaborative setting. © 2019 Copyright held by the owner/author(s).",82,Helmet mounted displays,Augmented reality - Street traffic control - Wear of materials,AR application - Co-located - Collaborative settings - Gesture - Head mounted displays - In-buildings - Interruption - Virtual and augmented reality,"406.2 Roads and Streets - 723 Computer Software, Data Handling and Applications - 951 Materials Science",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Extended Reality in Global Software Delivery - Towards a Common Fabric of Understanding and Insights,"Sharma, V.S. (1); Mehra, R. (1); Kaulgud, V. (1); Podder, S. (1) ","(1) Accenture Labs., Bangalore, India ",2019 ACM/IEEE 14th International Conference on Global Software Engineering (ICGSE),,"IEEE, Piscataway, NJ, USA",,80-1,2019,,,,,,978-1-5386-9196-0,10.1109/ICGSE.2019.00029,,2019 ACM/IEEE 14th International Conference on Global Software Engineering (ICGSE),25-26 May 2019,"Montreal, QC, Canada","Large IT organizations depend on a global software delivery model which involves large teams with various roles and stakeholders. As software delivery is inherently collaborative, it requires the different roles to share artifacts, knowledge and insights with each other throughout the delivery life-cycle. Typically, each role has different insight and understanding needs, based on her/his context and activities. An architect may need to understand the system from a modularity perspective, a developer from code quality purposes, whereas a tester would need to understand it from a user-triggered flow perspective. However, the way we represent, understand, and collaborate on software artefacts, is still limited by the confines of traditional 2D computer screens. In this paper, we present our early work of an Extended Reality (XR) based approach that leverages affordances of natural human perception to represent and visualize software applications in three dimensions. This immersive approach is aimed at becoming a common fabric across global delivery roles, making activities like application comprehension, architecture analysis, knowledge communication, and analysis of a software's dynamic aspects, more contextual, richer, and intuitive. We posit that the use of XR, specifically augmented/mixed reality-based multidimensional views of different software artefacts, can be adapted to be an intuitive bridge across different roles, and foster a novel way of collaboration (both locally and globally). Here, we present our immersive approach and its prototype implementation, along with examples of its usage by different project roles. We also discuss some early feedback and the way forward.",6,,augmented reality - groupware - program visualisation,code quality purposes - user-triggered flow perspective - XR - natural human perception - knowledge communication - global software delivery model - delivery life-cycle - 2D computer screens - extended reality based approach - software artefacts - application comprehension - architecture analysis - mixed reality-based multidimensional views - augmented reality-based multidimensional views,C6110B Software engineering techniques - C6130G Groupware - C6130V Virtual reality - C6110V Visual programming,G06F9/44,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
The Reality-Virtuality Interaction Cube: A Framework for Conceptualizing Mixed-Reality Interaction Design Elements for HRI,"Williams, T. (1); Szafir, D. (2); Chakraborti, T. (3) ","(1) Colorado Sch. of Mines, Golden, CO, United States (2) Univ. of Colorado Boulder, Boulder, CO, United States (3) IBM Res. AI, Cambridge, MA, United States ",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE Robot. & Autom. Soc.,"IEEE, Piscataway, NJ, USA",,520-1,2019,,,,,,978-1-5386-8555-6,10.1109/HRI.2019.8673071,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,"Daegu, South Korea","There has recently been an explosion of work in the human-robot interaction (HRI) community on the use of mixed, augmented, and virtual reality. We present a novel conceptual framework to characterize and cluster work in this new area and identify gaps for future research. We begin by introducing the Plane of Interaction: a framework for characterizing interactive technologies in a 2D space informed by the Model-View-Controller design pattern. We then describe how Interactive Design Elements that contribute to the interactivity of a technology can be characterized within this space and present a taxonomy of mixed-reality interactive design elements. We then discuss how these elements may be rendered onto both reality- and virtuality-based environments using a variety of hardware devices and introduce the Reality-Virtuality Interaction Cube: a three-dimensional continuum representing the design space of interactive technologies formed by combining the Plane of Interaction with the Reality-Virtuality Continuum. Finally, we demonstrate the feasibility and utility of this framework by clustering and analyzing the set of papers presented at the 2018 VAM-HRI workshop.",3,,augmented reality - human-robot interaction,virtual reality - interactive technologies - mixed-reality interactive design elements - design space - HRI - human-robot interaction community - mixed reality - model-view-controller design pattern - reality-virtuality continuum - augmented reality - mixed-reality interaction design elements - reality-virtuality interaction cube - plane of interaction,C3390 Robotics - C6180R Human-robot interaction - C6130V Virtual reality,B25J13/00,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Security and privacy approaches in mixed reality: A literature survey,"Guzman, Jaybie A.D.E. (1, 3); Thilakarathna, Kanchana (1, 2); Seneviratne, Aruna (1, 3) ","(1) Data 61, CSIRO, 13 Garden Street, Eveleigh; NSW; 2015, Australia (2) School of Computer Science, University of Sydney, Australia (3) School of Electrical Engineering and Telecommunications, University of New South Wales, Australia ",ACM Computing Surveys,,Association for Computing Machinery,"v 52, n 6",,Oct-19,,2019,,3600300,15577341,,10.1145/3359626,110,,,,"Mixed reality (MR) technology development is now gaining momentum due to advances in computer vision, sensor fusion, and realistic display technologies. With most of the research and development focused on delivering the promise of MR, the privacy and security implications of this technology are yet to be thoroughly investigated. This survey article aims to put in to light these risks and to look into the latest security and privacy work on MR. Specifically, we list and review the different protection approaches that have been proposed to ensure user and data security and privacy in MR. We extend the scope to include work on related technologies such as augmented reality, virtual reality, and human-computer interaction as crucial components, if not the origins, of MR, as well as numerous related work from the larger area of mobile devices, wearables, and Internet-of-Things. We highlight the lack of investigation, implementation, and evaluation of data protection approaches in MR. Further challenges and directions on MR security and privacy are also discussed. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",145,Mixed reality,Augmented reality - Data privacy - Human computer interaction - Security of data - Surveys - Wearable technology,Data security and privacy - Display technologies - Literature survey - Mixed reality technologies - Privacy and security - Research and development - Security - Security and privacy,"723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,"Number: -, Acronym: -, Sponsor: Research and Development; ","J. A. de Guzman is pursuing his PhD with a scholarship from the Philippine government through its Engineering Research and Development for Technology (ERDT) program. Authors&rsquo; addresses: J. A. de Guzman, Data 61, 13 Garden Street Eveleigh, NSW, 2015, Australia; email: j.deguzman@ student.unsw.edu.au; K. Thilakarathna, School of Computer Science, University of Sydney, Australia; email: kanchana. thilakarathna@sydney.edu.au; A. Seneviratne, School of Electrical Engineering and Telecommunications, University of New South Wales; email: a.seneviratne@unsw.edu.au. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. &copy; 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. 0360-0300/2019/10-ART110 $15.00 https://doi.org/10.1145/3359626",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An enriched visit to the Botanical Garden: Co-designing tools and contents,"Bettelli, Alice (1); Orso, Valeria (2); Pluchino, Patrik (2); Gamberini, Luciano (2) ","(1) Human Inspired Technology Research Centre, University of Padova, Padova, Italy (2) Human Inspired Technology Research Centre, Department of General Psychology, University of Padova, Padova, Italy ",ACM International Conference Proceeding Series,"Dipartimento dei Beni Culturali: Archeologia, Storia dell'Arte, del Cinema e della Musica; Dipartimento di Psicologia Generali (DPG); Human Inspired Technology Research Centre (HIT); Universita Ca'Foscari Venezia, Dipartimento di Scienze Ambientali, Informatica e Statistica; Universita degli Studi di Padova",Association for Computing Machinery,,,23-Sep-19,CHItaly 2019 - Proceedings of the 13th Biannual Conference of the Italian SIGCHI Chapter Designing the Next Interaction,2019,,,,9.78145E+12,10.1145/3351995.3352034,a1,"13th Biannual Conference of the Italian SIGCHI Chapter Designing the Next Interaction, CHItaly 2019","September 23, 2019 - September 25, 2019",,"The exploitation of Virtual and Augmented Reality in the context of cultural heritage and tourism is increasing. These new technologies enable to integrate the traditional cultural contents and innovative experience modalities. Several cultural sites provide visitors with such systems to create an immersive visiting experience. However, limited attention has been paid so far to the exploitation of such technologies in naturalistic cultural sites. This paper describes the project 'This is (not) just a tree', an applied research project aiming to develop an advanced system exploiting Virtual and Augmented Reality technology, which will enable visitors to enjoy closer experiences with plants and their interactions with animals. Special attention will be devoted to involving both stakeholders and target users in the development, thereby applying a co-design approach. Here we report on the participatory design method deployed to gather and organize requirements from stakeholders, the strategies implemented to inquire target users on their preferences, and the resulting implications for the application design. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",25,Augmented reality,User centered design - Virtual reality,Application design - Botanical gardens - Co-design approach - Co-designs - Cultural heritages - Limited attentions - Participatory design - Virtual and augmented reality,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Super size hero: An immersive VR experience,"Sander-Titgemeyer, Till (1); Chen, Jiayan (1); Schauer, Ramon (1); Bertsch, Mario (1); Selg, Sebastian (1); Von Sydow, York (1); Nomura, Verena (1); Al-Azzam, Ihab (1) ","(1) Filmakademie Baden-Württemberg/Animationsinstitut, Ludwigsburg, Germany ","SIGGRAPH Asia 2019 XR, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 40-41,17-Nov-19,"SIGGRAPH Asia 2019 XR, SA 2019",2019,,,,9.78145E+12,10.1145/3355355.3361876,,"SIGGRAPH Asia 2019 XR - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"Supersizehero is an immerse VR game for HTC Vive which puts the player in the role of an overweight hero trying to save the day. A special crafted, tracked fat suit allowing the player to actively use his belly serves as the main gameplay mechanic. The game is highscore based - each round the player needs to prevent a prison breakout or bank robbing by bouncing fleeing prisoners back into the prison, interrupt bank robbers and bring money back to the bank in order to gain as much points as possible in the given round. At the start of every level the player can choose one of three suits - each grants special abilities and a unique playstyle. © 2019 Association for Computing Machinery.",,Interactive computer graphics,Augmented reality - Prisons,3D interactions - Gameplay - Immersive VR - Virtual and augmented reality,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Resolving Interference in Time of Flight Range Sensors via Sparse Recovery Algorithm,"Patil, Swati S. (1); Inamdar, Vandana S. (2) ","(1) Department of Electronics and Telecommunication, College of Engineering Pune, Pune University, India (2) Department of Computer Engineering and Information Technology, College of Engineering Pune, Pune University, India ",ACM International Conference Proceeding Series,Nanyang Technological University; University of Bologna (UNIBO),Association for Computing Machinery,,p 106-112,8-Feb-20,ICIGP 2020 - Proceedings of the 2020 3rd International Conference on Image and Graphics Processing,2020,,,,9.78145E+12,10.1145/3383812.3383831,,"3rd International Conference on Image and Graphics Processing, ICIGP 2020","February 8, 2020 - February 10, 2020",,"A popular imaging technique called Time of Flight (ToF) camera provides a depth information in real time. Several applications like augmented reality (AR) applications, machine automation in factories, hand gesture controls, in military for autonomous navigation and object localization in robotics, ego motion estimation etc. are good source of ToF depth camera. These applications demand good accuracy to provide required services. ToF fulfill this demand of accuracy, however, faces problem like multipath interference (MPI). The MPI phenomena hampers the accuracy of depth map recovery and it can be up to several centimeters. In this paper, we solved the MPI problem by exploiting the sparsity of the received signal. We proposed an approach of sparse regularization technique based on compressed sensing framework with some modification such as applying positive and proximity constraint. This sparse recovery algorithm applied is robust for the MPI with two path. We demonstrate and validate the simulation results of our proposed algorithm for MPI removal. © 2020 ACM.",16,Military applications,Augmented reality - Cameras - Imaging techniques - Motion estimation - Recovery - Robots,Autonomous navigation - Ego-motion estimation - Hand gesture control - Machine automations - Multipath interferences - Object localization - Sparse regularizations - Time of flight (ToF) cameras,"404.1 Military Engineering - 723 Computer Software, Data Handling and Applications - 731.5 Robotics - 742.2 Photographic Equipment - 746 Imaging Techniques",,,,We thanks to Dr. Babasaheb Ambedkar Research and Training Institute (BARTI) for funding the project. We also thanks IFM Electronic India Private Limited to provide a visual sensor simulator for our research work.,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
User experience problems in immersive virtual environments,"Forte, Juan Luis Berenguel (1); Vela, Francisco Luis Gutiérrez (1); Rodríguez, Patricia Paderewski (1) ","(1) University of Granada, Granada, Spain ",ACM International Conference Proceeding Series,"Asociacion para la Interaccion Persona - Ordenador (AIPO); CHISpa, Spanish chapter of ACM SIGCHI; Eusko Jaurlaritza - Gobierno Vasco; Fundacion ONCE; Universidad del Pas Vasco - Euskal Herriko Unibertsitatea",Association for Computing Machinery,,,25-Jun-19,"Proceedings of the 22nd International Conference on Human-Computer Interaction, INTERACCION 2019",2019,,,,9.78145E+12,10.1145/3335595.3336288,18,"22nd International Conference on Human-Computer Interaction, INTERACCION 2019","June 25, 2019 - June 28, 2019",,"In recent years there has been a great boom in the use of immersive virtual environments applications, but research into interaction techniques for these technologies has not had the same growth. Therefore, it is necessary to study the user experience of the different forms of interaction that these technologies offer to users and give developers the information needed to use the techniques that best suit their applications and their users. This work is an exploratory study to detect the problems that the users find in this kind of applications. We aim to make an evaluation of a determined set of interaction techniques, both in virtual and augmented reality, attending to the problems produced in the users to know the strengths and weaknesses of each type of interaction technique. With this study, we will be able to make a guide for the developers, in order to give them clues about the best interaction technique for their applications. © 2019 ACM.",23,Human computer interaction,Augmented reality - Virtual reality,Evaluation - Immersive virtual environments - Interaction techniques - Usability - User experience,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An AR puzzle application for improving emotion recognition for as children,"Daniel, Vicente Lopez Trompo (1); Ting, Han (1); Photchara, Ratsamee (2); Haruo, Takemura (2) ","(1) Shanghai Jiaotong University, Shanghai, China (2) Osaka University, Osaka, Japan ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 52-56,25-Oct-19,ICDTE 2019 - 2019 the 3rd International Conference on Digital Technology in Education,2019,,,,9.78145E+12,10.1145/3369199.3369212,,"3rd International Conference on Digital Technology in Education, ICDTE 2019","October 25, 2019 - October 27, 2019",,"Affecting to around 1% of the population, Autism is sometimes described as a different approach to interacting with the world. Adapting the surrounding objects and systems can improve their experience and their relative's. This project is based on previous research where it has been shown that toys can influence positively in a child's development. Also, new technologies as Augmented Reality (AR) can be beneficial for these children in attracting and keeping their attention. The proposed game would engage the player by first creating a customized monster with the help of different AR markers. In a second stage, the player would try to guess the emotion of different monsters or virtual humans. The game will be tested in further stages to check its suitability for the AS children and the effect on their emotion recognition skills. © 2019 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM",30,E-learning,Augmented reality - Diseases - Educational technology - Speech recognition - Virtual reality,Autism - Child's development - Educational game - Emotion recognition - Virtual humans,"723 Computer Software, Data Handling and Applications - 751.5 Speech - 901.2 Education",,,"Number: 18BRK009, Acronym: NSSFC, Sponsor: National Office for Philosophy and Social Sciences; Number: -, Acronym: MOE, Sponsor: Ministry of Education of the People's Republic of China; ","This research is supported by the National Social Science Fund (Grant No. 18BRK009) of the Ministry of Education of China. This project has received help from Nattaon Techasarntikul, PhD candidate of Osaka University, giving assistance during the app development in Unity.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmenting good behaviour: Mixing digital and reality to promote sustainability in a campus community,"Prandi, Catia (1); Ceccarini, Chiara (1); Salomoni, Paola (1) ","(1) University of Bologna, Bologna, Italy ",ACM International Conference Proceeding Series,The European Alliance for Innovation (EAI),Association for Computing Machinery,,p 189-194,25-Sep-19,"Proceedings of the 5th EAI International Conference on Smart Objects and Technologies for Social Good, GOODTECHS 2019",2019,,,,9.78145E+12,10.1145/3342428.3342688,,"5th EAI International Conference on Smart Objects and Technologies for Social Good, GOODTECHS 2019","September 25, 2019 - September 27, 2019",,"In this paper, we present a case study inspired by sustainable initiatives carried out by the University of Bologna, such as decreasing the amount of paper used in favor of the digital form, and planting new trees in two Campus green areas to make sustainability tangible to communities. In this scenario, we envisioned the possibility to investigate how to increase awareness of sustainability issues in a specific community who is not directly involved in the decision process, with the final aim to convey sustainable behaviour. To accomplish this objective, we designed and developed an intervention mixing digital and reality experiences exploiting i) web-based technologies to create an interactive infographic, ii) mobile-based technologies to provide augmented reality (AR) features), iii) a sensors infrastructure to gather data about environmental conditions, and iv) an in-situ public installation to visualize the data collected by the sensors. © 2019 ACM.",40,Sustainable development,Augmented reality - Environmental technology - Mixing,Community awareness - Decision process - Digital forms - Environmental conditions - Green areas - Sustainability issues - Sustainable behaviours - Web-based technologies,"454 Environmental Engineering - 723 Computer Software, Data Handling and Applications - 802.3 Chemical Operations",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Potential of AR for the analysis and training of spatial skills: A case study,"Dohse, Friedemann (1); Nicolaisen, Vera (1); Wetzel, Stefanie (1); Bertel, Sven (1) ","(1) CIVU – Center for Interaction, Visualization and Usability, Hochschule Flensburg Kanzleistraße 91-93, Flensburg; 24943, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 537-541,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,German,,,9.78145E+12,10.1145/3340764.3344453,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"Good spatial skills are of particular importance in STEM domains. For the training of spatial skills, smartphones provide a promising platform because of their high dissemination. Additionally, new frameworks make it easy to develop and use augmented reality (AR) contents. To examine the potential of AR for the training of spatial skills, we developed a prototype of a smartphone app for the solving of cross section tasks.The prototype was tested in a user study with 32 university students. It was compared against a second prototype with a classical Arcball interaction. For both prototypes, success rates were equally high. However, 3D-objects could be rotated faster using the AR-app. For the AR-app, participants reported higher scores regarding innovation and motivation but also for physical demands. This is especially interesting because motor activity and spatial skills are highly connected. Therefore, the physical approach might be promising for the development of spatial skill training apps which might lead to good long-term training results. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",15,Students,Augmented reality - Smartphones,3D object - Motor activity - Physical approaches - Physical demand - Spatial skills - University students - User study,"718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"NextMed, Augmented and Virtual Reality platform for 3D medical imaging visualization: Explanation of the software platform developed for 3D models visualization related with medical images using Augmented and Virtual Reality technology","Izard, Santiago González (1); Plaza, Scar Alonso (2); Torres, Ramiro Sánchez (2); Méndez, Juan Antonio Juanes (3); García-Palvo, Francisco José (2) ","(1) University of Salamanca, ARSOFT, Salamanca, Spain (2) University of Salamanca, Salamanca, Spain (3) Department of Anatomy, University of Salamanca, Salamanca, Spain ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 459-468,16-Oct-19,Proceedings - TEEM 2019: 7th International Conference on Technological Ecosystems for Enhancing Multiculturality,2019,,,,9.78145E+12,10.1145/3362789.3362936,,"7th International Conference on Technological Ecosystems for Enhancing Multiculturality, TEEM 2019","October 16, 2019 - October 18, 2019",,"The visualization of the radiological results with more advanced techniques than the current ones, such as Augmented Reality and Virtual Reality technologies, represent a great advance for medical professionals, by eliminating their imagination capacity as an indispensable requirement for the understanding of medical images. The problem is that for its application it is necessary to segment the anatomical areas of interest, and this currently involves the intervention of the human being. The Nextmed project is presented as a complete solution that includes DICOM images import, automatic segmentation of certain anatomical structures, 3D mesh generation of the segmented area, visualization engine with Augmented Reality and Virtual Reality, all thanks to different software platforms that have been implemented and detailed, including results obtained from real patients. We will focus on the visualization platform using both Augmented and Virtual Reality technologies to allow medical professionals to work with 3d model representation of medical images in a different way taking advantage of new technologies. © 2019 ACM.",35,Medical imaging,3D modeling - Augmented reality - Ecosystems - Image segmentation - Mesh generation - Three dimensional computer graphics - Virtual reality - Visualization,Anatomical structures - Augmented and virtual realities - Automatic segmentations - Medical imaging visualization - Medical professionals - Virtual reality technology - Visualization engine - Visualization platforms,"454.3 Ecology and Ecosystems - 723 Computer Software, Data Handling and Applications - 746 Imaging Techniques",,,"Number: RTC-2017-6682-1, Acronym: EU, Sponsor: European Commission; ","The authors would like to specifically thank the members of ARSOFT, company specialized in Virtual and Augmented Reality systems and located in the Science Park of the University of Salamanca. This research work has been carried out within Education in Knowledge Society PhD Programme of the University of Salamanca [35,36]. This project and researches have been funded by the Challenges-Collaboration program of the European Union, with file number RTC-2017-6682-1.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Immersive search: Interactive information retrieval in three-dimensional space,"Ward, Austin R. (1) ","(1) University of North Carolina at Chapel Hill, Chapel Hill; NC, United States ",CHIIR 2020 - Proceedings of the 2020 Conference on Human Information Interaction and Retrieval,ACM Special Interest Group on Information Retrieval (ACM SIGIR),"Association for Computing Machinery, Inc",,p 503-506,14-Mar-20,CHIIR 2020 - Proceedings of the 2020 Conference on Human Information Interaction and Retrieval,2020,,,,9.78145E+12,10.1145/3343413.3377946,,"5th ACM SIGIR Conference on Human Information Interaction and Retrieval, CHIIR 2020","March 14, 2020 - March 18, 2020",,"Researchers in interactive information retrieval (IIR) have studied and refined 2D presentations of search results for years. Recent advances are bringing augmented reality (AR) and virtual reality (VR) to real-world systems, though the IIR community has done relatively little work to explore and understand aspects of 3D presentations of search results, effects of immersive environments, and the impacts of spatial cognition and different spatial arrangements of results displays in 3D. In the research proposed here, I outline my plan to use immerse environments to investigate how users' spatial cognition may influence the information retrieval process. Specifically, this work will observe how spatial arrangements of search results affect users' ability to find information in the post-query, visual search phase of the IIR process across quantitative and qualitative measures. © 2020 ACM.",20,IIR filters,Augmented reality - Information retrieval - Virtual reality,3-D presentations - Immersive - Immersive environment - Interactive information retrieval - Real-world system - Spatial arrangements - Spatial cognition - Three dimensional space,"703.2 Electric Filters - 723 Computer Software, Data Handling and Applications - 903.3 Information Retrieval and Use",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Learning from 3D (Point cloud) data,"Hsu, Winston H. (1) ","(1) National Taiwan University, Taipei, Taiwan ",MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,ACM SIGMM,"Association for Computing Machinery, Inc",,p 2697-2698,15-Oct-19,MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,2019,,,,9.78145E+12,10.1145/3343031.3350540,,"27th ACM International Conference on Multimedia, MM 2019","October 21, 2019 - October 25, 2019",,"Learning on (3D) point clouds is vital for a broad range of emerging applications such as autonomous driving, robot perception, augmented reality, gaming, and security. Such needs have increased recently due to the prevalence of 3D sensors such as LiDAR, 3D camera, and RGB-D. Point clouds consist of thousands to millions of points; They contain rich information and are complementary to the traditional 2D cameras that we have been working on for years in the multimedia (or vision) community. 3D learning algorithms on point cloud data are new, and exciting, for numerous core problems such as 3D classification, detection, semantic segmentation, and face recognition. Covers the requirements of point cloud data, the background of capturing the data, 3D representations, emerging applications, core problems, state-of-the art learning algorithms, and future research opportunities. © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6889-6/19/10.",11,Learning algorithms,Augmented reality - Autonomous vehicles - Cameras - Face recognition - Object detection - Optical radar - Robots - Semantics,3D imaging - Autonomous driving - Point cloud - PointNet - VoxelNet,"716.2 Radar Systems and Equipment - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 731.5 Robotics - 742.2 Photographic Equipment",,,"Number: NAT-410477, Acronym: -, Sponsor: -; Number: MOST 108-2634-F-002-004, Acronym: MOST, Sponsor: Ministry of Science and Technology, Taiwan; Number: -, Acronym: -, Sponsor: Nvidia; Number: AI Supercomputer, Acronym: -, Sponsor: -; ","This work was supported in part by the Ministry of Science and Technology, Taiwan, under Grant MOST 108-2634-F-002-004, FIH Mobile Limited, and Qualcomm Technologies, Inc., under Grant NAT-410477. We also benefit from the NVIDIA grants and the DGX-1 AI Supercomputer.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Privacy-aware eye tracking using differential privacy,"Steil, Julian (1); Hagestedt, Inken (2); Huang, Michael Xuelin (1); Bulling, Andreas (3) ","(1) Max Planck Institute for Informatics, Saarland Informatics Campus, Germany (2) CISPA Helmholtz Center for Information Security, Saarland Informatics Campus, Germany (3) University of Stuttgart, Institute for Visualisation and Interactive Systems, Germany ",Eye Tracking Research and Applications Symposium (ETRA),ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,25-Jun-19,Proceedings - ETRA 2019: 2019 ACM Symposium On Eye Tracking Research and Applications,2019,,,,9.78145E+12,10.1145/3314111.3319915,a27,"11th ACM Symposium on Eye Tracking Research and Applications, ETRA 2019","June 25, 2019 - June 28, 2019",,"With eye tracking being increasingly integrated into virtual and augmented reality (VR/AR) head-mounted displays, preserving users’ privacy is an ever more important, yet under-explored, topic in the eye tracking community. We report a large-scale online survey (N=124) on privacy aspects of eye tracking that provides the first comprehensive account of with whom, for which services, and to what extent users are willing to share their gaze data. Using these insights, we design a privacy-aware VR interface that uses differential privacy, which we evaluate on a new 20-participant dataset for two privacy sensitive tasks: We show that our method can prevent user re-identification and protect gender information while maintaining high performance for gaze-based document type classification. Our results highlight the privacy challenges particular to gaze data and demonstrate that differential privacy is a potential means to address them. Thus, this paper lays important foundations for future research on privacy-aware gaze interfaces. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",43,Eye tracking,Augmented reality - Classification (of information) - Data privacy - Eye movements - Eye protection - Helmet mounted displays - Information retrieval systems - Surveys,Data Sharing - Gaze behaviours - Online surveys - Privacy protection - User Modeling,"723 Computer Software, Data Handling and Applications - 903.1 Information Sources and Analysis - 903.3 Information Retrieval and Use - 914.1 Accidents and Accident Prevention",,,"Number: JPMJCR14E1, Acronym: -, Sponsor: -; Number: -, Acronym: -, Sponsor: Universit&Atilde;&curren;t des Saarlandes; Number: 16KIS0656, Acronym: BMBF, Sponsor: Bundesministerium f&Atilde;&frac14;r Bildung und Forschung; ","This work was funded, in part, by the Cluster of Excellence on Multimodal Computing and Interaction (MMCI) at Saarland University, Germany, by a JST CREST research grant under Grant No.: JPMJCR14E1, Japan, as well as by the German Federal Ministry of Education and Research (BMBF) for the Center for IT-Security, Privacy and Accountability (CISPA) (FKZ: 16KIS0656).",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Privaceye: Privacy-preserving head-mounted eye tracking using egocentric scene image and eye movement features,"Steil, Julian (1); Koelle, Marion (2); Heuten, Wilko (3); Boll, Susanne (2); Bulling, Andreas (4) ","(1) Max Planck Institute for Informatics, Saarland Informatics Campus, Germany (2) University of Oldenburg, Germany (3) OFFIS - Institute for IT, Germany (4) University of Stuttgart, Germany ",Eye Tracking Research and Applications Symposium (ETRA),ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,25-Jun-19,Proceedings - ETRA 2019: 2019 ACM Symposium On Eye Tracking Research and Applications,2019,,,,9.78145E+12,10.1145/3314111.3319913,a26,"11th ACM Symposium on Eye Tracking Research and Applications, ETRA 2019","June 25, 2019 - June 28, 2019",,"Eyewear devices, such as augmented reality displays, increasingly integrate eye tracking, but the first-person camera required to map a user’s gaze to the visual scene can pose a significant threat to user and bystander privacy. We present PrivacEye, a method to detect privacy-sensitive everyday situations and automatically enable and disable the eye tracker’s first-person camera using a mechanical shutter. To close the shutter in privacy-sensitive situations, the method uses a deep representation of the first-person video combined with rich features that encode users’ eye movements. To open the shutter without visual input, PrivacEye detects changes in users’ eye movements alone to gauge changes in the 'privacy level' of the current situation. We evaluate our method on a first-person video dataset recorded in daily life situations of 17 participants, annotated by themselves for privacy sensitivity, and show that our method is effective in preserving privacy in this challenging setting. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",58,Eye tracking,Augmented reality - Cameras - Display devices - Eye movements,Current situation - Daily-life situations - Gaze behaviours - Head-mounted eye tracking - Mechanical shutters - Privacy Levels - Privacy preserving - Privacy protection,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 742.2 Photographic Equipment",,,"Number: JPMJCR14E1, Acronym: -, Sponsor: -; ","This work was funded, in part, by a JST CREST research grant under Grant No.: JPMJCR14E1, Japan.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Demonstrating Vrbox-a virtual reality augmented sandbox,"Alexandrovsky, Dmitry (1); Putze, Susanne (1); Stabbert, Timo (1); DÖring, Tanja (1); FrÖhlich, Thomas (1); Malaka, Rainer (1) ","(1) University of Bremen, Bremen, Germany ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3313251,3313251,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"We present VRBox-an interactive sandbox for playful and immersive terraforming that combines the approach of augmented sandboxes with virtual reality technology and mid-air gestures. Our interactive demonstration offers a virtual reality (VR) environment containing a landscape, which the user designs via interacting with real sand while wearing a VR head-mounted display (HMD). Whereas real sandboxes have been used with augmented reality before, our approach using sand in VR offers novel and original interactive features such as exploring the sand landscape from a first person perspective. In this demo, users can experience our VR-sandbox system consisting of a box with sand, multiple Kinect depth sensing, an HMD, and hand tracking, as well as an interactive world simulation. © 2019 Copyright held by the owner/author(s). ACM",10,Helmet mounted displays,Augmented reality - Human engineering - Sand - Virtual reality,Augmented Sandbox - Gestural interaction - Natural materials - Playful interactions - Tangible interaction,"461.4 Ergonomics and Human Factors Engineering - 483.1 Soils and Soil Mechanics - 723 Computer Software, Data Handling and Applications",,,"Number: 688244, Acronym: -, Sponsor: Horizon 2020; ","This project has received funding from the European Unions' Horizon 2020 research and innovation programm under grant agreement No 688244, project first.stage and by the German Federal Ministry of Education and Research in the grant program Erfahrbares Lernen"" (experienceable learning).""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Enabling multiple applications to simultaneously augment reality: Challenges and directions,"Lebeck, Kiron (1); Kohno, Tadayoshi (1); Roesner, Franziska (1) ","(1) University of Washington, United States ",HotMobile 2019 - Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications,ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 81-86,22-Feb-19,HotMobile 2019 - Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications,2019,,,,9.78145E+12,10.1145/3301293.3302362,3302362,"20th International Workshop on Mobile Computing Systems and Applications, HotMobile 2019","February 27, 2019 - February 28, 2019",,"Augmented reality (AR) platforms are evolving to support immersive 3D experiences. Most modern AR platforms support only a single immersive app at a time, but users may also benefit from the ability to engage with multiple apps at once. The ability of different apps to simultaneously augment a user's world raises critical questions: how might apps visually conflict with each other, and how can we design AR platforms to support rich behaviors while mediating conflicts? In this work, we pose and explore these questions, identifying means of visual conflict as well as platform design strategies to mediate conflicts. We then analyze state-of-the-art AR platforms (HoloLens, Magic Leap One, and Meta 2) to understand their trade-offs and identify unexplored gaps in the broader design space. Our exploration reveals key guidelines and lessons to inform future multi-app AR efforts. © 2019 ACM.",17,Space platforms,Augmented reality - Economic and social effects - Mobile computing,3d experiences - Critical questions - Design spaces - Multiple applications - Platform design strategy - Security - State of the art - Trade off,"655.1 Spacecraft, General - 723 Computer Software, Data Handling and Applications - 971 Social Sciences",,,"Number: -, Acronym: NSF, Sponsor: National Science Foundation; ","We thank Niel Lebeck and Earlence Fernandes for many helpful discussions and feedback on earlier drafts.We also thank our anonymous reviewers and our shepherd, Mahadev Satyanarayanan, for their valuable guidance and feedback. This work was supported in part by the National Science Foundation under Award CNS-1651230.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Heimdall: A case for encrypted displays: Extended Abstract,"Srivastava, Animesh (1) ","(1) Google, United States ",HotMobile 2019 - Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications,ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 165,22-Feb-19,HotMobile 2019 - Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications,2019,,,,9.78145E+12,10.1145/3301293.3309556,3309556,"20th International Workshop on Mobile Computing Systems and Applications, HotMobile 2019","February 27, 2019 - February 28, 2019",,"It can be argued that among all the personal devices, a smartphone carries the most private information of one's life. The improved hardware, computing resource, ever-increasing storage, ubiquitous connectivity, and innovative ways of accessing useful information has slowly and steadily integrated smartphones in our lives. Whether it is interacting on social media, planning for the day using productivity apps or getting things done at work, the smartphone screen is the primary gateway between a user and the world of information accessible through the smartphones. There is always a risk of leaking sensitive information while accessing such information on one's smart-device in a public setting. Visual information can be easily taken advantage of by a malicious bystander. Even if the screen is not comprehensible to the human eye, an off-the-shelf recording device can capture the finer details displayed on the victim's smartphone. A popular way of protecting visual information displayed on smartphones is to use privacy screens [1]. However, privacy screens fail to protect visual privacy when the bystander or the recording device is pointed at it directly. To address this issue, in the poster, we propose, Heimdall, an augmented reality (AR) based system. The primary reason for the failure of privacy screens is that the medium used to broadcast the visual information is shared by the intended user and the bystanders. An AR based solution isolates the medium used to receive the visual information by the intended user from any other receiver. The primary guiding force behind the design of Heimdall are as follows: • On-demand Privacy Mode: The user should be able to interact with her smartphone without the need of the AR equipment. The user can enable privacy mode when sensitive information needs to be accessed and disable otherwise. • Minimal computation on AR device: We do not assume that the AR device will be capable of performing an intensive computation. AR glasses under Heimdall should be able to stream the user's view and replace it with an augmented view. © 2019 ACM.",2,Smartphones,Augmented reality - Mobile computing - Ubiquitous computing,Computing resource - Extended abstracts - Personal devices - Private information - Recording devices - Sensitive informations - Visual information - Visual privacy leaks,"718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
SIGCHI outstanding dissertation award: On-world computing,"Xiao, Robert (1) ","(1) University of British Columbia, Department of Computer Science, Vancouver; BC; V6T 1Z4, Canada ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3313774,3313774,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Computers are now ubiquitous. However, computers and digital content have remained largely separate from the physical world - users explicitly interact with computers through small screens and input devices, and the 'virtual world' of digital content has had very little overlap with the practical, physical world. My thesis work is concerned with helping computing escape the confines of screens and devices, spilling digital content out onto the physical world around us. In this way, I aim to help bridge the gap between the information-rich digital world and the familiar environment of the physical world and allow users to interact with digital content as they would ordinary physical content. I approach this problem from many angles: from the low-level work of providing high-fidelity touch interaction on everyday surfaces, easily transforming these surfaces into enormous touchscreens; to high-level questions surrounding the interaction design between physical and virtual realms. To achieve this end, building on my prior work, I developed two physical embodiments of this new mixed-reality design: a tiny, miniaturized projector and camera system providing the hardware basis for a projected on-world interface, and a head-mounted augmented-reality head-mounted display modified to support touch interaction on arbitrary surfaces. © 2019 Copyright is held by the author/owner. ACM",10,Ubiquitous computing,Augmented reality - Digital devices - Helmet mounted displays - Human computer interaction - Interactive computer graphics - Mixed reality,Arbitrary surfaces - Digital contents - Head mounted displays - Interaction design - On-world computing - Projector-and-camera system - Touch inputs - Touch interaction,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,"Foremost, I want to deeply thank both of my advisors at CMU, Chris Harrison and Scott Hudson, for being incredible sources of inspiration and support throughout my Ph.D. It is only now as a newly-minted professor that I can begin to understand the magnitude of their efforts. Hrvoje Benko, my committee member and unofficial third advisor, deserves very special mention for his patience and mentorship through my stints at Microsoft. I am particularly grateful for his support over the past two years. I'd like to thank the numerous wonderful collaborators and peers that I've worked with, including Andy Wilson, Yang Zhang, Gierad Laput and Julia Schwarz, for being such great folks to work with. Finally, I'd like to thank my family - my parents, sister and girlfriend - for always being there for me.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Multi-party mixed reality interaction for earth sciences education,"Chiou, Yan-Ming (1) ","(1) Computer and Information Sciences, University of Delaware, Newark; DE, United States ","TEI 2019 - Proceedings of the 13th International Conference on Tangible, Embedded, and Embodied Interaction",ACM SIGCHI,"Association for Computing Machinery, Inc",,p 719-722,17-Mar-19,"TEI 2019 - Proceedings of the 13th International Conference on Tangible, Embedded, and Embodied Interaction",2019,,,,9.78145E+12,10.1145/3294109.3302933,,"13th International Conference on Tangible, Embedded, and Embodied Interaction, TEI 2019","March 17, 2019 - March 20, 2019",,"Collaborative learning has been shown to be beneficial for children's learning performance, increasing the curiosity and intensity of the ability of cooperation. Mixed-Reality with collaborative learning is the trending research topic in the Human-Computer Interaction (HCI) area. Additionally, with the rise of attention to global warming which brings in more extreme weather and climate conditions, the earth science education would be one of the crucial topics for the next generation. Moreover, there are few augmented reality and mixed reality applications on earth science subject. In this paper, we propose a Mixed Reality Tornado Simulator which offers an earth science education in a collaborative setting. Students and the instructor can cooperate on learning the knowledge of the formation and its damage cause on human-built structures, farming, and vegetation by using our mixed reality application with the Microsoft HoloLens. Also, for evaluating the learning performance in this mixed reality setting, we propose to study the cognitive load while the student is learning the abstract knowledge in Earth Science. We will separate the student into a control group and experimental groups and use different teaching instruments to test the difference of cognitive load. © 2019 ACM.",14,Mixed reality,Augmented reality - Earth (planet) - Global warming - Human computer interaction - Structures (built objects) - Students,Climate condition - Collaborative learning - Collaborative settings - Experimental groups - Human computer interaction (HCI) - Learning performance - MicroSoft - Science education,"443.1 Atmospheric Properties - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Fire in your hands: Understanding thermal behavior of smartphones,"Kang, Soowon (1); Choi, Hyeonwoo (2); Park, Sooyoung (1); Park, Chunjong (3); Lee, Jemin (4); Lee, Uichin (1); Lee, Sung-Ju (1) ","(1) KAIST, Korea, Republic of (2) Samsung Electronics, Korea, Republic of (3) University of Washington, United States (4) ETRI, Korea, Republic of ","Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM",ACM SIGMobile,Association for Computing Machinery,,,7-Aug-19,MobiCom 2019 - Proceedings of the 25th Annual International Conference on Mobile Computing and Networking,2019,,,,9.78145E+12,10.1145/3300061.3300128,,"25th Annual International Conference on Mobile Computing and Networking, MobiCom 2019","October 21, 2019 - October 25, 2019",,"Overheating smartphones could hamper user experiences. While there have been numerous reports on smartphone overheating, a systematic measurement and user experience study on the thermal aspect of smartphones is missing. Using thermal imaging cameras, we measure and analyze the temperatures of various smartphones running diverse application workloads such as voice calling, video recording, video chatting, and 3D online gaming. Our experiments show that running popular applications such as video chat, could raise the smartphone's surface temperature to over 50C in only 10 minutes, which could easily cause thermal pain to users. Recent ubiquitous scenarios such as augmented reality and mobile deep learning also have considerable thermal issues. We then perform a user study to examine when the users perceive heat discomfort from the smartphones and how they react to overheating. Most of our user study participants reported considerable thermal discomfort while playing a mobile game, and that overheating disrupted interaction flows. With this in mind, we devise a smartphone surface temperature prediction model, by using only system statistics and internal sensor values. Our evaluation showed high prediction accuracy with root-mean-square errors of less than 2C. We discuss several insights from our findings and recommendations for user experience, OS design, and developer support for better user-thermal interactions. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",65,Smartphones,Atmospheric temperature - Augmented reality - Deep learning - Infrared imaging - Mean square error - Mobile computing - Surface properties - Temperature indicating cameras - Three dimensional computer graphics - Video recording,Diverse applications - Root mean square errors - Surface temperature prediction - Surface temperatures - Thermal characteristics - Thermal imaging cameras - Thermal model - User guidance,"443.1 Atmospheric Properties - 716.4 Television Systems and Equipment - 718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications - 742.2 Photographic Equipment - 746 Imaging Techniques - 922.2 Mathematical Statistics - 951 Materials Science",,,"Number: 2016R1A2B4014068, Acronym: MSIP, Sponsor: Ministry of Science, ICT and Future Planning; Number: NRF-2017M3C4A7065960, Acronym: -, Sponsor: -; Number: -, Acronym: NRF, Sponsor: National Research Foundation of Korea; ",This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No.2016R1A2B4014068) and the Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT (NRF-2017M3C4A7065960).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Virtual, augmented, and mixed reality for human-robot interaction (VAM-HRI)","Williams, Tom (1); Szafir, Daniel (2); Chakraborti, Tathagata (3); Soh Khim, Ong (4); Rosen, Eric (5); Booth, Serena (6); Groechel, Thomas (7) ","(1) Colorado School of Mines, Golden; CO, United States (2) University of Colorado Boulder, Boulder; CO, United States (3) IBM Research AI, Cambridge; MA, United States (4) National University of Singapore, Singapore, Singapore (5) Brown University, Providence; RI, United States (6) Massachusetts Institute of Technology, Cambridge; MA, United States (7) University of Southern California, Los Angeles; CA, United States ",ACM/IEEE International Conference on Human-Robot Interaction,ARM; Cambridge Consultants; et al.; FN Robotics; Furhat Robotics; Halodi,IEEE Computer Society,,p 663-664,23-Mar-20,HRI 2020 - Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction,2020,,,21672148,9.78145E+12,10.1145/3371382.3374850,,"15th Annual ACM/IEEE International Conference on Human Robot Interaction, HRI 2020","March 23, 2020 - March 26, 2020",,"The 3rd International Workshop on Virtual, Augmented, and Mixed Reality for Human-Robot Interactions (VAM-HRI) will bring together HRI, Robotics, and Mixed Reality researchers to address challenges in mixed reality interactions between humans and robots. Topics relevant to the workshop include development of robots that can interact with humans in mixed reality, use of virtual reality for developing interactive robots, the design of augmented reality interfaces that mediate communication between humans and robots, comparisons of the capabilities and perceptions of robots and virtual agents, and best design practices. VAM-HRI 2020 will follow on the success of VAM-HRI 2018 and 2019, and advance the cause of this nascent research community. © 2020 ACM.",24,Human robot interaction,Agricultural robots - Augmented reality - Machine design - Man machine systems - Mixed reality - Virtual addresses,Design practice - Interactive robot - International workshops - Reality interface - Research communities - Virtual agent,"601 Mechanical Design - 722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Designing an informal learning curriculum to develop 3D modeling knowledge and improve spatial thinking skills,"Bhaduri, Srinjita (1); Sumner, Tamara (1); Van Horne, Katie (2) ","(1) Institute of Cognitive Science, Department of Computer Science, University of Colorado Boulder, Boulder; CO, United States (2) Institute of Cognitive Science, School of Education, University of Colorado Boulder, Boulder; CO, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3299039,3299039,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"We report on the design and implementation of a 3-week long summer academy introducing high school students to 3D modeling and 3D printing experiences. Supporting youth in developing 3D modeling knowledge can enhance their capacity to effectively use an array of emerging technologies such as Virtual Reality, Augmented Reality, and digital fabrication. We used tools and practices from both formal and informal education, such as storylining, to inform the design of the curriculum. We collected data through surveys, artifacts, observations, screen recordings, and group videos. Our findings suggest that (1) emphasizing curricular coherence as a design goal and (2) providing youth with multiple avenues for engaging in 3D modeling can help to: spark youth interest in 3D printing/modeling, maintain youth engagement in learning activities over the course of several weeks, and provide youth with opportunities to develop their spatial thinking skills. © 2019 Copyright held by the owner/author(s). ACM",11,3D modeling,3D printers - Augmented reality - Curricula - Human engineering - Learning systems - Virtual reality,3-D printing - Informal learning - Spatial thinking - Storylines - Youth,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 745.1.1 Printing Equipment - 901.2 Education",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
There's more than meets the eye: Enhancing robot control through augmented visual cues,"Arévalo Arboleda, Stephanie (1); DIerks, Tim (1); Rücker, Franziska (1); Gerken, Jens (1) ","(1) Westphalian University of Applied Sciences, Gelsenkirchen, Germany ",ACM/IEEE International Conference on Human-Robot Interaction,ARM; Cambridge Consultants; et al.; FN Robotics; Furhat Robotics; Halodi,IEEE Computer Society,,p 104-106,23-Mar-20,HRI 2020 - Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction,2020,,,21672148,9.78145E+12,10.1145/3371382.3378240,,"15th Annual ACM/IEEE International Conference on Human Robot Interaction, HRI 2020","March 23, 2020 - March 26, 2020",,"In this paper, we present the design of a visual feedback mechanism using Augmented Reality, which we call augmented visual cues, to assist pick-and-place tasks during robot control. We propose to augment the robot operator's visual space in order to avoid attention splitting and increase situational awareness (SA). In particular, we aim to improve on the SA concepts of perception, comprehension, and projection as well as the overall task performance. For that, we built upon the interaction design paradigm proposed by Walker et al.. On the one hand, our design augments the robot to support picking-tasks; and, on the other hand, we augment the environment to support placing-tasks. We evaluated our design in a first user study, and results point to specific design aspects that need improvement while showing promise for the overall approach, in particular regarding user satisfaction and certain SA concepts. © 2020 ACM.",14,Human robot interaction,Agricultural robots - Augmented reality - Machine design - Man machine systems - Visual communication - Visual servoing,Interaction design - Pick and place - Robot controls - Robot operators - Situational awareness - Task performance - User satisfaction - Visual feedback,"601 Mechanical Design - 717.1 Optical Communication Systems - 723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,"Number: -, Acronym: BMBF, Sponsor: Bundesministerium f&Atilde;&frac14;r Bildung und Forschung; ",This research is supported by the German Federal Ministry of Education and Research (BMBF).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Edge cloud offloading algorithms: Issues, methods, and perspectives","Wang, Jianyu (1); Pan, Jianli (1); Esposito, Flavio (2); Calyam, Prasad (3); Yang, Zhicheng (4); Mohapatra, Prasant (4) ","(1) 326 Express Scripts Hall, One University Blvd., St.Louis; MO; 63121, United States (2) Ritter Hall 217, 220 North Grand Blvd., St. Louis; MO; 63103, United States (3) 221 Naka Hall, Columbia; MO; 65211, United States (4) 2063 Kemper Hall, Davis; CA; 95616, United States ",ACM Computing Surveys,,Association for Computing Machinery,"v 52, n 1",,Feb-19,,2019,,3600300,15577341,,10.1145/3284387,2,,,,"Mobile devices supporting the 'Internet of Things' often have limited capabilities in computation, battery energy, and storage space, especially to support resource-intensive applications involving virtual reality, augmented reality, multimedia delivery, and artificial intelligence, which could require broad bandwidth, low response latency, and large computational power. Edge cloud or edge computing is an emerging topic and a technology that can tackle the deficiencies of the currently centralized-only cloud computing model and move the computation and storage resources closer to the devices in support of the above-mentioned applications. To make this happen, efficient coordination mechanisms and 'offloading' algorithms are needed to allow mobile devices and the edge cloud to work together smoothly. In this survey article, we investigate the key issues, methods, and various state-of-the-art efforts related to the offloading problem. We adopt a new characterizing model to study the whole process of offloading from mobile devices to the edge cloud. Through comprehensive discussions, we aim to draw an overall 'big picture' on the existing efforts and research directions. Our study also indicates that the offloading algorithms in the edge cloud have demonstrated profound potentials for future technology and application development. © 2019 Association for Computing Machinery.",53,Multimedia systems,Augmented reality - Edge computing - Internet of things - Mathematical models - Virtual reality,Application development - Broad bandwidths - Computational power - Coordination mechanisms - Future technologies - Multimedia delivery - State of the art - Storage resources,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: CNS-1647084, Acronym: NSF, Sponsor: National Science Foundation; ","This work has been partially supported by a University of Missouri Research Board (UMRB) award and the National Science Foundation award CNS-1647084. Authors&rsquo; addresses: J. Wang and J. Pan, 326 Express Scripts Hall, One University Blvd., St.Louis, MO 63121; emails: {jwgxc, pan}@umsl.edu; F. Esposito, Ritter Hall 217, 220 North Grand Blvd., St. Louis, MO 63103; email: espositof@slu.edu; P. Calyam, 221 Naka Hall, Columbia, MO 65211; email: calyamp@missouri.edu; Z. Yang and P. Mohapatra, 2063 Kemper Hall, Davis, CA 95616; emails: {zcyang, pmohapatra}@ucdavis.edu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. &copy; 2019 Association for Computing Machinery. 0360-0300/2019/02-ART2 $15.00 https://doi.org/10.1145/3284387",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Indoor localization improved by spatial context - A survey,"Gu, Fuqiang (1); Hu, Xuke (2); Ramezani, Milad (1); Acharya, Debaditya (1); Khoshelham, Kourosh (1); Valaee, Shahrokh (3); Shang, Jianga (4) ","(1) University of Melbourne, Parkville, Melbourne; VIC; 3010, Australia (2) Heidelberg University, Grabengasse 1, Heidelberg; 69117, Germany (3) University of Toronto, 10 King's College Rd, Toronto; ON, Canada (4) China University of Geosciences, 388 Lumo Rd, Wuhan, Hubei, China ",ACM Computing Surveys,,Association for Computing Machinery,"v 52, n 3",,Jul-19,,2019,,3600300,15577341,,10.1145/3322241,64,,,,"Indoor localization is essential for healthcare, security, augmented reality gaming, and many other locationbased services. There is currently a wealth of relevant literature on indoor localization. This article focuses on recent advances in indoor localization methods that use spatial context to improve the location estimation. Spatial context in the form of maps and spatial models have been used to improve the localization by constraining location estimates in the navigable parts of indoor environments. Landmarks such as doors and corners, which are also one form of spatial context, have proved useful in assisting indoor localization by correcting the localization error. This survey gives a comprehensive review of state-of-the-art indoor localization methods and localization improvement methods using maps, spatial models, and landmarks. © 2019 Association for Computing Machinery.",198,Indoor positioning systems,Augmented reality - Location - mHealth - Smartphones - Surveys,Hybrid localization - Indoor positioning - Landmark detection - Sensory landmarks - Spatial informations - Wireless localization,"718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: 2016YFB0502200, Acronym: -, Sponsor: National Basic Research Program of China (973 Program); ","This work is jointly supported by the National Key Research and Development Program of China (Grant No. 2016YFB0502200) and the China Scholarship Council-University of Melbourne Research Scholarship (Grant No. CSC 201408420117). Authors&rsquo; addresses: F. Gu, University of Melbourne, Parkville, Melbourne, VIC, 3010, Australia; email: fuqiangg@student.unimelb.edu.au; X. Hu (corresponding author), Grabengasse 1, Heidelberg University, Heidelberg, 69117, Germany; email: xuke.hu@uni-heidelberg.de; M. Ramezani, D. Acharya, and K. Khoshelham, University of Melbourne, Parkville, Melbourne, VIC, 3010, Australia; emails: {mramezani, acharyad}@student.unimelb.edu.au, k.khoshelham@unimelb.edu.au; S. Valaee, University of Toronto, 10 King&rsquo;s College Rd, Toronto, ON, Canada; email: valaee@ece.utoronto.ca; J. Shang, 388 Lumo Rd, China University of Geosciences, Wuhan, Hubei, China; email: jgshang@cug.edu.cn. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. &copy; 2019 Association for Computing Machinery. 0360-0300/2019/07-ART64 $15.00 https://doi.org/10.1145/3322241",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Local light field fusion: Practical view synthesis with prescriptive sampling guidelines,"Mildenhall, Ben (1); Srinivasan, Pratul P. (1); Ortiz-Cayon, Rodrigo (2); Kalantari, Nima Khademi (3); Ramamoorthi, Ravi (4); Ng, Ren (1); Kar, Abhishek (2) ","(1) University of California, Berkeley, United States (2) Fyusion Inc., United States (3) Texas A and M University, United States (4) University of California, San Diego, United States ",ACM Transactions on Graphics,,Association for Computing Machinery,"v 38, n 4",,Jul-19,,2019,,7300301,15577368,,10.1145/3306346.3322980,29,,,,"We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000× fewer views. We demonstrate our approach's practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms. © 2019 held by the owner/author(s).",51,Rendering (computer graphics),Augmented reality - Deep learning - Image processing,Image based rendering - Irregular grids - Light fields - Perceptual quality - Plenoptic samplings - Scene representation - View synthesis - Virtual exploration,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
User Interface model-based for basic-skills training in medical applications,"Vergara, Tania (1); Bautista, Luis (1); Pedraza, Gabriel (1) ","(1) Universidad Industrial de Santander -UIS, Bucaramanga, Colombia ",ACM International Conference Proceeding Series,ACM SIGCHI; ACM SIGCHI Latin American HCI Community,Association for Computing Machinery,,,30-Sep-19,CLIHC 2019 - Proceedings of the 9th Latin American Conference on Human Computer Interaction,2019,,,,9.78145E+12,10.1145/3358961.3358986,a25,"9th Latin American Conference on Human Computer Interaction, CLIHC 2019","September 30, 2019 - October 4, 2019",,"Creating scenarios for training motor skills such as hand-eye coordination using augmented reality (AR) technology has several challenges. One of them, the application of task variability, a principle to foster flexibility and to unanticipated tasks adaptation. Due to its benefits, a model-based approach was applied to implement it in the training scenario development. In this paper, concretization rules were proposed and evaluated, from a task model that describes the practical exercise. The results show the concretization effectiveness. A case study is made for the design of hand-eye coordination exercises, a skill required for surgical procedures such as the transpedicular screws placement in spine surgery. © 2019 Copyright is held by the owner/author(s).",12,User interfaces,Augmented reality - Human computer interaction - Medical applications - Personnel training - Transplantation (surgical),Hand eye coordination - MBUID - Medical skills - Model based approach - Skills training - Surgical procedures - Training scenario - User interface models,"462.4 Prosthetics - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 912.4 Personnel",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Improving maintenance efficiency with an adaptive AR-assisted maintenance system,"Siew, C.Y. (1); Nee, A.Y.C. (2); Ong, S.K. (2) ","(1) NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, 28 Medical Drive, Singapore; 117456, Singapore (2) Mechanical Engineering Department, National University of Singapore, 9 Engineering Drive 1, Singapore; 117576, Singapore ",ACM International Conference Proceeding Series,China University of Geosciences; Guangdong University of Technology; University of Electronic Science and Technology of China; Xihua University,Association for Computing Machinery,,p 74-78,26-Jul-19,"Proceedings of the 2019 4th International Conference on Robotics, Control and Automation, ICRCA 2019 - Workshop 2019 the 4th International Conference on Robotics and Machine Vision, ICRMV 2019",2019,,,,9.78145E+12,10.1145/3351180.3351203,,"2019 4th International Conference on Robotics, Control and Automation, ICRCA 2019 and its Workshop of 2019 4th International Conference on Robotics and Machine Vision, ICRMV 2019","July 26, 2019 - July 28, 2019",,"The lack of adaptability of existing Augmented Reality (AR) assisted maintenance systems has prevented the implementation of many existing AR systems in real industrial maintenance scenarios. This paper presents an adaptive Augmented Reality human-machine interface (AR-HMI) framework that can provide suitable sets of maintenance information and guidance to an operator during maintenance to enhance efficiency and safety. A human-centric framework has been developed to determine the most suitable types of information to be augmented and presented to the user. During maintenance, the AR-assisted system allows a user to request for a change in the types of augmentation via an explicit request or an implicit mechanism, which is based on the head-gaze of the user. To demonstrate the viability of the AR-HMI framework and AR-assisted system, a case study based on an industrial robot has been conducted. © 2019 Association for Computing Machinery.",13,Computer vision,Augmented reality - Efficiency - Human computer interaction - Maintenance - Robotics,Adaptive augmented realities - AR system - Human Machine Interface - Human-centric - Industrial maintenance - Maintenance efficiency - Maintenance information - Maintenance systems,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 731.5 Robotics - 913.1 Production Engineering - 913.5 Maintenance",,,,"This research is supported by the Singapore A*STAR Agency for Science, Technology and Research Public Sector Research Funding Programme, Project No. 1521200081.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Using visual intelligence to automate maintenance task guidance and monitoring on a head-mounted display,"Ng, Lai Xing (1); Ng, Jamie (1); Tang, Keith T.W. (1); Li, Liyuan (1); Rice, Mark (1); Wan, Marcus (1) ","(1) Institute for Infocomm Research, A*STAR, 1 Fusionopolis Way, 138632, Singapore ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 70-75,22-Nov-19,ICRAI 2019 - 2019 5th International Conference on Robotics and Artificial Intelligence,2019,,,,9.78145E+12,10.1145/3373724.3373727,,"5th International Conference on Robotics and Artificial Intelligence, ICRAI 2019","November 22, 2019 - November 24, 2019",,"Maintenance is a required process in many commercial domains. Commonly, paper-based instruction manuals are digitised to improve accessibility to maintenance information. Among the different forms of digital media, augmented reality (AR) combined with machine learning has the potential to provide real-time user guidance and interaction. In this paper, we present an Augmented Reality Visual Intelligence (ARVI) framework, which combines visual perception with cognitive task reasoning to monitor user performance and provide contextualised guidance for maintenance tasks. The framework is implemented in Microsoft Hololens for a wheel removal operation. The implemented software can recognise aspects of the environment, objects and a user's hands, as well as infer the task being completed. The evaluation of visual perception models for object detection using a single task video for training obtained relatively high F1 scores, and a small-scale usability study reported some positive feedback on the application development. © 2019 Association for Computing Machinery.",23,Helmet mounted displays,Augmented reality - Digital storage - Maintenance - Object detection - Robotics - Street traffic control - Vision,Application development - Automated tasks - Head mounted displays - Instruction manuals - Maintenance information - Task monitoring - Visual intelligence - Visual perception models,"406.2 Roads and Streets - 722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 731.5 Robotics - 913.5 Maintenance",,,"Number: 162150016, Acronym: -, Sponsor: -; Number: -, Acronym: I&Acirc;&sup2;R, Sponsor: Institute for Infocomm Research; ","We would like to thank Joy Wang, DSTA, the A*STAR Aerospace Programme, and ST Aerospace for their support in this research project. This research is supported by the Agency for Science Technology and Research (A*STAR) Aerospace Programme Grant No. 162150016 and the Institute for Infocomm Research.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented object selection through smart glasses,"Franco, Jéssica (1); Cabral, Diogo (2) ","(1) ITI, LARSyS, University of Madeira, Funchal, Portugal (2) ITI, LARSyS, IST, University of Lisbon, Lisboa, Portugal ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,10.1145/3365610.3368416,3368416,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"Several interaction methods for smart glasses have been studied, but it is not clear which method could be the best to interact with augmented objects. In this study, an Augmented Reality (AR) marker-based application for smart glasses was developed, implementing three different interaction methods for marker selection: one using mid-air hand gestures, other using head movements and double-tap in the glasses, and the last one using a handheld controller. We studied the users’ interaction times, preferences, and their willingness to perform the interaction method in public. We find that users’ felt more comfortable using a familiar handheld controller. However, the solution with the smart glass’s tap function and head movement got results close to the results of smart glass’s controller. © 2019 Copyright is held by the owner/author(s).",17,Glass,Augmented reality - Controllers,Interaction methods - Marker-based - Smart glass - Touch inputs - Touchless,"723 Computer Software, Data Handling and Applications - 732.1 Control Equipment - 812.3 Glass",,,"Number: UID/EEA/50009/2019, Acronym: -, Sponsor: -; ",This work was partially funded by FCT/MCTES LARSyS (UID/EEA/50009/2013 (2015-2017)) and by FCT/MCTES LARSyS (UID/EEA/50009/2019).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A review of visual-based localization,"Xin, Xing (1); Jiang, Jie (1); Zou, Yin (1) ","(1) College of Systems Engineering, National University of Defense Technology, Changsha, China ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 94-105,20-Sep-19,"Proceedings of the 2019 International Conference on Robotics, Intelligent Control and Artificial Intelligence, RICAI 2019",2019,,,,9.78145E+12,10.1145/3366194.3366211,,"2019 International Conference on Robotics, Intelligent Control and Artificial Intelligence, RICAI 2019","September 20, 2019 - September 22, 2019",,"The visual-based localization (VBL) obtains the corresponding pose estimation in the localization system by utilizing various useful information in the surrounding environment, such as images, point cloud models, geometric information, semantic information. In recent years, visual-based localization (VBL) has been widely concerned by scientists, mainly because the commonly used GPS localization system cannot be effectively used in various environments. When GPS localization fails in some scenes such as very messy environments and severe signal occlusion, we can consider using visual-based localization to obtain the pose of the query images. Visual-based localization (VBL) has been widely used in the field of visual tasks, such as augmented reality, unmanned vehicle navigation, robotics, closed-loop detection, SFM (Structure from Motion) models. After years of development, the methods of visual-based localization (VBL) have been enriched and developed, In order to better understand the latest developments in VBL, overall research status and possible future development trends, we need make a systematic detailed classification of VBL. Although the predecessors have summarized the methods of VBL, due to the many new breakthroughs in VBL in recent years, the original summary is not perfect enough. So this paper will make a new and more detailed review of VBL in recent years. This paper divides the visual-based localization methods into three categories: image-based localization, localization based on learning model and localization based on 3D structure. And we also detail the principle, development of methods and the advantages and disadvantages of each method and future development trends. © 2019 Association for Computing Machinery.",81,Learning systems,Augmented reality - Global positioning system - Object recognition - Robotics - Robots - Semantics - Vision,3D Structure - Closed-loop detections - Detailed classification - Geometric information - Image-based localizations - Learning models - Surrounding environment - Visual-based localization,"723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,"Number: 61873274, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; ","Thanks to the guidance of the instructor and the help of the schoolmate, and thanks to the National Natural Science Foundation of China under Project 61873274 for funding.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Supporting self-evaluation for children with mental disabilities through Augmented Reality,"Torrado, Juan C. (1); Gomez, Javier (1); Jaccheri, Letizia (2) ","(1) UAM, Madrid, Spain (2) NTNU, Trondheim, Norway ","Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",Boise Osmo; Boise State University; et al.; Langan Barber Foundation; St. Luke's; STEM Action Center,"Association for Computing Machinery, Inc",,p 635-641,12-Jun-19,"Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",2019,,,,9.78145E+12,10.1145/3311927.3325307,,"18th ACM International Conference on Interaction Design and Children, IDC 2019","June 12, 2019 - June 15, 2019",,"Self-evaluation is the ability to assess one's work, and is a key element in the psycho-pedagogical development of children with special needs in their path towards autonomy and self-determination. Acquiring this skill requires explicit training and materials, and it is often cumbersome and timeconsuming. In this paper we present a study to ascertain to what extent systems based on Augmented Reality (AR) are a suitable and less expensive alternative to help children with cognitive disabilities to train self-evaluation skills in special education schools. For this purpose, we have developed tablet application (BART) that offers assistance to children with special needs to self-evaluate basic arithmetic operations. The system was designed through the involvement of 2 educators, 2 experts on psycho-pedagogy, and 2 software designers. The contribution of this paper is the description of BART, an innovative system for children with special needs and a concrete plan for an empirical study that is to be carried out on a short-term basis. Here we describe the methodology that is to be applied to the proposed study and outline the main expectations about the results and their implications in the issue of self-evaluation skills acquisition for children in special education. © 2019 Association for Computing Machinery.",14,Augmented reality,Cognitive systems,Arithmetic operations - Cognitive disability - Innovative systems - Mental disabilities - Self evaluation - Software designers - Special education - Tablet applications,"723 Computer Software, Data Handling and Applications",,,,"This work is carried out during the tenure of two ERCIM Alain Bensoussan"" Fellowship Programme.""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Adopting conversational interfaces for exploring OSGi-Based software architectures in augmented reality,"Seipel, Peter (1); Stock, Adrian (1); Santhanam, Sivasurya (1); Baranowski, Artur (1); Hochgeschwender, Nico (1); Schreiber, Andreas (1) ","(1) German Aerospace Center (DLR), Simulation and Software Technology, Cologne, Germany ","Proceedings - 2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering, BotSE 2019",,Institute of Electrical and Electronics Engineers Inc.,,p 20-21,May-19,"Proceedings - 2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering, BotSE 2019",2019,,,,9.78173E+12,10.1109/BotSE.2019.00013,8823630,"1st IEEE/ACM International Workshop on Bots in Software Engineering, BotSE 2019",28-May-19,,"We propose conversational interfaces as a convenient and complementary way for users to explore OSGi-based software architectures in immersive Augmented Reality (AR). By providing a conversational interface we aim to remedy some peculiarities of AR devices, but also enhancing the exploration task at hand. We exemplify a use case and sketch how different user utterances can be used to retrieve information about the to-be-explored OSGi-based software architecture. We identify crucial components such as natural language generation and intent recognition which are required to implement the user story and we outline the status of our implementation. © 2019 IEEE.",5,Augmented reality,Botnet - Natural language processing systems - Software engineering,Chatbot - Conversational interface - Exploration tasks - Immersive augmented realities - Intent recognition - Natural language generation - Software visualization - User stories,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
FUROSHIKI: Augmented reality media that conveys Japanese traditional culture,"Kobayashi, Kei (1); Nagata, Kazuma (1); Rakuten, Soh Masuko (2); Hoshino, Junichi (2) ","(1) Systems and Information Engineering, University of Tsukuba, Ibaraki, Japan (2) Institute of Technology Rakuten, Inc., Tokyo, Japan ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365716,a27,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"Furoshiki is a traditional culture of Japan which has prevailed in its everyday life and is widely used even today. In this paper, we propose a system through which users can be, while getting interested, exposed including to Japanese wrapping culture and attitude of dealing with things with care by augmenting an act to 'wrap' with a piece of Japanese traditional furoshiki using the image-recognition technique and the motion-image projection technique. © 2019 Association for Computing Machinery.",4,Augmented reality,Cell culture - Image recognition - Interactive computer graphics - Virtual reality,Interaction - Media arts - Motion images - Traditional cultures,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Magical realism and augmented reality: Designing apps with children in a cultural institution,"Schofield, Tom (1); Pisanty, Diego Trujillo (1); Arrigoni, Gabriella (1); Reynolds, Kim (2); Pattinson, Rachel (3) ","(1) Culture Lab., School of Arts and Cultures, Newcastle University, Newcastle upon Tyne, United Kingdom (2) School of English Language and Linguistics, Newcastle University, Newcastle upon Tyne, United Kingdom (3) Vital North Partnership, Newcastle University, Newcastle upon Tyne, United Kingdom ",DIS 2019 - Proceedings of the 2019 ACM Designing Interactive Systems Conference,Adobe; Google; Sketch; UC San Diego's Design Lab; Virginia Tech,"Association for Computing Machinery, Inc",,p 737-749,18-Jun-19,DIS 2019 - Proceedings of the 2019 ACM Designing Interactive Systems Conference,2019,,,,9.78145E+12,10.1145/3322276.3322293,,"2019 ACM Conference on Designing Interactive Systems, DIS 2019","June 23, 2019 - June 28, 2019",,"We describe the development and implementation of a 7-month-long project which used a series of creative workshops designed in collaboration with a cultural institution and conducted with children to draw influences from magical realist literature into the development AR applications. The project culminated in the release of an AR app for Android and iOS platforms, Magical Reality. After describing the design and implementation of the research we discuss its findings as they support the two facets of our contribution to DIS: First, we assess our attempts to apply inspiration, derived from workshopping ideas from magical realist literature with children to the design of AR experiences, making recommendations for future design practice seeking to include comparable influences. Second, we consider the degree to which our workshops were successful in combining specialist knowledges from across the different departments of a cultural organization to answer sectoral challenges and describe both advantages and challenges for future collaborative work. © 2019 Association for Computing Machinery.",49,Augmented reality,,AR application - Children - Collaborative Work - Creative workshops - Cultural institutions - Design and implementations - Magical realism - Workshopping,"723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: AHRC, Sponsor: Arts and Humanities Research Council; Number: AH/R009155/1, Acronym: EPSRC, Sponsor: Engineering and Physical Sciences Research Council; ","We are thankful to our funders the Arts and Humanities Research Council and the Engineering and Physical Sciences Research Council for their generous support under grant number AH/R009155/1. We wish also to thank our partners Seven Stories, the National Centre for Children's Books and in particular the staff who supported our work. Lastly, we thank the eighty children and young people, story tellers, archivists, exhibition designers and app developers who attended our workshops and to David Almond for the use of his work.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Creating your first augmented reality experience with ArCore,"Ruffle, Germain ; Meng, Eugene ",,"SIGGRAPH Asia 2019 Courses, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,17-Nov-19,"SIGGRAPH Asia 2019 Courses, SA 2019",2019,,,,9.78145E+12,10.1145/3355047.3359416,3359416,"SIGGRAPH Asia 2019 Courses - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,,,,,,,,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Blocks: Collaborative and persistent augmented reality experiences,"Guo, Anhong (1); Canberk, Ilter (2); Murphy, Hannah (3); Monroy-Hernández, Andrés (4); Vaish, Rajan (5) ","(1) Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh; PA; 15213, United States (2) Snap Inc., Venice; CA; 90291, United States (3) Wellesley College, Wellesley; MA; 02481, United States (4) Snap Inc., Seattle; WA; 98121, United States (5) Snap Inc., Santa Monica; CA; 90405, United States ","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",,Association for Computing Machinery,"v 3, n 3",,Sep-19,,2019,,,24749567,,10.1145/3351241,83,,,,"We introduce Blocks, a mobile application that enables people to co-create AR structures that persist in the physical environment. Using Blocks, end users can collaborate synchronously or asynchronously, whether they are colocated or remote. Additionally, the AR structures can be tied to a physical location or can be accessed from anywhere. We evaluated how people used Blocks through a series of lab and field deployment studies with over 160 participants, and explored the interplay between two collaborative dimensions: space and time. We found that participants preferred creating structures synchronously with colocated collaborators. Additionally, they were most active when they created structures that were not restricted by time or place. Unlike most of today's AR experiences, which focus on content consumption, this work outlines new design opportunities for persistent and collaborative AR experiences that empower anyone to collaborate and create AR content. © 2019 Copyright held by the owner/author(s).",57,Augmented reality,Argon - Location,Collaboration - Content consumption - Mobile - Mobile applications - Persistence - Physical environments - Physical locations - Shared experiences,"723 Computer Software, Data Handling and Applications - 804 Chemical Products Generally",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
DeepMobilear: A mobile augmented reality application with integrated visual SLAM and object detection,"Lee, Suwoong (1); Lee, Seungjae (1); Lee, Keundong (1); Ko, Jong Gook (1) ","(1) Electronics and Telecommunications Research Institute, Korea, Republic of ","SIGGRAPH Asia 2019 Posters, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,17-Nov-19,"SIGGRAPH Asia 2019 Posters, SA 2019",2019,,,,9.78145E+12,10.1145/3355056.3364557,3364557,"SIGGRAPH Asia 2019 Posters - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,,5,,,,,,,"Number: 2018-0-00198, Acronym: MSIP, Sponsor: Ministry of Science ICT and Future Planning; Number: IITP, Acronym: IITP, Sponsor: Institute for Information and Communications Technology Promotion; ","This work was supported by Institute for Information and communications Technology Planning and Evaluation (IITP) grant funded by the Korea government (MSIP) (No. 2018-0-00198), Object information extraction and real-to-virtual mapping based AR technology)",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Approaches and challenges to virtual and augmented reality in health care and rehabilitation a short course,"Jorge, Joaquim (1); Campos, Pedro (2, 3); Lopes, Daniel Simões (4, 5) ","(1) Instituto Superior Técnico, Universidade de Lisboa, Visualization and Multimodal Interfaces, Portugal (2) University of Madeira, Portugal (3) Interactive Technologies Institute, Portugal (4) Instituto Superior Técnico UT Austin, Portugal (5) Biomedical Research Line PI of project IT-MEDEX, Portugal ","SIGGRAPH Asia 2019 Courses, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,17-Nov-19,"SIGGRAPH Asia 2019 Courses, SA 2019",2019,,,,9.78145E+12,10.1145/3355047.3359418,3359418,"SIGGRAPH Asia 2019 Courses - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,,,,,,,,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ACM International Conference Proceeding Series,,,ACM International Conference Proceeding Series,,Association for Computing Machinery,,,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"The proceedings contain 132 papers. The topics discussed include: information processing in real and in stereoscopic environments; the effect of presence and appearance of guides in virtual reality exhibitions; perceived authenticity, empathy, and pro-social intentions evoked through avatar-mediated self-disclosures; more human-likeness, more trust? the effect of anthropomorphism on self-reported and behavioral trust in continued and interdependent human-agent cooperation; perceptions of a help-requesting robot - effects of eye-expressions, colored lights and politeness of speech; augmented-reality-enhanced product comparison in physical retailing; learning patient transfers with technology: a qualitative investigation of the design space; and supporting anatomy education with a 3D puzzle in a virtual reality environment - results from a pilot study.",,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ACM International Conference Proceeding Series,,,ACM International Conference Proceeding Series,University of Texas-Dallas; Xi'an Jiaotong-Liverpool University,Association for Computing Machinery,v Part F148152,,2019,,2019,,,,9.78145E+12,,,"3rd International Conference on Innovation in Artificial Intelligence, ICIAI 2019","March 15, 2019 - March 18, 2019",,The proceedings contain 52 papers. The topics discussed include: forecasting model of traffic flow prediction model based on multi-resolution SVR; reconstructing attention with dynamic regularization; inference adaptive thresholding based non-maximum suppression for object detection in video image sequence; application research of neural network in river elevation map; ice surface profile three-dimensional reconstruction based on line structured light; a topic matching based CNN for sentence classification; the gap between NMT and professional translation from the perspective of discourse; predicting customer churn in the telecommunication industry by analyzing phone call transcripts with convolutional neural networks; semantic relationship between abbreviations and the original words based on word vectors; automatic object recognition in a light-weight augmented reality-based vocabulary learning application for children with autism; and authorship attribution of the golden lotus based on text classification methods.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",,,"ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",2019,,,,9.78145E+12,,,"ACM SIGGRAPH 2019 Appy Hour - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"The proceedings contain 10 papers. The topics discussed include: Aire - visualize air quality; ARCalVR: augmented reality playground on mobile devices; nira: view, review, and present GBytes-sized assets with interactive rendering on any device; PlayGAMI: augmented reality origami creativity platform; REALITY: broadcast your virtual beings from everywhere; sur.faced.io: augmented reality content creation for your face and beyond by drawing on paper; ViVid: depicting dynamics in stylized live photos; UBeBot - voice-driven, personalized, avatar-based communicative video content in A/R; VR Tsunami!; and Xploro: multi-user AR.",,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,,,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,,,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,The proceedings contain 70 papers. The topics discussed include: reducing latency in a collaborative augmented reality service; the impact of remote user's role in a mixed reality mixed presence system; within a virtual crowd: exploring human movement behavior during immersive virtual crowd interaction; investigating the use of different visual cues to improve social presence within a 360 mixed reality remote collaboration; motion volume: visualization of human motion manifolds; using augmented reality to improve productivity and safety for heavy machinery operators: state of the art; a data-driven optimization approach to urban multi-site selection for public services and retails; a bowl-shaped display for controlling remote vehicles; and analysis of VR sickness and gait parameters during non-isometric virtual walking with large translational gain.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,,,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,,,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,The proceedings contain 35 papers. The topics discussed include: pursuit sensing: extending hand tracking space in mobile VR applications; minuet: multimodal interaction with an Internet of things; analysis of peripheral vision and vibrotactile feedback during proximal search tasks with dynamic virtual entities in augmented reality; investigating the effect of distractor interactivity for redirected walking in virtual reality; LIVE: the human role in learning in immersive virtual environments; blended agents: manipulation of physical objects within mixed reality environments and beyond; extending virtual reality DisplayWall environments using augmented reality; extramission: a large scale interactive virtual environment using head mounted projectors and retro-reflectors; and effects of dark mode on visual fatigue and acuity in optical see-through head-mounted displays.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,,,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"The proceedings contain 50 papers. The topics discussed include: is it real? measuring the effect of resolution, latency, frame rate and jitter on the presence of virtual entities; creating accessible interactive audio-tactile drawings using spatial augmented reality; spotlight on off-screen points of interest in handheld augmented reality: halo-based techniques; CARDS: a mixed-reality system for collaborative learning at school; modeling pen steering performance in a single constant-width curved path; eliciting pen-holding postures for general input with suitability for EMG armband detection; and EyeDescribe: combining eye gaze and speech to automatically create accessible touch screen artwork.",,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Proceedings: SCF 2019 - ACM Symposium on Computational Fabrication,,,Proceedings: SCF 2019 - ACM Symposium on Computational Fabrication,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,16-Jun-19,Proceedings: SCF 2019 - ACM Symposium on Computational Fabrication,2019,,,,9.78145E+12,,,"2019 ACM Symposium on Computational Fabrication, SCF 2019","June 16, 2019 - June 18, 2019",,The proceedings contain 9 papers. The topics discussed include: human-in-the-loop fabrication of 3D surfaces with natural tree branches; exploring mechanical meta-material structures through personalized shoe sole design; automated reconstruction of smoothly joining 3D printed restorations to fix broken objects; rough carving of 3D models with spatial augmented reality; echidna: mixed-domain computational implementation via decision trees; volumetric Michell trusses for parametric design & fabrication; a method to evaluate the formability and fluidity of concrete based materials for 3D printing; 3D-printed formwork for bespoke concrete stairs: from computational design to digital fabrication; and computational laser forming origami of convex surfaces.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"WearSys 2019 - Proceedings of the 5th ACM Workshop on Wearable Systems and Applications, co-located with MobiSys 2019",,,"WearSys 2019 - Proceedings of the 5th ACM Workshop on Wearable Systems and Applications, co-located with MobiSys 2019",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,,12-Jun-19,"WearSys 2019 - Proceedings of the 5th ACM Workshop on Wearable Systems and Applications, co-located with MobiSys 2019",2019,,,,9.78145E+12,,,"5th ACM Workshop on Wearable Systems and Applications, WearSys 2019, co-located with MobiSys 2019",21-Jun-19,,The proceedings contain 13 papers. The topics discussed include: a mobility evaluation of tilt panning and offset sensing smart watch input; understanding the predictability of smartwatch usage; wrist-worn wearable sensors to understand insides of the human body: data quality and quantity; protecting visual information in augmented reality from malicious application developers; WhereWear: calibration-free wearable device identification through ambient sensing; smart KT tape - a bendable wearable system for muscle fatigue sensing; position: wearable polymorphic light sensors; on the use of low-cost sensors for non-intrustive newborn sepsis monitoring; and towards machine learning with zero real-world data.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,,,UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Oct-19,UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,The proceedings contain 56 papers. The topics discussed include: cross-ratio based gaze estimation for multiple displays using a polarization camera; evaluating the minimum jerk motion model for redirected reach in virtual reality; gel-based haptic mediator for high-definition tactile communication; a new approach to studying sleep in autonomous vehicles: simulating the waking situation; towards instantaneous recovery from autonomous system failures via predictive crowdsourcing; estimating focused object using smooth pursuit eye movements and interest points in the real world; voice input interface failures and frustration: developer and user perspectives; and distance-driven user interface for collaborative exhibit viewing in augmented reality museum.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Proceedings - Pervasive Displays 2019 - 8th ACM International Symposium on Pervasive Displays, PerDis 2019",,,"Proceedings - Pervasive Displays 2019 - 8th ACM International Symposium on Pervasive Displays, PerDis 2019",,"Association for Computing Machinery, Inc",,,12-Jun-19,"Proceedings - Pervasive Displays 2019 - 8th ACM International Symposium on Pervasive Displays, PerDis 2019",2019,,,,9.78145E+12,,,"8th ACM International Symposium on Pervasive Displays, PerDis 2019","June 12, 2019 - June 14, 2019",,The proceedings contain 42 papers. The topics discussed include: self-moving robots and pulverized urban displays: newcomers in the pervasive display taxonomy; enhancing physical objects with actuated levitating particles; feasibility study on water flow visualization using cellulose particles and pervasive display; designing an interactive gravestone display; increasing trust in fully automated driving: route indication on an augmented reality head-up display; LAME - light-controlled attention guidance for multi-monitor environments; design and evaluation of graphical feedback on tangible interactions in a low-resolution edge display; on a side note { observations on using digital notes on a large display with users sitting at extreme sides; touch or touchless? evaluating usability of interactive displays for persons with autistic spectrum disorders; showboater: insight into sustainable rural community display networks from a longitudinal study; and why simple is best: lessons from designing an emergency system for public displays.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,,,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,The proceedings contain 256 papers. The topics discussed include: sensor network to measure MAAI on value co-creation process; neural network-based indoor tag-less localization using capacitive sensors; augmented reality smartphone compasses: opportunity or oxymoron?; efficient convolutional neural network for FMCW radar based hand gesture recognition; shadower: applying shadows to children’s outdoor interaction; exploring the usefulness of Bluetooth and WiFi proximity for transportation mode recognition; pneuxels: a platform for physically manifesting object-based crowd interactions in large scales; tracing the intangible; SmartLobby: using a 24/7 remote head-eye-tracking for content personalization; and a method to recognize entering and leaving person based on door opening and closing movement using angular velocity sensor.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",,,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",ACM SIGAI; Association for Computing Machinery (ACM); IEEE; IEEE Computer Society; IEEE Technical Council on Software Engineering (TCSE); Special Interest Group on Software Engineering (SIGSOFT),Institute of Electrical and Electronics Engineers Inc.,,,Nov-19,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019,,,,9.78173E+12,,,"34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019","November 10, 2019 - November 15, 2019",,The proceedings contain 154 papers. The topics discussed include: lancer: your code tell me what you need; trusted software supply chain; XRaSE: towards virtually tangible software using augmented reality; automated trainability evaluation for smart software functions; an industrial experience report on performance-aware refactoring on a database-centric web application; improving the decision-making process of self-adaptive systems by accounting for tactic volatility; understanding exception-related bugs in large-scale cloud systems; efï¬ücient transaction-based deterministic replay for multi-threaded programs; and RENN: efï¬ücient reverse execution with neural-network-assisted alias analysis.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Proceedings - 2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering, BotSE 2019",,,"Proceedings - 2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering, BotSE 2019",,Institute of Electrical and Electronics Engineers Inc.,,,May-19,"Proceedings - 2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering, BotSE 2019",2019,,,,9.78173E+12,,,"1st IEEE/ACM International Workshop on Bots in Software Engineering, BotSE 2019",28-May-19,,"The proceedings contain 15 papers. The topics discussed include: a bot for suggesting questions that match each user's expertise; should I stale or should I close? an analysis of a bot that closes abandoned issues and pull requests; TutorBot : contextual learning guide for software engineers; building sankie: an AI platform for DevOps; building an expert recommender chatbot; a smart advisor for software delivery - a bot framework for awareness, alerts and advice; towards an autonomous bot for automatic source code refactoring; and adopting conversational interfaces for exploring OSGi-based software architectures in augmented reality.",,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The ethical and privacy implications of mixed reality,"Bye, Kent (1); Hosfelt, Diane (2); Chase, Sam (2); Miesnieks, Matt (3); Beck, Taylor (4) ","(1) Voices of VR Podcast (2) Mozilla, United States (3) 6D.ai (4) MagicLeap, United States ","ACM SIGGRAPH 2019 Panels, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Panels, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3306212.3328138,,"ACM SIGGRAPH 2019 Panels - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"The spatial computing affordances of virtual and augmented reality introduce new ethical and privacy dilemmas. This panel will explore the many implications of biometric data (eye tracking, facial tracking, gait detection, emotional sentiment analysis, galvanic skin response, EEG, EMG, and ECG) to contextually-aware computing that can scan and identify your immediate surroundings. There are many unknown ethical thresholds with immersive computing, and this panel will discuss our own moral intuitions on the topic while inviting the audience to share their own questions and insights for how to navigate this landscape. © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6312-9/19/07.",9,Mixed reality,Augmented reality - Data privacy - Electrophysiology - Eye tracking - Face recognition - Interactive computer graphics - Philosophical aspects - Sentiment analysis - Virtual reality,Affordances - Biometric data - Ethics - Facial tracking - Gait detection - Galvanic skin response - Spatial computing - Virtual and augmented reality,"461.1 Biomedical Engineering - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Viewers' visions of the future: Co-creating hyper-personalized and immersive TV and video experiences,"Geerts, David (1); Van Beek, Evert (2); Miranda, Fernanda Chocron (3) ","(1) Meaningful Interactions Lab (Mintlab), KU Leuven Leuven, Belgium (2) Delft University of Technology, Delft, Netherlands (3) Universidade Federal do Rio Grande do Sul (UFRGS) Porto Alegre, Brazil and Meaningful Interactions Lab (Mintlab), KU Leuven, Leuven, Belgium ",TVX 2019 - Proceedings of the 2019 ACM International Conference on Interactive Experiences for TV and Online Video,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 59-69,4-Jun-19,TVX 2019 - Proceedings of the 2019 ACM International Conference on Interactive Experiences for TV and Online Video,2019,,,,9.78145E+12,10.1145/3317697.3323356,,"6th ACM International Conference on Interactive Experiences for TV and Online Video, TVX 2019","June 5, 2019 - June 7, 2019",,"The past decade has shown that new technologies can have a profound impact on how we consume television and online video content. As technologies such as VR/AR, sensors, and smart voice assistants are maturing, it is becoming pertinent to study how they could influence the next generation of TV and video experiences. While some experiments already incorporate one or more of these technologies, a systematic study into user expectations for these new technologies has not yet been conducted. In this paper, we present the results of a co-creation session resulting in two future video watching scenarios visualized using storyboards: one presenting a hyper-personalized experience based on the automatic recognition of emotions, and another one presenting an immersive experience using Virtual and Augmented Reality. We conclude with user evaluations of both concepts, offering insights in the opportunities and challenges these concepts could bring for the future of television and video experiences. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",21,Interactive television,Augmented reality - Digital television,Automatic recognition of emotions - Co-designs - Future concepts - Hyper-personalized viewing - Immersive - Systematic study - User expectations - Virtual and augmented reality,"716.4 Television Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,,"We would like to thank all participants that took part in the workshops and evaluations. We would also like to thank the Dutch public broadcaster NPO (Nederlandse Publieke Omroep) for financially supporting this work, and especially Joost Negenman and Sarah Van der Land for their continuous feedback and support throughout the project. Fernanda Chocron Miranda was funded through the project Comparative matrix of qualitative research with digital technology users""", approved for the 2nd Call of the International Cooperation Program (PGCI," Call n. 02/2015) funded by the Brazilian development agency CAPES. The project involves researchers from the Federal University of Par&aacute; (UFPA) and the Federal University of Rio Grande do Sul (UFRGS) - both located in Brazil - as well as from KU Leuven (Belgium).""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,
360-degree Video Gaze Behaviour: A ground-truth data set and a classification algorithm for eye movements,"Agtzidis, Ioannis (1); Startsev, Mikhail (1); Dorr, Michael (1) ","(1) Technical University of Munich, Munich, Germany ",MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,ACM SIGMM,"Association for Computing Machinery, Inc",,p 1007-1015,15-Oct-19,MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,2019,,,,9.78145E+12,10.1145/3343031.3350947,,"27th ACM International Conference on Multimedia, MM 2019","October 21, 2019 - October 25, 2019",,"Eye tracking and the analysis of gaze behaviour are established tools to produce insights into how humans observe their surroundings and consume visual multimedia content. For example, gaze recordings may be directly used to study attention allocation towards the areas and objects of interest. Furthermore, segmenting the raw gaze traces into their constituent eye movements has applications in the assessment of subjective quality and mental load, and may improve computational saliency prediction of the content as well. Currently, eye trackers are beginning to be integrated into commodity virtual and augmented reality set-ups that allow for more diverse stimuli to be presented, including 360 content. However, because of the more complex eye-head coordination patterns that emerge, the definitions and the well-established methods that were developed for monitor-based eye tracking are often no longer directly applicable. The main contributions of this work to the field of 360 content analysis are threefold: First, we collect and partially annotate a new eye tracking data set for naturalistic 360 videos. Second, we propose a new two-stage pipeline for reliable manual annotation of both 'traditional' (fixations and saccades) and more complex eye movement types that is implemented in a flexible user interface. Lastly, we develop and test a proof-of-concept algorithm for automatic classification of all the eye movement types in our data set. The data set and the source code for both the annotation tool and the algorithm are publicly available at https://gin.g-node.org/ioannis.agtzidis/360_em_dataset. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",51,Eye movements,Augmented reality - Classification (of information) - Coordination reactions - Eye tracking - Helmet mounted displays - Statistical tests - User interfaces,Automatic classification - Classification algorithm - Eye movement classifications - Eye-head coordination - Flexible user interfaces - Multimedia contents - Smooth pursuit - Virtual and augmented reality,"716.1 Information Theory and Signal Processing - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 802.2 Chemical Reactions - 922.2 Mathematical Statistics",,,,"This research was supported by the Elite Network Bavaria, funded by the Bavarian State Ministry of Science and the Arts.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Aura: Inside-out electromagnetic controller tracking,"Whitmire, Eric (1); Parizi, Farshid Salemi (1); Patel, Shwetak (1) ","(1) Paul G. Allen School of Computer Science and Engineering Seattle, Washington, United States ","MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 300-312,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,10.1145/3307334.3326090,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,"The ability to track handheld controllers in 3D space is critical for interaction with head-mounted displays, such as those used in virtual and augmented reality systems. Today’s systems commonly rely on dedicated infrastructure to track the controller or only provide inertial-based rotational tracking, which severely limits the user experience. Optical inside-out systems ofer mobility but require line-of-sight and bulky tracking rings, which limit the ubiquity of these devices. In this work, we present Aura, an inside-out electromagnetic 6-DoF tracking system for handheld controllers. The tracking system consists of three coils embedded in a head-mounted display and a set of orthogonal receiver coils embedded in a handheld controller. We propose a novel closed-form and computationally simple tracking approach to reconstruct position and orientation in real time. Our handheld controller is small enough to ft in a pocket and consumes 45 mW of power, allowing it to operate for multiple days on a typical battery. An evaluation study demonstrates that Aura achieves a median tracking error of 5.5 mm and 0.8° in 3D space within arm’s reach. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",37,Controllers,Augmented reality - Helmet mounted displays - Mixed reality - Street traffic control - Tracking (position) - Virtual reality,Electromagnetic tracking - Evaluation study - Handheld controllers - Head mounted displays - Position and orientations - Tracking approaches - Tracking errors - Virtual and augmented reality,"406.2 Roads and Streets - 723 Computer Software, Data Handling and Applications - 732.1 Control Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Wirtinger holography for near-eye displays,"Chakravarthula, Praneeth (1); Peng, Yifan (2); Kollin, Joel (3); Fuchs, Henry (1); Heide, Felix (4) ","(1) University of North Carolina at Chapel Hill, Chapel Hill; NC, United States (2) Stanford University, Stanford; CA, United States (3) Microsoft Research, Redmond; WA, United States (4) Princeton University, Princeton; NJ, United States ",ACM Transactions on Graphics,,Association for Computing Machinery,"v 38, n 6",,Nov-19,,2019,,7300301,15577368,,10.1145/3355089.3356539,3356539,,,,"Near-eye displays using holographic projection are emerging as an exciting display approach for virtual and augmented reality at high-resolution without complex optical setups D shifting optical complexity to computation. While precise phase modulation hardware is becoming available, phase retrieval algorithms are still in their infancy, and holographic display approaches resort to heuristic encoding methods or iterative methods relying on various relaxations. In this work, we depart from such existing approximations and solve the phase retrieval problem for a hologram of a scene at a single depth at a given time by revisiting complex Wirtinger derivatives. We also discuss extending our framework to render 3D volumetric scenes. Using Wirtinger derivatives allows us to pose the phase retrieval problem as a quadratic problem which can be minimized with first-order optimization methods. The proposed Wirtinger Holography is flexible and facilitates the use of different loss functions, including learned perceptual losses parametrized by deep neural networks, as well as stochastic optimization methods. We validate this framework by demonstrating holographic reconstructions with an order of magnitude lower error, both in simulation and on an experimental hardware prototype. © 2019 Copyright held by the owner/author(s).",60,Holographic displays,Augmented reality - Complex networks - Computer generated holography - Computer hardware - Deep neural networks - Heuristic algorithms - Heuristic methods - Holograms - Iterative methods - Optimization - Semiconductor device manufacture - Virtual reality,First order optimization method - Holographic projection - Holographic reconstruction - Optical complexity - Phase retrieval algorithm - Stochastic optimization methods - Vergences - Virtual and augmented reality,"714.2 Semiconductor Devices and Integrated Circuits - 722 Computer Systems and Equipment - 723 Computer Software, Data Handling and Applications - 743 Holography - 743.1 Holographic Techniques - 921.5 Optimization Techniques - 921.6 Numerical Methods",,,"Number: -, Acronym: NSF, Sponsor: National Science Foundation; Number: -, Acronym: NTU, Sponsor: Nanyang Technological University; Number: -, Acronym: UNC, Sponsor: University of North Carolina; Number: -, Acronym: NRF, Sponsor: National Research Foundation Singapore; ","The authors thank Bernard Kress for lending the HOLOEYE LETO-I SLM, Roarke Horstmeyer for many fruitful discussions and also lending the laser diode controller, Andreas Georgiou and Nicolas Pegard for useful suggestions and Pavan Chandra Konda for help with the hardware prototype and experimental captures. This research is supported by an NSF Equipment grant (NSF Award Number:1405847) and in part by the BeingTogether Centre, a collaboration between Nanyang Technological University (NTU) Singapore and Universityof North Carolina (UNC) at Chapel Hill, supported by UNC and the Singapore National Research Foundation, Prime Minister&rsquo;s Office, Singapore under its International Research Centres in Singapore Funding Initiative.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Holographic near-eye displays based on overlap-add stereograms,"Padmanaban, Nitish (1); Peng, Yifan (1); Wetzstein, Gordon (1) ","(1) Electrical Engineering Department, Stanford University, United States ",ACM Transactions on Graphics,,Association for Computing Machinery,"v 38, n 6",,Nov-19,,2019,,7300301,15577368,,10.1145/3355089.3356517,3356517,,,,"Holographic near-eye displays are a key enabling technology for virtual and augmented reality (VR/AR) applications. Holographic stereograms (HS) are a method of encoding a light field into a hologram, which enables them to natively support view-dependent lighting effects. However, existing HS algorithms require the choice of a hogel size, forcing a tradeoff between spatial and angular resolution. Based on the fact that the short-time Fourier transform (STFT) connects a hologram to its observable light field, we develop the overlap-add stereogram (OLAS) as the correct method of 'inverting' the light field into a hologram via the STFT. The OLAS makes more efficient use of the information contained within the light field than previous HS algorithms, exhibiting better image quality at a range of distances and hogel sizes. Most remarkably, the OLAS does not degrade spatial resolution with increasing hogel size, overcoming the spatio-angular resolution tradeoff that previous HS algorithms face. Importantly, the optimal hogel size of previous methods typically varies with the depth of every object in a scene, making the OLAS not only a hogel size-invariant method, but also nearly scene independent. We demonstrate the performance of the OLAS both in simulation and on a prototype near-eye display system, showing focusing capabilities and view-dependent effects. © 2019 Copyright held by the owner/author(s).",63,Holographic displays,Augmented reality - Display devices - Holograms - Holography - Virtual reality,Angular resolution - Enabling technologies - Eye-display systems - Light fields - Lighting effects - Short time Fourier transforms - Spatial resolution - Virtual and augmented reality,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 743 Holography",,,"Number: 1553333, Acronym: NSF, Sponsor: National Science Foundation; Number: -, Acronym: -, Sponsor: Intel Corporation; ","The authors would like to thank Jane Xia for work on the first version of the hardware prototype and Ryan Spicer from Raxium for providing a Unity plugin we used to generate our scenes. This project was generously supported by funding from the National Science Foundation (NSF, award numbers 1553333 and 1839974), a Sloan Fellowship, an Okawa Research Grant, and Intel.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Stressed by design? The problems of transferring interaction design from workstations to mobile interfaces,"Kiss, Francisco (1); Schmidt, Albrecht (2) ","(1) Institute for Visualization and Interactive Systems, University of Stuttgart, Stuttgart, Germany (2) Institute for Human-Centered Ubiquitious Media, Ludwig-Maximilians-University Munich, Munich, Germany ",ACM International Conference Proceeding Series,The European Alliance for Innovation (EAI),Association for Computing Machinery,,p 377-382,20-May-19,"Proceedings of the 13th EAI International Conference on Pervasive Computing Technologies for Healthcare, PervasiveHealth 2019",2019,,,,9.78145E+12,10.1145/3329189.3329232,,"13th EAI International Conference on Pervasive Computing Technologies for Healthcare, PervasiveHealth 2019","May 20, 2019 - May 23, 2019",,"Modern technology use has been linked to stress, with detrimental effects for users’ health. Evidence indicates that stress is caused by the design of interaction between users and systems. Since the introduction of graphical user interfaces, designing the interaction between computing systems and the user has been largely incremental. Moving from the PC to mobile devices has added new interaction modalities and interaction metaphors, but the overall way we interact is still very similar. However, desktop computers were used in specific office situations, whereas mobile devices are in ubiquitous use. A lot of the experienced stress of users is linked to the interaction design that priorities computer initiated interactions over the real world and focuses on providing as much information as possible. Moving into the future and transferring the current interaction design to augmented reality systems is likely to worsen the problem by increasing causes of stress. In our research, we identified the problems for future interactions with augmented reality systems and propose principles that re-think interaction concepts to tackle the causes of stress. We propose a longer-term vision about how daily interactions might be designed to reduce the demand on the user. Based on this we suggest a research agenda to create the framework for stress-free interactions. © 2019 Copyright held by the owner/author(s).",24,Graphical user interfaces,Augmented reality - Human computer interaction - mHealth - Personal computers - Stresses - Ubiquitous computing,Augmented reality systems - Computing system - Current interactions - Interaction concepts - Interaction design - Interaction metaphors - Mobile interface - Modern technologies,"722.2 Computer Peripheral Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: 16SV7527, Acronym: BMBF, Sponsor: Bundesministerium f&uuml;r Bildung und Forschung; Number: 683008, Acronym: ERC, Sponsor: H2020 European Research Council; ","This research was supported by the European Union&rsquo;s Horizon 2020 Program under ERCEA grant no. 683008 AMPLIFY. This work is also part of the project Be-Greifen and was supported by the German Federal Ministry of Education and Research, under grant no. 16SV7527.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Container-based architecture for optimal face-recognition tasks in edge computing,"Tellez, Nadim (1); Jimeno, Miguel (1); Salazar, Augusto (1); Nino-Ruiz, Elias D. (1) ","(1) Systems Engineering Department, Universidad del Norte, Barranquilla, Colombia ","Proceedings of the 4th ACM/IEEE Symposium on Edge Computing, SEC 2019",ACM SIGMOBILE; IEEE Computer Society,"Association for Computing Machinery, Inc",,p 301-303,7-Nov-19,"Proceedings of the 4th ACM/IEEE Symposium on Edge Computing, SEC 2019",2019,,,,9.78145E+12,10.1145/3318216.3363323,,"4th ACM/IEEE Symposium on Edge Computing, SEC 2019","November 7, 2019 - November 9, 2019",,"Edge Computing has been proposed as an architecture for offering services to mobile devices or sensors. Modern multimedia applications such as face recognition, object and pose identification, and mobile augmented reality require cloud computing resources close to the mobile and sensors devices. Docker and containers have been proposed as a platform for edge computing and as a tool for efficient service handoff across edge computing servers. This paper presents a novel container-based Fog computing environment to improve resource usage for the architecture using optimization algorithms. © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6733-2/19/11…$15.00",9,Face recognition,Augmented reality - Computer architecture - Containers - Edge computing - Fog computing - Multimedia systems - Optimization,Computing environments - Docker - Mobile augmented reality - Multimedia applications - Optimization algorithms - Resource usage - Service handoff,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 921.5 Optimization Techniques",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Audio-augmented museum experiences using wearable visual-inertial odometry,"Yang, Jing (1); Sörös, Gábor (2) ","(1) Department of Computer Science, ETH Zurich, Switzerland (2) Nokia Bell Labs, Budapest, Hungary ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,10.1145/3365610.3365616,3365616,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"The auditory sense is an intuitive and immersive channel to experience our surroundings, which motivates us to augment our perception of the real world with digital auditory content. We present a wearable audio augmented reality prototype that tracks the user with six degrees of freedom in a known environment, synthesizes 3D sounds, and plays spatialized audio from arbitrary objects to the user. Our prototype is built using head-mounted visual-inertial odometry, a sound simulation engine on a laptop, and off-the-shelf headphones. We demonstrate an application in a gallery scenario in which visitors can hear objects and scenes drawn in the paintings, feeling audio-visually engaged in the depicted surroundings. In a user study involving 26 participants, we observed that the audio-enhanced exhibition improved people’s experience, as well as helped them remember more lively details of the artworks. © 2019 Association for Computing Machinery.",26,Audio acoustics,Augmented reality - Degrees of freedom (mechanics) - Exhibitions - Museums - Wearable computers,Arbitrary objects - Audio augmented reality - Known environments - Odometry - Six degrees of freedom - Spatialized audio - User experience - Wearable,"402.2 Public Buildings - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 751.1 Acoustic Waves - 931.1 Mechanics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Hearing is believing: Synthesizing spatial audio from everyday objects to users,"Yang, Jing (1); Frank, Yves (1); Soros, Gábor (1) ","(1) ETH, Zurich, Switzerland ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,11-Mar-19,"Proceedings of the 10th Augmented Human International Conference, AH 2019",2019,,,,9.78145E+12,10.1145/3311823.3311872,a28,"10th Augmented Human International Conference, AH 2019","March 11, 2019 - March 12, 2019",,"The ubiquity of wearable audio devices and the importance of the auditory sense imply great potential for audio augmented reality. In this work, we propose a concept and a prototype of synthesizing spatial sounds from arbitrary real objects to users in everyday interactions, whereby all sounds are rendered directly by the user's own ear pods instead of loudspeakers on the objects. The proposed system tracks the user and the objects in real time, creates a simplified model of the environment, and generates realistic 3D audio effects. We thoroughly evaluate the usability and the usefulness of such a system based on a user study with 21 participants. We also investigate how an acoustic environment model improves the sense of engagement of the rendered 3D sounds. © 2019 Association for Computing Machinery.",20,Audition,Augmented reality,Acoustic environment - Audio augmented reality - Audio devices - Auditory sense - Human-object interaction - Spatial audio - Spatial sound - System tracks,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Thermodule:wearable and modular thermal feedback system based on a wireless platform,"Maeda, Tomosuke (1); Kurahashi, Tetsuo (1) ","(1) Human Science Research Domain Nagakute, Toyota Central R and D Labs. Inc., Aichi, Japan ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,11-Mar-19,"Proceedings of the 10th Augmented Human International Conference, AH 2019",2019,,,,9.78145E+12,10.1145/3311823.3311826,a14,"10th Augmented Human International Conference, AH 2019","March 11, 2019 - March 12, 2019",,"Humans have specific sensory organs and they can feel tactile sensation on the whole body. However, many haptic devices have limitations due to the location of the body part and might not provide natural haptic feedback. Thus, we propose a novel interface, TherModule, which is a wearable and modular thermal feedback system for embodied interactions based on a wireless platform. TherModule can be worn on multiple body parts such as the wrist, forearm, ankle, and neck. In this paper, we describe the system concept, module implementation, and applications. To demonstrate and explore the embodied interaction with thermal feedback, we implemented prototype applications, such as movie experiences, projector-based augmented reality, navigation, and notification based on a wireless platform, with TherModule on multiple parts of the body. The result of an experiment on movie experience showed that participants felt more interactions between temperature and visual stimulus. ©2019 Copyright held by the owner/author(s).",34,Wearable computers,Augmented reality,Embodied interaction - Haptic feedbacks - Haptics - Projector-based augmented reality - Tactile sensation - Thermal feedback - Wearable - Wireless platform,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
PhD Forum: Towards an embedded stereo matching algorithm based on multiple correlation windows,"Palacios-Ramos, M.A. (1); Vázquez-Delgado, H.D. (1); Aguilar-González, Abiel (1); Pérez-Patricio, M. (1) ","(1) Instituto Tecnológico de Tuxtla Gutiérrez (ITTG), Carretera Panamericana Km. 1080, Tuxtla Gutiérrez, Chiapas; C.P. 29050, Apartado Postal: 599, Mexico ",ACM International Conference Proceeding Series,University of Trento,Association for Computing Machinery,,,9-Sep-19,ICDSC 2019 - 13th International Conference on Distributed Smart Cameras,2019,,,,9.78145E+12,10.1145/3349801.3357128,a26,"13th International Conference on Distributed Smart Cameras, ICDSC 2019","September 9, 2019 - September 11, 2019",,"Stereo matching consists in extracting 3D information from digital images, such as those obtained by a CCD camera. It is an important issue under several real world applications, such as positioning systems for mobile robots, augmented reality systems, etc. In previous works one of the most popular trend to address the stereo matching challenge is that compares scene information from two viewpoints (left-right) with an eppipolar geometry via correlation metrics. In regard to the correlation metrics, most previous works compute the similarity between pixels in the left image and pixels in the right image using a correlation index computed on neighborhoods of these pixels called correlation windows. Unfortunately, in order to preserve edges, small correlation windows need to be used, while, for homogeneous areas, large correlation windows are required. To address this problem, we lay down on the hypothesis that small correlation windows combined with large correlation windows should deliver accurate results under homogeneous areas while at the same time edges are preserved. To validate our hypothesis, in this paper a similarity criterion based on the grayscale homogeneity of the correlation window being processed is presented. Preliminary results are encourageous, validates our hypothesis and demonstrated the viability performance and scope of the proposed approach. © 2019 Association for Computing Machinery.",6,Stereo image processing,Augmented reality - CCD cameras - Pixels,Augmented reality systems - Disparity map - Local correlations - Multiple correlation - Positioning system - Similarity criteria - Stereo matching - Stereo matching algorithm,"714.2 Semiconductor Devices and Integrated Circuits - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Computer science replugged: What is the use of virtual reality in computer science education?,"Dengel, Andreas (1) ","(1) University of Würzburg, Germany ",ACM International Conference Proceeding Series,JP Morgan; SICSA; University of Glasgow,Association for Computing Machinery,,,23-Oct-19,"Proceedings of the 14th Workshop in Primary and Secondary Computing Education, WiPSCE 2019",2019,,,,9.78145E+12,10.1145/3361721.3362113,3362113,"14th Workshop in Primary and Secondary Computing Education, WiPSCE 2019","October 23, 2019 - October 25, 2019",,"By following the idea of not using computers at all, Computer Science Unplugged has set the course for many interactive, social, and hands-on activities dealing with concepts and problems of Computer Science Education. Through recent developments in immersive media, technologies like Virtual and Augmented Reality could enhance such activities or even enable new ones. When providing immersive educational media that induce a sense of presence in the virtual environment, the illusion of unmediated learning experience can be delivered. Hence, the concept of 'Computer Science Replugged' can benefit from the affordances that Computer Science Unplugged thrives on while facilitating or enabling activities that might be impossible, dangerous, or expensive to carry out in reality. In order to foster research and activities associated with the Computer Science Replugged approach, this paper concludes with research questions for using immersive media in Computer Science Education. © 2018 Association for Computing Machinery.",19,Engineering education,Augmented reality - E-learning - Education computing - Virtual reality,Computer Science Education - Hands-on activities - Immersive learning - Learning experiences - Research questions - Sense of presences - Technology enhanced learning - Virtual and augmented reality,"723 Computer Software, Data Handling and Applications - 901.2 Education",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Put that hologram there-probing mobile interaction experiences for a vision of mixed material public spaces,"Aslan, Ilhan (1); Dang, Chi Tai (1); Schlagowski, Ruben (1); Dietz, Michael (1); Brain, Fabian (1); André, Elisabeth (1) ","(1) Human-Centered Multimedia Lab, Augsburg University, Augsburg, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,22-Oct-19,"Proceedings of the 9th International Conference on the Internet of Things, IoT 2019",2019,,,,9.78145E+12,10.1145/3365871.3365877,a6,"9th International Conference on the Internet of Things, IoT 2019","October 22, 2019 - October 25, 2019",,"Contemporary progress in mobile augmented reality technologies has already introduced novel pervasive displays, such as Microsoft's HoloLens, which allows to display mixed realities containing tangible real and 'intangible' holographic objects. In order to allow prototyping and probing such seemingly near-future (urban) interaction experiences, we built a system, which combines a mobile phone with Microsoft's HoloLens and enables exemplary interaction techniques to manipulate 'holograms' by combining the capabilities of both these personal and mobile devices. In this paper, we first describe details of this system and then report on a study with 12 participants probing participants' experiences and expectations of a future urban mixed material space. Results of a thematic analysis highlight a two-sided view, in which despite some 'fears' of radical change, which may cause a disparity of what (materials) matter more, participants demonstrated a desire to benefit from material complementarity. © 2019 Association for Computing Machinery.",20,Mixed reality,Augmented reality - Holograms - Holographic displays - Internet of things,Interaction experiences - Interaction techniques - Materiality - Mobile augmented reality - Mobile HCI - Mobile interaction - Thematic analysis - User experience,"723 Computer Software, Data Handling and Applications - 743 Holography",,,"Number: -, Acronym: BMBF, Sponsor: Bundesministerium f&uuml;r Bildung und Forschung; Number: 01UO1820A, Acronym: -, Sponsor: Haridus- ja Teadusministeerium; ","This research was partly funded by the Bundesministerium f&uuml;r Bildung und Forschung (BMBF, Ministry of Education and Research) in the DIGISTA project (no. 01UO1820A).",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A Comparative Evaluation of Techniques for Sharing AR Experiences in Museums,"Franz, J. (1); Alnusayri, M. (1); Malloch, J. (1); Reilly, D. (1) ","(1) Dalhousie Univ., Halifax, NS, Canada ",Proceedings of the ACM on Human-Computer Interaction,,"ACM, USA","v 3, n CSCW",124 (20 pp.),Nov. 2019,,,,2573-0142,,,10.1145/3359226,,,,,"Museums are constantly searching for new ways to increase engagement with their exhibits, from electronic guides to modern digital technologies such as special-purpose tablets, smartphones, and virtual and augmented reality (AR). For AR exhibits in particular, promoting shared experience and group cohesion is not straightforward. In this work, we investigate scenarios in which not everyone is using a head-worn display (HWD), either because there aren't enough available or simply because someone might feel uncomfortable using it. We propose two sharing techniques for AR experiences and evaluate them in a long term in-the-wild study: Over-the-Shoulder AR, which renders a real-time virtual representation of the augmented reality content on a large secondary display; Semantic Linking, which displays contextual information about the virtual content on the same large display. We also introduce a complementary technique: Indicator Rings, which display the locations of the HWD user's objects-of-focus. We observed that participants in the Over-the-Shoulder AR and Semantic Linking conditions stayed together and exhibited more verbal exchanges than participants in a Baseline condition, which could indicate that they were more engaged. Self-reported measures indicated an increase in pair communication and increased comprehension of the virtual content for participants without the HWD. Participants without the HWD also displayed a greater understanding of the location of virtual elements with support from the Indicator Rings, and used them as a tool to guide the HWD user through the virtual content. We discuss design implications for interactive augmented reality exhibits and possible applications outside the cultural heritage scenario.",,,augmented reality - helmet mounted displays - history - museums - virtual reality,Indicator Rings - HWD user - Semantic Linking conditions - increased comprehension - virtual content - virtual elements - interactive augmented reality exhibits - AR experiences - museums - increase engagement - electronic guides - modern digital technologies - special-purpose tablets - virtual reality - AR exhibits - shared experience - group cohesion - head-worn display - sharing techniques - long term in-the-wild study - real-time virtual representation - augmented reality content - secondary display - complementary technique,C6130V Virtual reality - C6180 User interfaces - C7820 Humanities computing,,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Visual behavior during engagement with tangible and virtual representations of archaeological artifacts,"Ramkumar, Niveta (1); Fereydooni, Nadia (1); Shaer, Orit (2); Kun, Andrew L. (1) ","(1) University of New Hampshire, Durham; NH, United States (2) Wellesley College, Wellesley; MA, United States ","Proceedings - Pervasive Displays 2019 - 8th ACM International Symposium on Pervasive Displays, PerDis 2019",,"Association for Computing Machinery, Inc",,,12-Jun-19,"Proceedings - Pervasive Displays 2019 - 8th ACM International Symposium on Pervasive Displays, PerDis 2019",2019,,,,9.78145E+12,10.1145/3321335.3324930,a21,"8th ACM International Symposium on Pervasive Displays, PerDis 2019","June 12, 2019 - June 14, 2019",,"In this paper, we present results from a study of users' visual behavior while engaging with tangible and virtual representations of archaeological artifacts. We replicated and extended a recent study that introduced an augmented reality system implemented using HoloLens, for engaging with the artifacts. Our study goes beyond the original study to estimate the distribution of users' visual attention for both tangible and virtual representations of the artifacts. Our study confirmed the results of the original study in various aspects. Specifically, participants in both studies confirmed the immersive nature of the HoloLens condition and showed similar learning outcomes in terms of post-task open questions. Additionally, our findings indicate that users allocate their visual attention in similar ways when interacting with virtual and tangible learning material, in terms of total gaze duration, gaze on object duration, and object fixation duration. © 2019 ACM.",42,Behavioral research,Augmented reality - Eye tracking - Object recognition,Archaeological artifacts - Augmented reality systems - Gesture input - Human-centered computing - Learning materials - Mixed/augmented reality - Object based - Virtual representations,"723 Computer Software, Data Handling and Applications - 971 Social Sciences",,,"Number: OISE-1658594, Acronym: NSF, Sponsor: National Science Foundation; ",Thiswork was supported in part by the National Science Foundation through grant OISE-1658594.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Quadmetric optimized thumb-to-finger interaction for force assisted one-handed text entry on mobile headsets,"Lik Hang Lee (1); Kit Yung Lam (2); Tong Li (2); Braud, T. (2); Xiang Su (3); Pan Hui (4) ","(1) Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Oulu, Finland (2) Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Hong Kong, China (3) Dept. of Comput. Sci., Univ. of Helsinki, Helsinki, Finland (4) Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Helsinki, Finland ","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",,"ACM, USA","v 3, n 3",94 (27 pp.),Sept. 2019,,,,2474-9567,,,10.1145/3351252,,,,,"Augmented reality head-worn computers often feature small-sized touch interfaces that complicate interaction with content, provide insufficient space for comfortable text input, and can be awkward to use in social situations. This paper presents a novel one-handed thumb-to-finger text entry solution for augmented reality head-worn computers. We design a glove composed of 12 force-sensitive nodes featuring an ambiguous keyboard layout. We first explore the viability of force disambiguation to evaluate the force division within the force spectrum. We select a 3-level force division as it allows to considerably reduce the number of keys while featuring a high (83.9%) accuracy. Following this pilot study, we map the 26 English characters onto the 9 nodes located on the index, middle and ring fingers in a 3-3-3 configuration, and attribute the space, enter and backspace keys to the remaining three nodes. We consider text entry performance as a quadmetric optimization problem considering the following criteria: goodness of character pairs, layout similarity to the QWERTY keyboard, easiness of force interaction, and comfort level of thumb reach. The resulting layout strikes a balance between performance and usability. We finally evaluate the quadmetric optimized layout over 6 sessions with 12 participants. The participants achieve an average text entry rate of 6.47 WPM with 6.85% error rate in the final session, which is significantly faster than existing thumb-to-finger solutions. In addition, our one-handed text entry system enhances the user mobility compared to other state-of-the-art solutions by freeing one hand, while allowing the user to direct his visual attention to other activities.",,,augmented reality - ergonomics - human computer interaction - keyboards - mobile computing - touch sensitive screens,quadmetric optimization problem - layout similarity - force interaction - quadmetric optimized layout - average text entry rate - text entry system - thumb-to-finger interaction - mobile headsets - augmented reality head-worn computers - small-sized touch interfaces - thumb-to-finger text entry solution - force disambiguation - force spectrum - 3-level force division - text entry performance - force-sensitive nodes - QWERTY keyboard,"C6190V Mobile, ubiquitous and pervasive computing - C6130V Virtual reality - C6180 User interfaces - C0240 Ergonomic aspects of computing - C5540B Interactive-input devices",G06F3/00 - G06F3/02 - G06F9/44,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Wirtinger holography for near-eye displays,"Chakravarthula, P. (1); Yifan Peng (2); Kollin, J. (3); Fuchs, H. (1); Heide, F. (4) ","(1) Univ. of North Carolina at Chapel Hill, Chapel Hill, NC, United States (2) Stanford Univ., Stanford, CA, United States (3) Microsoft Res., Redmond, WA, United States (4) Princeton Univ., Princeton, NJ, United States ",ACM Transactions on Graphics,,"ACM, USA","v 38, n 6",213 (13 pp.),Nov. 2019,,,,0730-0301,,,10.1145/3355089.3356539,,,,,"Near-eye displays using holographic projection are emerging as an exciting display approach for virtual and augmented reality at high-resolution without complex optical setups --- shifting optical complexity to computation. While precise phase modulation hardware is becoming available, phase retrieval algorithms are still in their infancy, and holographic display approaches resort to heuristic encoding methods or iterative methods relying on various relaxations. In this work, we depart from such existing approximations and solve the phase retrieval problem for a hologram of a scene at a single depth at a given time by revisiting complex Wirtinger derivatives, also extending our framework to render 3D volumetric scenes. Using Wirtinger derivatives allows us to pose the phase retrieval problem as a quadratic problem which can be minimized with first-order optimization methods. The proposed Wirtinger Holography is flexible and facilitates the use of different loss functions, including learned perceptual losses parametrized by deep neural networks, as well as stochastic optimization methods. We validate this framework by demonstrating holographic reconstructions with an order of magnitude lower error, both in simulation and on an experimental hardware prototype.",,,augmented reality - holographic displays - holography - image reconstruction - iterative methods - learning (artificial intelligence) - neural nets - optimisation - phase modulation - rendering (computer graphics),Wirtinger holography - near-eye displays - holographic projection - exciting display approach - virtual reality - augmented reality - complex optical setups - optical complexity - precise phase modulation hardware - phase retrieval algorithms - holographic display - heuristic encoding methods - iterative methods - phase retrieval problem - complex Wirtinger derivatives - quadratic problem - first-order optimization methods - Wirtinger Holography - stochastic optimization methods - holographic reconstructions - 3D volumetric scenes rendering,"B4350 Holography - B6135 Optical, image and video signal processing - B0260 Optimisation techniques - B0290F Interpolation and function approximation (numerical analysis) - C6130B Graphics techniques - C6130V Virtual reality - C1180 Optimisation techniques - C4130 Interpolation and function approximation (numerical analysis) - C5260B Computer vision and image processing techniques - C5290 Neural computing techniques",G03H - G06T - G06N20/00,Practical (PRA); Theoretical or Mathematical (THR),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
iMapper: interaction-guided scene mapping from monocular videos,"Monszpart, A.; Guerrero, P.; Ceylan, D.; Yumer, E.; Mitra, N.J. ",,ACM Transactions on Graphics,,"ACM, USA","v 38, n 4",92 (15 pp.),Jul-19,,,,0730-0301,,,10.1145/3306346.3322961,,,,,"Next generation smart and augmented reality systems demand a computational understanding of monocular footage that captures humans in physical spaces to reveal plausible object arrangements and human-object interactions. Despite recent advances, both in scene layout and human motion analysis, the above setting remains challenging to analyze due to regular occlusions that occur between objects and human motions. We observe that the interaction between object arrangements and human actions is often strongly correlated, and hence can be used to help recover from these occlusions. We present iMapper, a data-driven method to identify such human-object interactions and utilize them to infer layouts of occluded objects. Starting from a monocular video with detected 2D human joint positions that are potentially noisy and occluded, we first introduce the notion of interaction-saliency as space-time snapshots where informative human-object interactions happen. Then, we propose a global optimization to retrieve and fit interactions from a database to the detected salient interactions in order to best explain the input video. We extensively evaluate the approach, both quantitatively against manually annotated ground truth and through a user study, and demonstrate that iMapper produces plausible scene layouts for scenes with medium to heavy occlusion. Code and data are available on the project page.",,,augmented reality - image motion analysis - image sequences - learning (artificial intelligence) - object detection - optimisation - spatiotemporal phenomena - video signal processing,iMapper - interaction-guided scene mapping - monocular video - augmented reality systems - plausible object arrangements - human-object interactions - human motion analysis - human actions - occluded objects - detected 2D human joint positions - interaction-saliency - detected salient interactions - plausible scene layouts - data-driven method,"B6135 Optical, image and video signal processing - B0260 Optimisation techniques - C5260D Video signal processing - C6130V Virtual reality - C6170K Knowledge engineering techniques - C1180 Optimisation techniques - C5260B Computer vision and image processing techniques",G06T - G06T7/20 - G06N5/04 - G06N20/00,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Stochastic Optimization for Green Multimedia Services in Dense 5G Networks,"Tengfei Cao (1); Changqiao Xu (1); Mu Wang (1); Zhongbai Jiang (1); Xingyan Chen (1); Lujie Zhong (2); Grieco, L.A. (3) ","(1) Beijing Univ. of Posts & Telecommun., Beijing, China (2) Capital Normal Univ., Beijing, China (3) Politec. di Bari, Bari, Italy ","ACM Transactions on Multimedia Computing, Communications and Applications",,"ACM, USA","v 15, n 3",79 (22 pp.),Sept. 2019,,,,1551-6857,,,10.1145/3328996,,,,,"The manyfold capacity magnification promised by dense 5G networks will make possible the provisioning of broadband multimedia services, including virtual reality, augmented reality, and mobile immersive video, to name a few. These new applications will coexist with classic ones and contribute to the exponential growth of multimedia services in mobile networks. At the same time, the different requirements of past and old services pose new challenges to the effective usage of 5G resources. In response to these challenges, a novel Stochastic Optimization framework for Green Multimedia Services named SOGMS is proposed herein that targets the maximization of system throughput and the minimization of energy consumption in data delivery. In particular, Lyapunov optimization is leveraged to face this optimization objective, which is formulated and decomposed into three tractable subproblems. For each subproblem, a distinct algorithm is conceived, namely quality of experience--based admission control, cooperative resource allocation, and multimedia services scheduling. Finally, extensive simulations are carried out to evaluate the proposed method against state-of-art solutions in dense 5G networks.",,,augmented reality - mobile computing - multimedia communication - optimisation - quality of service - resource allocation - scheduling - telecommunication congestion control - virtual reality,virtual reality - augmented reality - mobile immersive video - mobile networks - old services - novel Stochastic Optimization framework - Green Multimedia Services - Lyapunov optimization - optimization objective - multimedia services scheduling - dense 5G networks - manyfold capacity magnification - broadband multimedia services,"B6250F Mobile radio systems - B0260 Optimisation techniques - B6210R Multimedia communications - C1180 Optimisation techniques - C6130V Virtual reality - C6190V Mobile, ubiquitous and pervasive computing",H04N - G06F9/44 - H04B7/00 - H04B7/26 - H04W,Practical (PRA); Theoretical or Mathematical (THR),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Security and Privacy Approaches in Mixed Reality: A Literature Survey,"De Guzman, J.A. (1); Thilakarathna, K. (2); Seneviratne, A. (3) ","(1) Univ. of New South Wales, Sydney, NSW, Australia (2) Univ. of Sydney, Sydney, NSW, Australia (3) Univ. of New South Wales, Sydney, NSW, Australia ",ACM Computing Surveys,,"ACM, USA","v 52, n 6",110 (37 pp.),Dec. 2019,,,,0360-0300,,,10.1145/3359626,,,,,"Mixed reality (MR) technology development is now gaining momentum due to advances in computer vision, sensor fusion, and realistic display technologies. With most of the research and development focused on delivering the promise of MR, the privacy and security implications of this technology are yet to be thoroughly investigated. This survey article aims to put in to light these risks and to look into the latest security and privacy work on MR. Specifically, we list and review the different protection approaches that have been proposed to ensure user and data security and privacy in MR. We extend the scope to include work on related technologies such as augmented reality, virtual reality, and human-computer interaction as crucial components, if not the origins, of MR, as well as numerous related work from the larger area of mobile devices, wearables, and Internet-of-Things. We highlight the lack of investigation, implementation, and evaluation of data protection approaches in MR. Further challenges and directions on MR security and privacy are also discussed.",,,augmented reality - data privacy - Internet - security of data - sensor fusion - virtual reality,mixed reality - technology development - computer vision - sensor fusion - realistic display technologies - research - security implications - survey article - latest security - privacy work - MR - different protection approaches - data security - augmented reality - virtual reality - human-computer interaction - numerous related work - data protection approaches,B7260 Display technology - C6130S Data security - C6130V Virtual reality - E1410 Ergonomics,G06F21/00 - H04N5/66,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Design and Evaluation of DIO Construction Toolkit for Co-making Shared Constructions,"Arora, J. (1); Mathur, K. (1); Goel, M. (1); Kumar, P. (1); Mishra, A. (1); Parnami, A. (1) ","(1) Weave Lab., IIIT-Delhi, Delhi, India ","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",,"ACM, USA","v 3, n 4",127 (25 pp.),Dec. 2019,,,,2474-9567,,,10.1145/3369833,,,,,"We present the design and implementation of DIO, a novel digital-physical construction toolkit to enable constructionist learning for children from age group 8-12 years. The toolkit comprises of dome-shaped (D) tangible modules with various attachments that allow suspension on the body of multiple children and/or in the environment to support a variety of sensing/input (I), actuation/output (O) functionalities. The modules are enabled for wireless communication and can be linked together using an Augmented Reality based programming interface running on a smartphone. The smartphone recognizes our hemispherical modules omnidirectionally through novel computer vision based 3D patterns; custom made to provide logical as well as semantic encoding. In this paper, we show how, owing to its unique form-factor, the toolkit enables multi-user constructions for the children and offers a shared learning experience. We further reflect on our learning from a one-year long iterative design process and contribute a social scaffolding based procedure to engage children with such constructionist toolkits effectively.",,,augmented reality - civil engineering computing - computer aided instruction - computer vision - construction - mobile computing - user interfaces,dome-shaped tangible modules - multiple children - wireless communication - smartphone - hemispherical modules - unique form-factor - multiuser constructions - learning experience - one-year long iterative design process - social scaffolding based procedure - constructionist toolkits - DIO construction toolkit - constructionist learning - co-making shared constructions - digital-physical construction toolkit - augmented reality based programming interface - computer vision based 3D patterns - age 8.0 year to 12.0 year,"C7810C Computer-aided instruction - C6190V Mobile, ubiquitous and pervasive computing - C5260B Computer vision and image processing techniques - C6130V Virtual reality - C6180 User interfaces",E04 - G06F9/44 - G06T - G09B5/00,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Navigation graph for tiled media streaming,"Park, Jounsup (1); Nahrstedt, Klara (1) ","(1) University of Illinois at Urbana-Champaign, Urbana; IL, United States ",MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,ACM SIGMM,"Association for Computing Machinery, Inc",,p 447-455,15-Oct-19,MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,2019,,,,9.78145E+12,10.1145/3343031.3351021,,"27th ACM International Conference on Multimedia, MM 2019","October 21, 2019 - October 25, 2019",,"After the emergence of video streaming services, more creative and diverse multimedia content has become available, and now the capability of streaming 360-degree videos will open a new era of multimedia experiences. However, streaming these videos requires larger bandwidth and less latency than what is found in conventional video streaming systems. Rate adaptation of tiled videos and view prediction techniques are used to solve this problem. In this paper, we introduce the Navigation Graph, which models viewing behaviors in the temporal (segments) and the spatial (tiles) domains to perform the rate adaptation of tiled media associated with the view prediction. The Navigation Graph allows clients to perform view prediction more easily by sharing the viewing model in the same way in which media description information is shared in DASH. It is also useful for encoding the trajectory information in the media description file, which could also allow for more efficient navigation of 360-degree videos. This paper provides information about the creation of the Navigation Graph and its uses. The performance evaluation shows that the Navigation Graph based view prediction and rate adaptation outperform other existing tiled media streaming solutions. Navigation Graph is not limited to 360-degree video streaming applications, but it can also be applied to other tiled media streaming systems, such as volumetric media streaming for augmented reality applications. © 2019 Association for Computing Machinery.",22,Navigation,Augmented reality - Forecasting - Graphic methods - Multimedia services - Multimedia systems - Video streaming,Augmented reality applications - Description information - Media streaming systems - Rate adaptation - Trajectory information - Video Streaming Applications - Video streaming services - View predictions,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Xploro: Multi-user AR,"Ebeling, Chris (1); Skinner, Benjamin (1); Bluff, Andrew (1) ","(1) Animal Logic Academy, University of Technology Sydney, Australia ","ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3305365.3329724,a10,"ACM SIGGRAPH 2019 Appy Hour - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,Xploro is an educational game for iOS which combines augmentedreality (AR) technology and spatial computing multi-user game-play mechanics. It was created by the UTS Animal Logic Academy (UTSALA) 2018 cohort to educate children aged 8-12 in a fun and social way. Xploro uses emerging augmented reality technology to create the hide and seek fun of 'Wheres Wally' alongside educational aspects similar to 'Carmen Sandiego'. © 2019 Copyright Held by the Owner/Author(s).,2,Interactive computer graphics,Augmented reality - Computation theory - Computer games - iOS (operating system),Augmented reality technology - Educational aspects - Educational game - Mobile gaming - Multi-user - Multi-user games - Real time - Spatial computing,"721.1 Computer Theory, Includes Formal Logic, Automata Theory, Switching Theory, Programming Theory - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Tango of edge and cloud execution for reliability,"Suryavansh, Shikhar (1); Bothra, Chandan (1); Chiang, Mung (1); Peng, Chunyi (1); Bagchi, Saurabh (1) ","(1) Purdue University, United States ","MECC 2019 - Proceedings of the 2019 4th Workshop on Middleware for Edge Clouds and Cloudlets, Part of Middleware 2019",ACM,"Association for Computing Machinery, Inc",,p 10-15,9-Dec-19,"MECC 2019 - Proceedings of the 2019 4th Workshop on Middleware for Edge Clouds and Cloudlets, Part of Middleware 2019",2019,,,,9.78145E+12,10.1145/3366614.3368103,,"4th Workshop on Middleware for Edge Clouds and Cloudlets, MECC 2019 - Part of Middleware 2019","December 9, 2019 - December 13, 2019",,"The increasing cost of cloud services and the need for decentralization of servers has led to a rise of interest in Edge computing. Edge computing brings the servers closer to the end users which helps in reducing network latency. However, the presence of multiple edge servers adversely affects the reliability due to difficulty in maintenance of heterogeneous servers. This paper aims at evaluating the performance of various server configuration models in edge computing using EdgeCloudSim, a popular simulator for edge computing. The performance is evaluated in terms of service time and percentage of failed tasks for an Augmented Reality application. We evaluated the performance of the following edge computing models, Exclusive: Mobile only, Edge only, Cloud only; and Hybrid: Edge & Cloud hybrid with load-balancing on the Edge, and Mobile & Edge hybrid. We analyzed the impact of variation of different parameters such as WAN bandwidth, cost of cloud resources, heterogeneity of edge servers, etc., on the performance of the edge computing models. We show that due to variation in the above parameters, the exclusive models are not sufficient for computational requirements and there is a need for hybrid edge computing models. © 2019 Association for Computing Machinery.",19,Parameter estimation,Augmented reality - Cloud computing - Edge computing - Middleware - Reliability,Augmented reality applications - Computational requirements - Configuration model - Heterogeneous servers - Hybrid model - Network latencies - Performance Evaluation - Terms of services,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.1 Computer Programming",,,"Number: CNS-1527262, Acronym: NSF, Sponsor: National Science Foundation; Number: -, Acronym: -, Sponsor: Northrop Grumman; ","This material is based in part upon work supported by the National Science Foundation under Grant Numbers CNS-1527262 and CNS-1718637 and by Northrop Grumman Corporation through their NG Cybersecurity Research Consortium. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Enhancing the AR experience with machine learning services,"Englert, Michael (1); Klomann, Marcel (1); Weber, Kai (1); Grimm, Paul (1); Jung, Yvonne (1) ","(1) Fulda University of Applied Sciences, Fulda, Germany ",Proceedings - Web3D 2019: 24th International ACM Conference on 3D Web Technology,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,26-Jul-19,Proceedings - Web3D 2019: 24th International ACM Conference on 3D Web Technology,2019,,,,9.78145E+12,10.1145/3329714.3338134,,"24th International ACM Conference on 3D Web Technology, Web3D 2019","July 26, 2019 - July 28, 2019",,"In this paper, we present and evaluate a web service that offers cloud-based machine learning services to improve Augmented Reality applications on mobile and web clients with special regards to tracking quality and registration of complex scenes that require an application-specific coordinate frame. Specifically, our service aims at reducing camera drift that still occurs in modern AR frameworks as well as helps with the initial camera alignment in a known scene by estimating the absolute camera pose using a configurable context-based image segmentation in combination with an adaptive image classification. We demonstrate real-world applications that utilize our web service and evaluate the performance and accuracy of the underlying image segmentation and the camera pose estimation. We also discuss the initial configuration along with the semi-automatic process of generating training data, and the training of the machine learning models for the corresponding tasks. © 2019 Association for Computing Machinery.",27,Web services,Augmented reality - Cameras - Computer vision - Image segmentation - Learning systems - Machine learning - Mixed reality - Quality control - Surface discharges - Websites,Application specific - Augmented reality applications - Camera pose estimation - Initial configuration - Machine learning models - Mobile mixed realities - Tracking quality - Training data,"701.1 Electricity: Basic Concepts and Phenomena - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 742.2 Photographic Equipment - 913.3 Quality Assurance and Control",,,"Number: -, Acronym: BMBF, Sponsor: Bundesministerium f&Atilde;&frac14;r Bildung und Forschung; ",This work was carried out in the project NetFlinCS and was funded by the German Federal Ministry of Education and Research (BMBF).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Introduction to IATK: An immersive visual analytics toolkit,"Cordeil, Maxime (1); Dwyer, Tim (1) ","(1) Monash University, Caulfield East, Australia ",ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 431-435,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,10.1145/3343055.3361927,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"Immersive Analytics [3, 6, 1] is an emerging interdisciplinary research area that investigates the use of nontraditional display and input technology to immerse users in their data. A prominent aspect this research investigates is 'immersive data visualization' [5, 2, 9, 8], which uses augmented and virtual reality technology to visually immerse users in their data to facilitate collaboration, exploration and understanding. Currently, there is a lack of simple tools to build interactive data visualizations in immersive environments. The de facto approach is to use off-the-shelf game engines because they allow easy prototyping of 3D user interfaces in these immersive environments. However, game engines do not consider the specialized requirements of data visualization, such as the type and structure of datasets, the breadth of data, especially in the age of 'big data', and typical information visualization tasks. IATK: Immersive Analytics Toolkit is an open source visualization toolkit for the Unity game engine that fills this gap. Specifically, IATK: • supports an infovis pipeline for virtual and augmented reality environments; • visualizes large (up to 1 million) data points at an optimal framerate for immersive applications; and, • provides a technology-agnostic model for user interactions with immersive visualizations. This tutorial will introduce participants to the Unity game engine and teach practical skills for implementing immersive data visualisations using IATK. © Copyright is held by the author/owner(s).",10,Data visualization,Augmented reality - Information systems - Large dataset - User interfaces - Virtual reality - Visualization,Augmented and virtual realities - Immersive - Immersive visualization - Information visualization - Interdisciplinary research - Toolkit - Virtual and augmented reality - Visualization toolkits,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 903.2 Information Dissemination",,,"Number: DP180100755, Acronym: -, Sponsor: -; ",This research was supported under the Australian Research Council&rsquo;s Discovery Projects funding scheme (DP180100755).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
From lab to field: Demonstrating mixed reality prototypes for augmented sports experiences,"Lo, Wei Hong (1); Zollmann, Stefanie (1); Regenbrecht, Holger (1); Loos, Moritz (1) ","(1) University of Otago, New Zealand ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365728,a62,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"Traditional sports events related data have no direct spatial relationship to what spectators see when attending a live sports event. The idea of our work is to address this gap and ultimately to provide spectators insights of a sports game by embedding sports statistics into their field of view of the game using mobile Augmented Reality. Research in the area of live sport events comes with several challenges such as tracking and visualisation challenges as well as with the challenge that there are only limited opportunities to test and study new features during live games on-site. In this work, we developed a set of prototypes that allow for researching dedicated features for an AR sports spectator experience off-site in the lab before testing them live on the field. © 2019 Association for Computing Machinery.",5,Mixed reality,Augmented reality - Interactive computer graphics - Recreational facilities - Sports,Field of views - Mobile augmented reality - Off sites - Prototype designs - Spatial relationships - Sport events - Sports events - Sports statistics,"403 Urban and Regional Planning and Development - 461.3 Biomechanics, Bionics and Biomimetics - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Pupil diameter as a measure of emotion and sickness in VR,"John, Brendan (1) ","(1) Department of Computer and Information Science and Engineering, University of Florida, United States ",Eye Tracking Research and Applications Symposium (ETRA),ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,25-Jun-19,Proceedings - ETRA 2019: 2019 ACM Symposium On Eye Tracking Research and Applications,2019,,,,9.78145E+12,10.1145/3314111.3322868,a49,"11th ACM Symposium on Eye Tracking Research and Applications, ETRA 2019","June 25, 2019 - June 28, 2019",,"Eye tracking is rapidly becoming popular in consumer technology, including virtual and augmented reality. Eye trackers commonly provide an estimate of gaze location, and pupil diameter. Pupil diameter is useful for interactive systems, as it provides means to estimate cognitive load, stress, and emotional state. However, there are several roadblocks that limit the use of pupil diameter. In VR HMDs there are a lack of models that account for stereoscopic viewing and the increased brightness of near eye displays. Existing work has shown correlations between pupil diameter and emotion, but have not been extended to VR environments. The scope of this work is to bridge the gap between existing research on emotion and pupil diameter to VR, while also attempting to use pupillary data to tackle the problem of simulator sickness in VR. © 2019 Copyright held by the owner/author(s).",23,Eye tracking,Augmented reality - Cognitive systems - Diseases - Stereo image processing - Virtual reality,Arousal classifications - Cognitive loads - Interactive system - Pupil diameter - Pupillary Light Reflex - Simulator sickness - Stereoscopic viewing - Virtual and augmented reality,"723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Power-efficient and shift-robust eye-tracking sensor for portable VR headsets,"Katrychuk, Dmytro (1); Griffith, Henry K. (1); Komogortsev, Oleg V. (1) ","(1) Department of Computer Science, Texas State University, San Marcos; TX, United States ",Eye Tracking Research and Applications Symposium (ETRA),ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,25-Jun-19,Proceedings - ETRA 2019: 2019 ACM Symposium On Eye Tracking Research and Applications,2019,,,,9.78145E+12,10.1145/3314111.3319821,a19,"11th ACM Symposium on Eye Tracking Research and Applications, ETRA 2019","June 25, 2019 - June 28, 2019",,"Photosensor oculography (PSOG) is a promising solution for reducing the computational requirements of eye tracking sensors in wireless virtual and augmented reality platforms. This paper proposes a novel machine learning-based solution for addressing the known performance degradation of PSOG devices in the presence of sensor shifts. Namely, we introduce a convolutional neural network model capable of providing shift-robust end-to-end gaze estimates from the PSOG array output. Moreover, we propose a transfer-learning strategy for reducing model training time. Using a simulated workflow with improved realism, we show that the proposed convolutional model offers improved accuracy over a previously considered multilayer perceptron approach. In addition, we demonstrate that the transfer of initialization weights from pre-trained models can substantially reduce training time for new users. In the end, we provide the discussion regarding the design trade-offs between accuracy, training time, and power consumption among the considered models. © 2019 Association for Computing Machinery.",15,Eye tracking,Augmented reality - Convolution - Economic and social effects - Learning systems - Machine learning - Neural networks - Optical sensors - Virtual reality,Computational requirements - Convolutional model - Convolutional neural network - Eye-tracking sensors - Performance degradation - Photo-sensors - PSOG - Virtual and augmented reality,"716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications - 741.3 Optical Devices and Systems - 971 Social Sciences",,,"Number: CNS-1250718, Acronym: NSF, Sponsor: National Science Foundation; ","We are thankful to Dr. Lee Friedman for his help with statistical analysis, and to Dr. Evgeny Abdulin for creating the hardware used in this study. This work is supported by the National Science Foundation under Grant Nos: CNS-1250718 and CNS-1714623. The work is inspired by the Google Virtual Reality Research Award and Google Global Faculty Research Award bestowed on Dr. Komogortsev in 2017 and 2019, respectively.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Video: Banner – An image sensor reconfiguration framework for seamless resolution-based tradeoffs,"Hu, Jinhan (1); Shearer, Alexander (1); Rajagopalan, Saranya (1); LiKamWa, Robert (1) ","(1) Arizona State University, Tempe; AZ, United States ","MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 705-706,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,10.1145/3307334.3328594,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,"The high energy consumption of visual sensing continues to impede the future of mobile vision in which devices will continuously compute visual information from sensory data, e.g,. for visual personal assistants or for augmented reality (AR). While vision algorithms continue to improve in task accuracy and speed, mobile and wearable vision systems fail to achieve sufficient battery life when vision tasks are continuously running. Continuous video capture drains the battery of Google Glass in 30 minutes [2]. It is well known that a common culprit is the energy-expensive traffic of image data [8, 9]. Transferring high resolutions at high frame rates draws substantial power consumption from the analog-digital conversion, the sensor interface transactions, and the memory usage. Simply capturing 1080p frames at 30 frames per second consumes more than 2.4 W of system power measured on a MOTO Z smartphone. However, capturing and displaying 480p frames only consumes 1.3 W of system power. Image resolution can create an interesting tradeoff for visual tasks: low resolution promotes low energy consumption, while high resolution promotes high imaging fidelity for high visual task accuracy. For example, as we explore with our AR marker-based pose estimation case study, lower resolutions suffice when an AR marker is close, but high resolutions are needed when the AR marker is far away or small. This tradeoff has been explored by several visual computing system works including marker pose estimation, object detection, and face recognition [3–6, 9, 10, 13, 14, 17]. We too advocate that mobile vision systems should be able to benefit from the ability to situationally sacrifice image resolution to save system energy when imaging detail is unnecessary. Unfortunately, any change in sensor resolution leads to a substantial pause in frame delivery. This is illustrated in Figure 1a. We measure that reconfiguring sensor resolution in the Android OS prevents the application from receiving frames for about 267 ms, the equivalent of dropping 9 frames (working at 30 FPS) from vision processing pipelines [6]. Consequently, computer vision applications don’t change resolutions at runtime, despite the significant energy savings at lower resolutions. For example, Augmented Reality applications such as 'Augment' and 'UnifiedAR' constantly work at 1080p, drawing 2.7 W of system power. © 2019 Copyright held by the owner/author(s).",17,Green computing,Augmented reality - Computer operating systems - Electric batteries - Energy efficiency - Energy utilization - Face recognition - Image resolution - Object detection - Vision,Augmented reality applications - Computer vision applications - Device Driver - High energy consumption - Low energy consumption - Reconfiguration - Sensor reconfigurations - Visual computing,"525.2 Energy Conservation - 525.3 Energy Utilization - 702.1 Electric Batteries - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
How important is immersion for learning in computer science replugged games?,"Dengel, Andreas (1) ","(1) University of Wurzburg, Wurzburg, Germany ","Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE","Association for Computing Machinery, Special Interest Group on Computer Science Education (ACM SIGCSE)",Association for Computing Machinery,,p 1165-1171,26-Feb-20,SIGCSE 2020 - Proceedings of the 51st ACM Technical Symposium on Computer Science Education,2020,,1942647X,,9.78145E+12,10.1145/3328778.3366837,,"51st ACM SIGCSE Technical Symposium on Computer Science Education, SIGCSE 2020","March 11, 2020 - March 14, 2020",,"By following the idea of not using computers at all, Computer Science Unplugged has set the course for many interactive, social, and hands-on activities dealing with concepts and problems of Computer Science Education. Through recent developments in immersive media, technologies like Virtual and Augmented Reality could enhance such activities or even enable new ones. When providing immersive educational media that induce a sense of presence in the virtual environment, the illusion of unmediated learning experience can be delivered. Hence, the concept of Computer Science Replugged can benefit from the affordances that Computer Science Unplugged thrives on while facilitating or enabling activities that might be impossible, dangerous, or expensive to carry out in reality. This paper presents three concepts from Computer Science Education that have been modeled as 3-D immersive educational virtual environments: components of a computer, asymmetric encryption/decryption, and finite state machines. To get a first impression of the effectiveness of these approaches and in order to determine the importance of the level of immersion for the learning process, a study with 78 participants was conducted in which the software was tested on different devices. All activities were found to be significantly effective with regards to the pre- and post-tests. When analyzing these results on the basis of comparing the least immersive setting (laptop) with the most immersive setting (headmounted- display), the findings indicate different effects with effect sizes between = -.17 and = .41. This raises two questions: Which topics from Computer Science Education can benefit from immersive technology? and What are the opportunities and challenges of the didactical design of Computer Science Replugged activities? that have to be adressed in further research. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",16,Computer games,Augmented reality - Cryptography - E-learning - Education computing - Engineering education - Social computing - Software testing - Virtual reality,Asymmetric encryption - Computer Science Education - Educational virtual environments - Head mounted displays - Immersive learning - Immersive technologies - Technology enhanced learning - Virtual and augmented reality,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 901.2 Education",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Enabling seamless container migration in edge platforms,"Elgazar, Ali E. (1); Harras, Khaled A. (1) ","(1) Carnegie Mellon University, United States ","Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM",ACM SIGMOBILE,Association for Computing Machinery,,p 1-6,7-Oct-19,"CHANTS 2019 - Proceedings of the 14th Workshop on Challenged Networks, co-located with MobiCom 2019",2019,,,,9.78145E+12,10.1145/3349625.3355438,,"14th Workshop on Challenged Networks, CHANTS 2019, co-located with MobiCom 2019",25-Oct-19,,"Fog and cloud computing are becoming increasingly popular as computational offloading platforms. However, many areas throughout the world either do not have a stable enough network connection to effectively use cloud computing, or do not have the required infrastructure or resources to set up fog nodes. Moreover, many application categories such as cognitive assistance and augmented reality applications require single digit latency, which both cloud and fog cannot provide. As such, these areas can benefit tremendously from edge computing. To ensure single digit latency, Edge nodes are typically other co-located devices within single-hop proximity. Having said that, due to user mobility, single edge devices set up at home will not provide the required latency if the user is on the move. Therefore, we introduce Teddybear, a Docker based system that seamlessly and efficiently migrates server containers between edge computing platforms by utilizing both the Internet and the user’s mobile device as a carrier for the container. We show how Teddybear can continue to provide ultra-low latency services to users on the move, with minimal service downtime as they change locations. © 2019 Association for Computing Machinery.",20,Containers,Augmented reality - Edge computing - Fog - Maintenance - Mobile computing,Augmented reality applications - Cognitive assistance - Computing platform - EDGE architectures - Edge nodes - Low latency - Network connection - User mobility,"443.1 Atmospheric Properties - 716 Telecommunication; Radar, Radio and Television - 723 Computer Software, Data Handling and Applications - 913.5 Maintenance",,,"Number: -, Acronym: QNRF, Sponsor: Qatar National Research Fund; Number: -, Acronym: QF, Sponsor: Qatar Foundation; ",This publication was made possible by NPRP grant # 8-1645-1-289 from the Qatar National Research Fund (a member of Qatar Foundation). The findings achieved herein are solely the responsibility of the authors.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Projection-Based Augmented Reality Robot Prototype with Human-Awareness,"Ro, Hyocheol (1); Byun, Jung-Hyun (1); Kim, Inhwan (1); Park, Yoon Jung (1); Kim, Kyuri (1); Han, Tack-Don (1) ","(1) Yonsei University, Media System Lab, Seoul, Korea, Republic of ",ACM/IEEE International Conference on Human-Robot Interaction,ACM; ACM SIGAI; ACM SIGCHI; IEEE; IEEE Robotics and Automation Society,IEEE Computer Society,v 2019-March,p 598-599,22-Mar-19,HRI 2019 - 14th ACM/IEEE International Conference on Human-Robot Interaction,2019,,,21672148,9.78154E+12,10.1109/HRI.2019.8673173,8673173,"14th Annual ACM/IEEE International Conference on Human-Robot Interaction, HRI 2019","March 11, 2019 - March 14, 2019",,"Since projection augmented reality (AR) robot can provide a lot of information through projector, it can be useful in museums and art galleries that need to provide information to the crowd. Therefore, it is necessary to continue to interact with people, and human-aware path planning is also needed. We prototyped projection AR mobile robot implemented human-aware path planning and wrote about future research direction. © 2019 IEEE.",4,Human robot interaction,Augmented reality - Man machine systems - Mobile robots - Motion planning - Robot programming,Art gallery - Future research directions - Human-aware - Robot prototypes - Service robots,"723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,"Number: -, Acronym: NRF, Sponsor: National Research Foundation of Korea; Number: -, Acronym: MSIP, Sponsor: Ministry of Science, ICT and Future Planning; Number: -, Acronym: NRF, Sponsor: National Research Foundation of Korea; ",ACKNOWLEDGEMENTS This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIP) (No.NRF-2018R1A2A1A05078628).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality simulation toward improving therapeutic healthcare communication techniques,"Bonham, Matthew (1) ","(1) University of Delaware, United States ","International Conference on Intelligent User Interfaces, Proceedings IUI",ACM Special Interest Group on Artificial Intelligence (SIGAI); ACM Special Interest Group on Computer-Human Interaction (SIGCHI),Association for Computing Machinery,,p 161-162,16-Mar-19,"Proceedings of the 24th International Conference on Intelligent User Interfaces, IUI 2019",2019,,,,9.78145E+12,10.1145/3308557.3308726,,"24th International Conference on Intelligent User Interfaces, IUI 2019","March 16, 2019 - March 20, 2019",,"There is a lack of diversity in the simulations or training, that nursing students get to experience before making their way to a real hospital bedside. There is existing evidence and practical concern for nurses' ability to communicate therapeutically, such as calming and de-escalation techniques, when interacting with a diverse clientele, specifically in the way that they interact with patients. We look at leveraging augmented reality and natural language processing to build on existing simulation labs, giving what is now only a static and uniform simulation mannequin, a face, colors, and most importantly ears and a voice. All for the sake of improved communication and patient satisfaction once this learning is transferred into practice. © 2019 ACM.",6,User interfaces,Augmented reality - Natural language processing systems - Nursing - Students,Communication techniques - NAtural language processing - Nursing students - Patient satisfaction - Uniform simulations,"461.7 Health Care - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Design, large-scale usage testing, and important metrics for augmented reality gaming applications","Pierdicca, Roberto (1); Emanuele, Frontoni (2); Primo, Zingaretti (2); Mancini, Adriano (2); Loncarski, Jelena (3); Paolanti, Marina (2) ","(1) Universita Politecnica Delle Marche (DICEA), Via Brecce Bianche, Ancona, IT; 60131, Italy (2) Universita Politecnica Delle Marche (DII), Via Brecce Bianche, Ancona, IT; 60131, Italy (3) Department of Engineering Sciences, Division for Electricity Research, Uppsala University, Lagerhyddsvagen 1, Uppsala, SE; S-751 21, Sweden ","ACM Transactions on Multimedia Computing, Communications and Applications",,Association for Computing Machinery,"v 15, n 2",,1-Jun-19,,2019,,15516857,15516865,,10.1145/3311748,a41,,,,"Augmented Reality (AR) offers the possibility to enrich the real world with digital mediated content, increasing in this way the quality of many everyday experiences. While in some research areas such as cultural heritage, tourism, or medicine there is a strong technological investment, AR for game purposes struggles to become awidespread commercial application. In this article, a novel framework for AR kid games is proposed, already developed by the authors for other AR applications such as Cultural Heritage and Arts. In particular, the framework includes different layers such as the development of a series of AR kid puzzle games in an intermediate structure which can be used as a standard for different applications development, the development of a smart configuration tool, together with general guidelines and long-life usage tests and metrics. The proposed application is designed for augmenting the puzzle experience, but can be easily extended to other AR gaming applications. Once the user has assembled the real puzzle, AR functionality within the mobile application can be unlocked, bringing to life puzzle characters, creating a seamless game that merges AR interactions with the puzzle reality. The main goals and benefits of this framework can be seen in the development of a novel set of AR tests and metrics in the pre-release phase (in order to help the commercial launch and developers), and in the release phase by introducing the measures for long-life app optimization, usage tests and hint on final users together with a measure to design policy, providing a method for automatic testing of quality and popularity improvements. Moreover, smart configuration tools, as part of the general framework, enabling multi-app and eventually also multi-user development, have been proposed, facilitating the serialization of the applications. Results were obtained from a large-scale user test with about 4 million users on a set of eight gaming applications, providing the scientific community a workflow for implicit quantitative analysis in AR gaming. Different data analytics developed on the data collected by the framework prove that the proposed approach is affordable and reliable for long-life testing and optimization. © 2019 Association for Computing Machinery.",39,Augmented reality,Automatic testing - Data Analytics,Applications development - Augmented reality gaming - Commercial applications - Gaming framework - Intermediate structures - Large scale testing - Mobile gaming - Puzzle,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The Supernumerary hand illusion in augmented reality,"Rosa, Nina (1); Veltkamp, Remco C. (1); Hurst, Wolfgang (1); Nijboer, T. (2); Gilbers, Carolien (3); Werkhoven, Peter (4) ","(1) Utrecht University, Department of Information and Computing Sciences, Princetonplein 5, Utrecht; 3584CC, Netherlands (2) Utrecht University, Department of Psychology, University Medical Center Utrecht, Rehabilitation Medicine, Rehabilitation Center de Hoogstraat, Netherlands (3) Department of Information and Computing Sciences, Utrecht University, Netherlands (4) Utrecht University, Department of Information and Computing Sciences, TNO, Technical Sciences, Netherlands ",ACM Transactions on Applied Perception,,Association for Computing Machinery,"v 16, n 2",,Aug-19,,2019,,15443558,15443965,,10.1145/3341225,12,,,,"The classic rubber hand illusion (RHI) experiment studies the sense of embodiment over a fake limb. Distinguished subcomponents of embodiment are ownership (sense of self-attribution of a body), agency (sense of having motor control), and self-location (the spatial experience of being inside a body), and are typically evoked in either reality or virtual reality. In augmented reality (AR), however, visually present real limbs can be augmented with (multiple) fake virtual limbs, which results in a variation of the RHI, the augmented reality supernumerary hand illusion (ARSHI). Such conditions occur, for example, in frst-person AR games and in AR-interfaces for tele-robotics. In this article, we examined to what extent humans can experience the sense of embodiment over a supernumerary virtual arm in addition to one or two real arms. We also examine how embodiment is a?ected by the perceptual visual-tactile synchronicity of the virtual and real limbs, and by the synchronicity of active movement of the virtual and real hand. Embodiment was measured subjectively by questionnaire and objectively by skin conductance responses (SCRs). Questionnaire responses show that ownership, agency, and self-location can be evoked over the virtual arm in the presence of a real arm, and that they are signifcantly stronger for synchronous conditions than for asynchronous conditions. The perceptual and motorical synchronous condition with three visible hands led to an experience of owning the virtual hand. These responses further show that agency was also strongly experienced over the supernumerary virtual arm, and responses regarding self-location suggest a shift in sensed location when one real arm was in view and an additional location when both real arms where in view. SCRs show no signifcant e?ect of condition, but do show a signifcant habituation e?ect as a function of the number of conditions performed by participants. When analyzing the relations at the individual participant level between the questionnaire data and skin conductance, we found two clusters of participants: (1) participants with low questionnaire responses and low-medium SCRs and (2) participants with high questionnaire responses and low-high SCRs. Finally, we discuss how virtual hand appearance/realism and willingness to accept virtual limbs could play an important role in the ARSHI, and provide insights on intricacies involved with measuring and evaluating RHIs. © 2019 Association for Computing Machinery.",29,Virtual reality,Augmented reality - Location - Rubber - Surveys,Agency - Multisensory - Ownership - Sense of embodiment - Skin conductance - Supernumerary hand illusion - Virtual hand,"723 Computer Software, Data Handling and Applications - 818.1 Natural Rubber",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality smartphone compasses: Opportunity or oxymoron?,"Bowers, David S. (1) ","(1) School of Computing and Communications, Open University, Milton Keynes, United Kingdom ",UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,p 13-16,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,10.1145/3341162.3343777,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,"The development of augmented reality capabilities on smartphones has led to the emergence of a range of AR apps, including AR compasses. Some of these apps claim to be as good as professional magnetic navigation compasses, and suitable for navigation use. This poster presents detailed measurements of compass deviation (error) curves and offset errors for augmented reality compass apps on 17 mobile devices. The magnitude of the deviation errors measured casts serious doubt on claims the apps are appropriate for navigation purposes. This in turn emphasizes the need for the ubiquitous computing community to help ensure adequate awareness of the limitations of some onboard sensors, including compasses, on devices such as smartphones. © 2019 Copyright is held by the owner/author(s).",10,Ubiquitous computing,Augmented reality - Calibration - Compasses (magnetic) - Errors - Navigation - Smartphones - Wearable computers,Deviation errors - Error measurements - Magnetic navigation - Measurements of - Offset errors - On-board sensors,"718.1 Telephone Systems and Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 942.3 Magnetic Instruments",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Pervasive augmented reality for indoor uninterrupted experiences: A user study,"Marques, Bernardo (1); Carvalho, Raphael (2); Dias, Paulo (2); Santos, Beatriz S. (3) ","(1) DETI-IEETA University of Aveiro, Aveiro, Portugal (2) DETI University of Aveiro, Aveiro, Portugal (3) DETI-IEETA University of AveirO, Aveiro, Portugal ",UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,p 141-144,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,10.1145/3341162.3343759,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,"Augmented Reality (AR) adds additional layers of information on top of real environments. Recently, Pervasive AR extends this concept through an AR experience that is continuous in space, being aware of and responsive to the user’s context and pose (position and orientation). This paper focus on an exploratory user study with 27 participants meant to better understand some aspects of Pervasive AR, such as how users explore, select, recognize and manipulate virtual content in uninterrupted AR experiences, as well as their preferences. The approach used to provide this sort of engaging experiences allows the creation of indoor persistent location-based experiences, with a high level of accuracy and resilience to changes in dynamic environments. Results concerning user acceptance of uninterrupted AR experiences were encouraging. In particular, users were positively impressed by the continuous display of virtual content and were willing to use this technology more often and in different contexts. © 2019 Copyright held by the owner/author(s).",9,Ubiquitous computing,Augmented reality - Wearable computers,Context sensitive computing - Dynamic environments - Location based - Position and orientations - Real environments - Uninterrupted Experiences - User acceptance - User study,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: -, Acronym: FCT, Sponsor: Funda&Atilde;&sect;&Atilde;&pound;o para a Ci&Atilde;&ordf;ncia e a Tecnologia; Number: UID/CEC/00127/2019, Acronym: FCT, Sponsor: Funda&Atilde;&sect;&Atilde;&pound;o para a Ci&Atilde;&ordf;ncia e a Tecnologia; Number: -, Acronym: FEDER, Sponsor: European Regional Development Fund; ","The present study was developed in the scope of the Smart Green Homes Project [POCI-01-0247-FEDER-007678], a co-promotion between Bosch Termotecnologia S.A. and the University of Aveiro. It is financed by Portugal 2020 under the Competitiveness and Internationalization Operational Program, and by the European Regional Development Fund. This study was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through the FCT - Foundation for Science and Technology, in the context of the project UID/CEC/00127/2019.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Attention-driven interaction systems for augmented reality,"Vortmann, Lisa-Marie (1) ","(1) Cognitive Systems Lab, Department of Mathematics and Computer Science, University of Bremen, Bremen, Germany ",ICMI 2019 - Proceedings of the 2019 International Conference on Multimodal Interaction,ACM SIGCHI; AISpeech; Baidu; Microsoft; Openstream; SenseTime,"Association for Computing Machinery, Inc",,p 482-486,14-Oct-19,ICMI 2019 - Proceedings of the 2019 International Conference on Multimodal Interaction,2019,,,,9.78145E+12,10.1145/3340555.3356088,,"21st ACM International Conference on Multimodal Interaction, ICMI 2019","October 14, 2019 - October 18, 2019",,"Augmented reality (AR) glasses enable the embedding of visual content in a real-world surroundings. In this PhD project, I will implement user interfaces which adapt to the cognitive state of the user, for example by avoiding distractions or re-directing the user's attention towards missed information. For this purpose, sensory data from the user is captured (Brain activity via EEG of fNIRS, eye tracking, physiological measurements) and modeled with machine learning techniques. The focus of the cognitive state estimation is centered around attention related aspects. The main task is to build models for an estimation of a person's attentional state from the combination and classification of multimodal data streams and context information, as well as their evaluation. Furthermore, the goal is to develop prototypical user interfaces for AR glasses and to test their usability in different scenarios. © 2019 Copyright held by the owner/author(s).",28,User interfaces,Adaptive systems - Augmented reality - Brain - Classification (of information) - Eye tracking - Glass - Interactive computer systems - Interface states - Learning systems - Machine learning - Petroleum reservoir evaluation,Attention - Cognitive state - Context information - Interaction - Interaction systems - Machine learning techniques - Multimodal data streams - Physiological measurement,"461.1 Biomedical Engineering - 512.1.2 Petroleum Deposits : Development Operations - 716.1 Information Theory and Signal Processing - 722.2 Computer Peripheral Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 812.3 Glass - 931 Classical Physics; Quantum Theory; Relativity - 932 High Energy Physics; Nuclear Physics; Plasma Physics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A model-based method for cognitive user interface design for skills training in an augmented reality environment,"Bautista, Luis (1) ","(1) Universidad Industrial de Santander, Bucaramanga, Colombia ",ACM International Conference Proceeding Series,ACM SIGCHI; ACM SIGCHI Latin American HCI Community,Association for Computing Machinery,,,30-Sep-19,CLIHC 2019 - Proceedings of the 9th Latin American Conference on Human Computer Interaction,2019,,,,9.78145E+12,10.1145/3358961.3358988,a27,"9th Latin American Conference on Human Computer Interaction, CLIHC 2019","September 30, 2019 - October 4, 2019",,"Instructional and support information are very useful in learning and automation stages of skill acquisition. The spatial integration of information is fundamental to reduce in trainees the external cognitive processing. Non-integrated information generated spatial split attention, increasing the external cognitive load in trainees. The proposal presented in this document attempt to address the designing of user interfaces at the prototyping level, by proposing a model-based method to explore the design space as a strategy to reduce the external cognitive load. © 2019 Association for Computing Machinery.",5,User interfaces,Augmented reality - Human computer interaction,Cognitive loads - Cognitive processing - Integrated informations - Model-based method - Model-based OPC - Skill acquisition - Spatial integrations - User interface designs,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
XRaSE: Towards virtually tangible software using augmented reality,"Mehra, Rohit (1); Sharma, Vibhu Saujanya (1); Kaulgud, Vikrant (1); Podder, Sanjay (1) ","(1) Accenture Labs, Bangalore, India ","Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",ACM SIGAI; Association for Computing Machinery (ACM); IEEE; IEEE Computer Society; IEEE Technical Council on Software Engineering (TCSE); Special Interest Group on Software Engineering (SIGSOFT),Institute of Electrical and Electronics Engineers Inc.,,p 1194-1197,Nov-19,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019,,,,9.78173E+12,10.1109/ASE.2019.00135,8952170,"34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019","November 10, 2019 - November 15, 2019",,"Software engineering has seen much progress in recent past including introduction of new methodologies, new paradigms for software teams, and from smaller monolithic applications to complex, intricate, and distributed software applications. However, the way we represent, discuss, and collaborate on software applications throughout the software development life cycle is still primarily using the source code, textual representations, or charts on 2D computer screens-the confines of which have long limited how we visualize and comprehend software systems. In this paper, we present XRaSE, a novel prototype implementation that leverages augmented reality to visualize a software application as a virtually tangible entity. This immersive approach is aimed at making activities like application comprehension, architecture analysis, knowledge communication, and analysis of a software's dynamic aspects, more intuitive, richer and collaborative. © 2019 IEEE.",16,Application programs,Augmented reality - Life cycle - Software design,Distributed software applications - Immersive - Knowledge communication - Prototype implementations - Software comprehension - Software development life cycle - Software visualization - Textual representation,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented Reality Aided Analysis of Customer Satisfaction based on Taste-Induced Facial Expression Recognition Using Affdex Software Developer’s Kit,"Balbin, Jessie R. (1); Paglinawan, Charmaine C. (1); De Castro, Mary Josanne A. (1); Llamas, Jared Kobe C. (1); Medina, Mikka Ellah T. (1); Pangilinan, John Jomel O. (1); Valiente, Flordeliza L. (1) ","(1) School of Electrical, Electronics and Computer Engineering, Mapua University Muralla St., Intramuros, Manila; 1002, Philippines ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 204-209,28-Mar-19,"Proceedings of the 2019 9th International Conference on Biomedical Engineering and Technology, ICBET 2019",2019,,,,9.78145E+12,10.1145/3326172.3326221,,"9th International Conference on Biomedical Engineering and Technology, ICBET 2019","March 28, 2019 - March 30, 2019",,"Customer satisfaction is one of the main determinants in the success of a business or establishment. In food chains and restaurants, the food taste plays a major role in customer satisfaction. Different technologies are being adapted by businesses in order to improve the customer experience and provide ease to their employees but there is still no available alternative method which can obtain customer satisfaction for food taste automatically while being non-intrusive. To address the problem, this research aims to provide a device which can analyze customer satisfaction based on the taste-induced facial expressions and in real-time. For the facial expression recognition, Affdex SDK was used. The Affdex SDK is known for its reliability in emotion recognition due to its large and spontaneous facial dataset. The accuracy shown in the results indicates the effectiveness of this study. This device can benefit the food industry with its ease-of-use and real-time results. © 2019 Association for Computing Machinery.",7,Customer satisfaction,Augmented reality - Biomedical engineering - Face recognition - Image processing - Large dataset - Sales - Speech recognition,Customer experience - Ease-of-use - Emotion recognition - Facial expression recognition - Facial Expressions - Food industries - Non-intrusive - Software developer,"461.1 Biomedical Engineering - 723 Computer Software, Data Handling and Applications - 751.5 Speech",,,,"We would like to thank God for giving us strength, capacity, opportunity for partaking this thesis study, and earning knowledge from this thesis study. The accomplishment of this thesis study would not be possible if without the help and guidance from many people. With that, we would like to express our deepest gratitude to our advisers Engr. Jessie R. Balbin and Engr. Charmaine C. Paglinawan for imparting knowledge on the research topic, providing guidelines during the course of the study, and for being most accommodating and considerate to our group. We would also like to acknowledge the constructive ideas expressed by our panel members: Engr. Glenn O. Avenda&ntilde;o, Engr. Ernesto M. Vergara, and Engr. Julius T. Sese. Your suggestions gave room to improvements in our study. Lastly, we heartily thank our parents, family, and friends who gave us encouragement and support which helped us greatly in the completion of this study.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Marker based pedestrian detection using augmented reality,"Kim, Chanyong (1); Kim, Hyojin (1); Son, Huiseung (1); Shahid, Muhammad Tanseef (1); Song, Hyun Chul (1); Piccialli, Francesco (2); Choi, Kwang Nam (1) ","(1) Dept of Computer Science and Engineering, Chung-Ang University, Seoul, Korea, Republic of (2) Dept. of Electrical Engineering and Information Technology, University of Naples Federico II, Italy ",ACM International Conference Proceeding Series,Southwest Jiaotong University,Association for Computing Machinery,,p 219-223,3-Nov-19,ICAIP 2019 - 2019 3rd International Conference on Advances in Image Processing,2019,,,,9.78145E+12,10.1145/3373419.3373456,,"3rd International Conference on Advances in Image Processing, ICAIP 2019","November 8, 2019 - November 10, 2019",,"Pedestrian detection is a popular research topic from the last decade. Most of the pedestrian identification models are based on face recognition algorithms. It is a difficult task to detect and track individual pedestrians based on these algorithms because there is a compulsion that their faces should be towards the camera. This limitation makes face recognition algorithms inefficient to detect pedestrians from the backsides. In this paper, we proposed a method for pedestrian detection using marker recognition. Multiple pedestrians are detected and then tracked based on markers attached to their backsides. Attaching markers on backside of pedestrians helps to recognize them even when they are looking the other way. After the marker is recognized, the unique character related to that marker is displayed as a 3D object. This marker-based pedestrian detection is carried out using a mobile phone system and can be applied to embedded systems. The proposed method makes it possible to recognize up to three pedestrians located at different positions from the camera. © 2019 Association for Computing Machinery.",11,Face recognition,Augmented reality - Cameras - Embedded systems - Image processing,3D object - Face recognition algorithms - Identification model - Marker Recognition - Mobile phone systems - Pedestrian detection - Research topics,"723 Computer Software, Data Handling and Applications - 742.2 Photographic Equipment",,,"Number: -, Acronym: MSIT, Sponsor: Ministry of Science and ICT, South Korea; Number: -, Acronym: IITP, Sponsor: Institute for Information and Communications Technology Promotion; ",This research was supported by the MSIT(Ministry of Science and ICT), Korea," under the National Program for Excellence in SW supervised by the IITP(Institute of Information &""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,
A human-centered design process for an augmented reality based training system,"Sezgin, Abdullah (1) ","(1) Daimler AG, Bremen, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 771-775,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,10.1145/3340764.3344906,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"The purpose of this paper is to present a research proposal for an AR-based assembly training system. Findings from previous user studies on AR-based assembly training and their limitations are analyzed. Research questions are drafted and a research proposal for future research of an AR-based assembly training within the frame of a user study is presented. An iterative human-centered design process is proposed that integrates both production employees as well as training content creators (e.g. trainers or foremen) as end users in a co-creation process of an industrial assembly training system, which has not been done in previous studies. Including user groups in the early stages of the process will help to understand different user needs and requirements for an AR-based assembly training system that will be evaluated in an industrial training setting. A co-creation workshop according to the Design Sprint process is proposed to integrate different key users in the development process of the AR-based assembly training system. The final aim of this research project is to gain insights of how an assembly training system should be designed in order to create real value for the different users and the organization. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to",28,Personnel training,Augmented reality - Design - Human computer interaction,Assembly trainings - Development process - Human-centered designs - Industrial assemblies - Industrial training - Learning - Research proposals - Research questions,"723 Computer Software, Data Handling and Applications - 912.4 Personnel",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
CryptoAR wallet: A blockchain cryptocurrency wallet application that uses augmented reality for on-chain user data display,"Chen, You-Ping (1); Ko, Ju-Chun (1) ","(1) National Taipei University of Technology, Taipei, Taiwan ","Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019",ACM Special Interest Group on Computer-Human Interaction (SIGCHI),"Association for Computing Machinery, Inc",,,1-Oct-19,"Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019",2019,,,,9.78145E+12,10.1145/3338286.3344386,a39,"21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019","October 1, 2019 - October 4, 2019",,"Blockchain technology has recently become popular and its use in business and industry has increased, especially in finance and technology. A blockchain wallet plays a vital role in blockchain industry, but it is difficult to understand and get started, as well as hard to learn how to use blockchain wallet. From the aspect of Interaction Design, we proposed a blockchain cryptocurrency wallet that combine with augmented reality and crypto technology, that is, an Augmented Crypto Wallet (CryptoAR Wallet). We except that browsing and viewing virtual information (On-Chain User Data) through augmented reality, will shorten the distance between user and blockchain wallet services. Though our preliminary design, development and user testing, this in-development application shows its potential on increasing the level of trust and satisfactions, with more comprehensive user experience. © 2019 Association for Computing Machinery.",5,Blockchain,Augmented reality - Electronic money - Human computer interaction - User interfaces,Interaction design - Preliminary design - User data - User experience - User testing - User trust - Virtual information,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Exploring the potential of augmented reality in domestic environments,"Knierim, Pascal (1); Woniak, Pawel W. (2); Abdelrahman, Yomna (3); Schmidt, Albrecht (1) ","(1) Ludwig Maximilian University of Munich, Germany (2) Utrecht University, Netherlands (3) Bundeswehr University, Munich, Germany ","Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019",ACM Special Interest Group on Computer-Human Interaction (SIGCHI),"Association for Computing Machinery, Inc",,,1-Oct-19,"Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019",2019,,,,9.78145E+12,10.1145/3338286.3340142,a31,"21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019","October 1, 2019 - October 4, 2019",,"While Augmented Reality (AR) technologies are becoming increasingly available, our understanding of AR is primarily limited to controlled experiments which address use at work or for entertainment. Little is known about how it could enhance everyday interaction from a user's perspective. Personal use of AR at home may improve how users' interface with information on a daily basis. Through an online survey, we investigated attitudes towards domestic AR. We further explored the opportunities for AR at home in a technology probe. We first introduced the users to AR by offering an AR experience presented through mixed reality smart glasses. We then used a tailor-made tablet application to elicit photos illustrating how users imagine future AR experiences. Finally, we conducted semi-structured interviews based on elicited photos. Our results show that users are eager to benefit from on-demand information, assistance, enhanced sensory perception, and play offered by AR across many locations at home. We contribute insights for future AR systems designed for domestic environments. © 2019 Copyright held by the owner/author(s).",36,Human computer interaction,Augmented reality - Mixed reality - Probes - Sensory perception - Surveying - Surveys,Controlled experiment - Domestic environments - Domestication - On-demand informations - Online surveys - Personal use - Semi structured interviews - Tablet applications,"405.3 Surveying - 461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: H2020, Sponsor: Horizon 2020 Framework Programme; Number: -, Acronym: UU, Sponsor: Universiteit Utrecht; Number: 683008 AMPLIFY, Acronym: -, Sponsor: -; Number: 16SV7527, Acronym: -, Sponsor: -; ",This work was supported by the European Union's Horizon 2020 Programme under ERCEA grant no. 683008 AMPLIFY. Parts of this research was supported by Utrecht University's focus area Sport and Society and the German Federal Ministry of Education and Research as part of the project Begreifen (Grant No. 16SV7527).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Investigating smartphone-based pan and zoom in 3D data spaces in augmented reality,"Büschel, Wolfgang (1); Mitschick, Annett (1); Meyer, Thomas (1); Dachselt, Raimund (1) ","(1) Interactive Media Lab., Technische Universität Dresden, Dresden, Germany ","Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019",ACM Special Interest Group on Computer-Human Interaction (SIGCHI),"Association for Computing Machinery, Inc",,,1-Oct-19,"Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019",2019,,,,9.78145E+12,10.1145/3338286.3340113,a2,"21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019","October 1, 2019 - October 4, 2019",,"In this paper, we investigate mobile devices as interactive controllers to support the exploration of 3D data spaces in head-mounted Augmented Reality (AR). In future mobile contexts, applications such as immersive analysis or ubiquitous information retrieval will involve large 3D data sets, which must be visualized in limited physical space. This necessitates efficient interaction techniques for 3D panning and zooming. Smartphones as additional input devices are promising because they are familiar and widely available in mobile usage contexts. They also allow more casual and discreet interaction compared to free-hand gestures or voice input. We introduce smartphone-based pan & zoom techniques for 3D data spaces and present a user study comparing five techniques. Our results show that spatial device gestures can outperform both touch-based techniques and hand gestures in terms of task completion times and user preference. We discuss our findings in detail and suggest suitable techniques for specific AR navigation tasks. © 2019 Copyright held by the owner/author(s).",70,Human computer interaction,Augmented reality - Data visualization - Smartphones - Three dimensional computer graphics,3D data - 3D navigation - Efficient interaction - Immersive visualization - Interaction techniques - Interactive controller - Navigation tasks - Ubiquitous information,"718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: DFG, Sponsor: Deutsche Forschungsgemeinschaft; Number: DA 1319/11-1, Acronym: TUD, Sponsor: Technische Universit&auml;t Dresden; Number: -, Acronym: DFG, Sponsor: Deutsche Forschungsgemeinschaft; Number: -, Acronym: DFG, Sponsor: Deutsche Forschungsgemeinschaft; ","We thank Ricardo Langner for his invaluable feedback as well as all our participants for making this study possible. This work was funded by the German Research Foundation (DFG, Deutsche Forschungsgemeinschaft) as part of Germany's Excellence Strategy - EXC 2050/1 - Project ID 390696704 - Cluster of Excellence Centre for Tactile Internet with Human-in-the-Loop"" (CeTI) of Technische Universit?t Dresden and DFG grant CollabWall (DA 1319/11-1).""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
AssessAR: An augmented reality based environmental impact assessment framework,"Mehra, Rohit (1); Sharma, Vibhu Saujanya (1); Kaulgud, Vikrant (1); Podder, Sanjay (1) ","(1) Accenture Labs, Bangalore, India ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3365034,3365034,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Human activities can have a lasting impact on the environment and society. Environmental impact assessment (EIA) which focusses on evaluating the impact of proposed developmental projects on the environment, helps in transparent decision-making and involves multiple stakeholders. However, EIA is data and effort-intensive and often becomes complex and long-drawn. Moreover, EIA is currently performed using primarily two-dimensional traditional mediums which could be vastly restrictive and difficult to navigate and comprehend. Here, we present an immersive approach which can create 3D interactive elements, modelling the real-world using augmented/mixed reality. Because of the inherent benefits of using three-dimensional representations and associated real-world interactions, we posit that our approach will facilitate better and faster, collaboration-enabled analysis of a developmental project proposal, thereon reducing processing time and promoting high fidelity. © 2019 Copyright held by the owner/author(s).",6,Environmental impact,Augmented reality - Decision making - Environmental impact assessments - Virtual reality,Dimensional representation - Environmental impact assessments (EIA) - High-fidelity - Human activities - Impact on the environment - Interactive elements - Multiple stakeholders - Processing time,"454.2 Environmental Impact and Protection - 723 Computer Software, Data Handling and Applications - 912.2 Management",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Immersive analytics using augmented reality for computational fluid dynamics simulations,"Bednarz, Tomasz (1); Tobia, Michael (1); Nguyen, Huyen (2); Branchaud, Dominic (2) ","(1) CSIRO Data61, UNSW Art and Design, Australia (2) UNSW Art and Design, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365735,a46,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"This project aimed to employ multi-sensory visual analytics to a Computational Fluid Dynamics (CFD) dataset using augmented and mixed reality interfaces. Initial application was developed for a Hololens which allows users to interact with the CFD data using gestures, enabling control over the position, rotation and scale of the data, sampling, as well as voice commands that provide a range of functionalities such as changing a parameter or render a different view. This project leads to a more engaging and immersive experience of data analysis, generalised for CFD simulations. The application is also able to explore CFD datasets in fully collaborative ways, allowing engineers, scientists, and end-users to understand the underlying physics and behaviour of fluid flows together. © 2019 Association for Computing Machinery.",4,Computational fluid dynamics,Augmented reality - Flow of fluids - Interactive computer graphics - Mixed reality,CFD simulations - Computational fluid dynamics simulations - Immersive - Mixed reality interfaces - Multi-Sensory - Simulation - Visual analytics - Voice command,"631.1 Fluid Flow, General - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality guided respiratory liver tumors punctures: A preliminary feasibility study,"Li, Ruotong (1); Yang, Tianpei (2); Si, Weixin (2); Liao, Xiangyun (2); Wang, Qiong (2); Klein, Reinhard (1); Heng, Pheng-Ann (3) ","(1) University of Bonn, Germany (2) SIAT, CAS (3) CUHK T Stone Robotics Institute and CSE, Hungary ","SIGGRAPH Asia 2019 Technical Briefs, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 114-117,17-Nov-19,"SIGGRAPH Asia 2019 Technical Briefs, SA 2019",2019,,,,9.78145E+12,10.1145/3355088.3365166,,"SIGGRAPH Asia 2019 Technical Briefs - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"CT-guided radiofrequency ablation (RFA) has evolved rapidly over the past decade and become a widely accepted treatment option for patients with liver tumors. However, it is hard for doctors to locate tumors precisely while avoid damaging the surrounding risk structures with 2D CT images, which only provides limited static information, especially in case of respiratory motion. This paper presents a novel augmented reality guidance modality for improving the precision of liver tumors punctures by providing visual cue of 3D personalized anatomy with respiratory motion. Optical see-through display devices Epson MoveRio BT300 and Microsoft HoloLens are used to mix pre-operative 3D personalized data and intra-operative physical scene. Here an augmented reality based surgical navigation pipeline is proposed to achieve the transformation from raw medical data to virtual guidance information and precisely superimpose this information onto real experimental animal. In addition, to alleviate the difficulty during needle placement induced by respiratory motion, we proposed a correlation model to real-timely predict the tumor position via regression based respiration state estimation and the statistical tumor motion model. We experimentally validated the proposed system on in vivo beagle dogs with artificial lesion, which can effectively improve the puncture efficiency and precision. The proposed augmented reality modality is a general strategy to guide the doctors perform precise percutaneous puncture under respiration conditions and has the potential to be used for other surgical navigation tasks. © 2019 Association for Computing Machinery.",8,Tumors,Augmented reality - Computerized tomography - Display devices - Interactive computer graphics - Metadata - Navigation - Respiratory mechanics - Surgery,Correlation modeling - Experimental animals - Feasibility studies - Guidance information - Motion modeling - Optical see-through display - Radiofrequency ablation - Surgical navigation,"461 Bioengineering and Biology - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: 2016A020220013, Acronym: -, Sponsor: Science and Technology Planning Project of Guangdong Province; Number: T42-409/18-R, Acronym: -, Sponsor: -; Number: U1813204, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; ","We gratefully thank the reviewers for their constructive comments. This work was supported in part by the National Natural Science Foundation of China (U1813204, 61802385), in part by the HK RGC TRS project (T42-409/18-R), in part by the Guangdong province science and technology plan project (No.2016A020220013) and in part by the CUHK T Stone Robotics Institute.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Rough carving of 3D models with spatial augmented reality,"Hattab, Ammar (1); Taubin, Gabriel (1) ","(1) Brown University, Providence; RI, United States ",Proceedings: SCF 2019 - ACM Symposium on Computational Fabrication,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,16-Jun-19,Proceedings: SCF 2019 - ACM Symposium on Computational Fabrication,2019,,,,9.78145E+12,10.1145/3328939.3328998,4,"2019 ACM Symposium on Computational Fabrication, SCF 2019","June 16, 2019 - June 18, 2019",,"Carving is a subtractive process where we get the shape by removing materials. While most people can get roughly the right intended shape, it is usually challenging not to over-cut the model. We propose a method that helps an unskilled user to carve a rough physical replica of a 3D model using the minimum number of cuts while only using manual cutting tools. The method starts by analyzing the input 3D model and generates the minimum set of cutting steps that remove most of the material. Then using a projector, we project the instructions sequentially onto a block of material to guide the user in performing them. We use the projector-camera setup to 3D scan the object after cutting and automatically detect the changes to reflect them on the digital model. We demonstrate a complete system to support this operation and show several examples of manually carved 3D models while using the system. © 2019 Copyright held by the owner/author(s).",21,3D modeling,Augmented reality - Cutting tools,3-d scans - Assisted carving - Complete system - Digital model - Personal fabrication - Projector-camera - Rough 3D carving - Spatial augmented realities,"603.2 Machine Tool Accessories - 723 Computer Software, Data Handling and Applications",,,"Number: IIP-1500249, Acronym: NSF, Sponsor: National Science Foundation; ",The work described herein was partially supported by a Brown Fellowship and National Science Foundation under grant No.: IIS-1717355 and IIP-1500249.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Arspectator: Exploring augmented reality for sport events,"Zollmann, Stefanie (1); Langlotz, Tobias (1); Loos, Moritz (1); Lo, Wei Hong (1); Baker, Lewis (1) ","(1) University of Otago, New Zealand ","SIGGRAPH Asia 2019 Technical Briefs, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 75-78,17-Nov-19,"SIGGRAPH Asia 2019 Technical Briefs, SA 2019",2019,,,,9.78145E+12,10.1145/3355088.3365162,,"SIGGRAPH Asia 2019 Technical Briefs - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"Augmented Reality (AR) has gained a lot of interests recently and has been used for various applications. Most of these applications are however limited to small indoor environments. Despite the wide range of large scale application areas that could highly benefit from AR usage, until now there are rarely AR applications that target such environments. In this work, we discuss how AR can be used to enhance the experience of on-site spectators at live sport events. We investigate the challenges that come with applying AR for such a large scale environment and explore state-of-the-art technology and its suitability for an on-site AR spectator experience. We also present a concept design and explore the options to implement AR applications inside large scale environments. © 2019 Association for Computing Machinery.",8,Sports,Augmented reality - Interactive computer graphics,AR application - Concept designs - Indoor environment - Large-scale applications - Sport events - State-of-the-art technology,"461.3 Biomechanics, Bionics and Biomimetics - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: -, Acronym: MBIE, Sponsor: Ministry for Business Innovation and Employment; ","This project is supported by an MBIE Endeavour Smart Ideas grant. We thank Animation Research Ltd, Forsyth Barr Stadium, the Highlanders, Otago Rugby (ORFU) and OptaPerform for their support.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
When IoT met augmented reality: Visualizing the source of the wireless signal in AR view,"Park, Yongtae (1); Yun, Sangki (2); Kim, Kyu-Han (2) ","(1) Korea University, Korea, Republic of (2) Hewlett Packard Labs, United States ","MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 117-129,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,10.1145/3307334.3326079,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,"This paper presents VisIoT, a system that tracks the location of a wireless transmitter in IoT devices and displays it in the screen of an AR device such as smart glasses and tablet. The proposed system benefits existing IoT systems by enabling intuitive interaction between a user and IoT devices and further enhancing visualization of the data collected from IoT sensors. VisIoT achieves them through a combination of wireless sensing and camera motion tracking. By using the azimuth and elevation angles between the wireless transmitter and the camera-equipped mobile device, VisIoT can instantly identify the location of the IoT device from the camera image. This paper introduces novel azimuth and elevation estimation algorithms that leverage the phase difference of the signals from two antennas together with the tracked camera rotation. We prototype VisIoT using a tablet PC and a USRP software radio, and develop a software that tracks and visualizes the location of ZigBee nodes in real time. The evaluation results show that VisIoT can accurately track the nodes with the median position error of 6%. © 2019 Association for Computing Machinery.",41,Internet of things,Augmented reality - Cameras - Data visualization - Display devices - Location - Personal computers - Software prototyping - Software radio - Transmitters,Azimuth and elevation angles - Azimuth and elevation estimation - Camera rotations - Evaluation results - Intuitive interaction - Wireless devices - Wireless signals - Wireless transmitter,"716.3 Radio Systems and Equipment - 722.2 Computer Peripheral Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 742.2 Photographic Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
GleaM: An illumination estimation framework for real-time photorealistic augmented reality on mobile devices,"Prakash, Siddhant (1); Bahremand, Alireza (1); Nguyen, Linda D. (1); LiKamWa, Robert (1) ","(1) Arizona State University, United States ","MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 142-154,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,10.1145/3307334.3326098,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,"Mixed reality mobile platforms attempt to co-locate virtual scenes with physical environments, towards creating immersive user experiences. However, to create visual harmony between virtual and physical spaces, the virtual scene must be accurately illuminated with realistic lighting that matches the physical environment. To this end, we design GLEAM, a framework that provides robust illumination estimation in real-time by integrating physical light-probe estimation with current mobile AR systems. GLEAM visually observes reflective objects to compose a realistic estimation of physical lighting. Optionally, GLEAM can network multiple devices to sense illumination from different viewpoints and compose a richer estimation to enhance realism and fidelity. Using GLEAM, AR developers gain the freedom to use a wide range of materials, which is currently limited by the unrealistic appearance of materials that need accurate illumination, such as liquids, glass, and smooth metals. Our controlled environment user studies across 30 participants reveal the effectiveness of GLEAM in providing robust and adaptive illumination estimation over commercial status quo solutions, such as pre-baked directional lighting and ARKit 2.0 illumination estimation. Our benchmarks reveal the need for situation driven tradeoffs to optimize for quality factors in situations requiring freshness over quality and vice-versa. Optimizing for different quality factors in different situations, GLEAM can update scene illumination as fast as 30 ms by sacrificing richness and fidelity in highly dynamic scenes, or prioritize quality by allowing an update interval as high as 400 ms in scenes that require high-fidelity estimation. © 2019 Association for Computing Machinery.",34,Real time systems,Augmented reality - Benchmarking - Geometry - Image processing - Lighting - Mixed reality - Probes,Controlled environment - Illumination estimation - Image-based lighting - Light estimations - Light probes - Lighting model - Physical environments - Reflective objects,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 921 Mathematics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Poster Abstract: Augmented Reality Based Therapy System for Social Skill Deficits,"Sha, Kewei (1); Liu, Zhandong (2); Dempsey, Jack (3) ","(1) University of Houston-Clear Lake, Houston; TX, United States (2) Baylor College of Medicine, Houston; TX, United States (3) Children's Hospital Colorado, University of Colorado, Aurora; CO, United States ","Proceedings - 4th IEEE/ACM Conference on Connected Health: Applications, Systems and Engineering Technologies, CHASE 2019",,Institute of Electrical and Electronics Engineers Inc.,,p 19-20,Sep-19,"Proceedings - 4th IEEE/ACM Conference on Connected Health: Applications, Systems and Engineering Technologies, CHASE 2019",2019,,,,9.78173E+12,10.1109/CHASE48038.2019.00015,8908642,"4th IEEE/ACM Conference on Connected Health: Applications, Systems and Engineering Technologies, CHASE 2019","September 25, 2019 - September 27, 2019",,"Treating social skill deficits caused by Autism Spectrum Disorder (ASD) has been a significant challenge. Applied Behavior Analysis (ABA) has shown to be an effective treatment; however, many patients have to leave ABA because of the high cost, lack of insurance coverage, and shortage of ABA-skilled providers. This paper proposes and designs an augmented reality (AR) based system that aims to ease and improve the effectiveness of ABA, as well as to reduce its cost as the ABA can be performed in a clinic or at home with the support of the system. The system consists of three major components, including a web-based module, a HoloLens-based AR module, and a central control module. These modules work together to achieve an efficient treatment system. Ten therapy scenarios are designed and deployed in the prototype system to test the effectiveness of the system. © 2019 IEEE.",7,Patient treatment,Augmented reality - Cost benefit analysis - Diseases - Insurance,Autism spectrum disorders - Behavior analysis - Central control module - Efficient treatment - Insurance coverages - Social skills - system - therapy,"461.6 Medicine and Pharmacology - 723 Computer Software, Data Handling and Applications - 911 Cost and Value Engineering; Industrial Economics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Demo: GLEAM – An illumination estimation framework for real-time photorealistic augmented reality on mobile devices,"Prakash, Siddhant (1); Bahremand, Alireza (1); Nguyen, Linda D. (1); LiKamWa, Robert (1) ","(1) Arizona State University, United States ","MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 659-660,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,10.1145/3307334.3328570,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,"Mixed reality mobile platforms attempt to co-locate virtual scenes with physical environments, towards creating immersive user experiences. However, to create visual harmony between virtual and physical spaces, the virtual scene must be accurately illuminated with realistic lighting that matches the physical environment. To this end, we design GLEAM, a framework that provides robust illumination estimation in real-time by integrating physical light-probe estimation with current mobile AR systems. We present a demo implementation of GLEAM by means of an AR application that estimates environmental illumination and renders the scene with real-time illumination updates. We demonstrate the efficacy of GLEAM’s estimation against a current commercial status quo solution, Apple’s ARKit, with the same application. © 2019 Copyright held by the owner/author(s).",,Real time systems,Augmented reality - Geometry - Image processing - Lighting - Mixed reality - Probes,Illumination estimation - Image-based lighting - Light estimations - Light probes - Lighting model - Photo-realistic - Physical environments - Realistic Lighting,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 921 Mathematics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
360Proto: Making interactive virtual reality & augmented reality prototypes from paper,"Nebeling, Michael (1); Madier, Katy (1) ","(1) University of Michigan School of Information, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300826,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"We explore 360 paper prototyping to rapidly create AR/VR prototypes from paper and bring them to life on AR/VR devices. Our approach is based on a set of emerging paper prototyping templates specifically for AR/VR. These templates resemble the key components of many AR/VR interfaces, including 2D representations of immersive environments, AR marker overlays and face masks, VR controller models and menus, and 2D screens and HUDs. To make prototyping with these templates effective, we developed 360proto, a suite of three novel physical–digital prototyping tools: (1) the 360proto Camera for capturing paper mockups of all components simply by taking a photo with a smartphone and seeing 360-degree panoramic previews on the phone or stereoscopic previews in Google Cardboard; (2) the 360proto Studio for organizing and editing captures, for composing AR/VR interfaces by layering the captures, and for making them interactive with Wizard of Oz via live video streaming; (3) the 360proto App for running and testing the interactive prototypes on AR/VR capable mobile devices and headsets. Through five student design jams with a total of 86 participants and our own design space explorations, we demonstrate that our approach with 360proto is useful to create relatively complex AR/VR applications. © 2019 Copyright held by the owner/author(s).",37,Paper,Augmented reality - Digital devices - Human engineering - Stereo image processing - Virtual reality,Controller models - Design space exploration - Digital prototyping - Immersive environment - Interactive virtual reality - Live video streaming - Paper prototyping - Wizard of Oz,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 811.1 Pulp and Paper",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Evaluation of augmented reality SDKs for classroom teaching,"Dias, Bruno (1); Keller, Breno (1); Delabrida, Saul (1) ","(1) Universidade Federal de Ouro Preto, Ouro Preto Minas Gerais, Brazil ",IHC 2019 - Proceedings of the 18th Brazilian Symposium on Human Factors in Computing Systems,Brazilian Computer Society (SBC); CAPES; CNPq,"Association for Computing Machinery, Inc",,,22-Oct-19,IHC 2019 - Proceedings of the 18th Brazilian Symposium on Human Factors in Computing Systems,2019,Portuguese,,,9.78145E+12,10.1145/3357155.3358447,3358447,"18th Brazilian Symposium on Human Factors in Computing Systems, IHC 2019","October 21, 2019 - October 25, 2019",,"Augmented Reality (AR) has been used in different application domains. In education, it usually aims to improve the current teaching methodologies and is known as Augmented Learning. Currently, there are several Software Development Kits (SDKs), that requires a qualitative analysis to determine which of these tools are appropriate for use as academic support in face-to-face teaching. This paper evaluates the main SDKs for the development of a marker-based AR software to use in classrooms based on a case study in a public university in Brazil. Our study evaluated the accuracy of detecting markers with different characteristics in each of the SDKs. A comparative analysis is presented. The conclusion is that Vuforia and EasyAR are the better choices among the other selected SDKs for this evaluation. © 2019 ACM.",29,Software design,Augmented reality - Human engineering,Augmented learning - Classroom teaching - Comparative analysis - Evaluation - Public universities - Qualitative analysis - Software development kit - Teaching methodologies,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality action assistance and learning for cognitively impaired people - A systematic literature review,"Blattgerste, Jonas (1); Renner, Patrick (1); Pfeiffer, Thies (1) ","(1) CITEC - Cluster of Excellence Cognitive Interaction Technology, Bielefeld University, Germany ",ACM International Conference Proceeding Series,The Department of Computer Science and Engineering at UTA; The Human Centered Computing Laboratory (Heracleia) at UTA; The iPerform Industry-University NSF Center at UTA; The National Center for Scientific Research (NCSR)-Demokritos; The National Science Foundation (NSF),Association for Computing Machinery,,p 270-279,5-Jun-19,"Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019",2019,,,,9.78145E+12,10.1145/3316782.3316789,,"12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019","June 5, 2019 - June 7, 2019",,"Augmented reality (AR) is a promising tool for many situations in which assistance is needed, as it allows for instructions and feedback to be contextualized. While research and development in this area have been primarily driven by industry, AR could also have a huge impact on those who need assistance the most: cognitively impaired people of all ages. In recent years some primary research on applying AR for action assistance and learning in the context of this target group has been conducted. However, the research field is sparsely covered and contributions are hard to categorize. An overview of the current state of research is missing. We contribute to filling this gap by providing a systematic literature review covering 52 publications. We describe the often rather technical publications on an abstract level and quantitatively assess their usage purpose, the targeted age group and the type of AR device used. Additionally, we provide insights on the current challenges and chances of AR learning and action assistance for people with cognitive impairments. We discuss trends in the research field, including potential future work for researchers to focus on. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",75,Augmented reality,Computer applications - Computer programming - Education - Personnel training - Surveying,Assistance - Cognitive impairment - Cognitively impaired - Overview - Research and development - State of research - Systematic literature review - Technical publications,"405.3 Surveying - 723 Computer Software, Data Handling and Applications - 912.4 Personnel",,,"Number: EXC 277, Acronym: -, Sponsor: Exzellenzclusters Entz&uuml;ndungsforschung; ","This research was supported by the Cluster of Excellence Cognitive Interaction Technology - CITEC"" (EXC 277) at Bielefeld University"," which is funded by the German Research Foundation (DFG).""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,
Light me up: An augmented-reality projection system,"Bokaris, Panagiotis-Alexandros (1); Askenazi, Benjamin (1); Haddad, Michaël (2) ","(1) L'Oréal RandI, Clichy, France (2) L'Oréal RandI, Clark; NJ, United States ","SIGGRAPH Asia 2019 XR, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 21-22,17-Nov-19,"SIGGRAPH Asia 2019 XR, SA 2019",2019,,,,9.78145E+12,10.1145/3355355.3361885,,"SIGGRAPH Asia 2019 XR - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"Real-time facial projection mapping is a challenging problem due to the low system latency and the high spatial augmentation accuracy requirements. We propose a new compact and inexpensive projector-camera system (ProCam) composed of off-the-self devices that achieves dynamic facial projection mapping. A mini projector and a depth sensor camera are coupled together to project content on a user's face. In one application, the camera tracks the facial landmarks of a person and simulated makeup is mapped on the person's face. The latter is created by defining different zones of interest on the face. Instead of using sophisticated hardware, we propose an affordable system that can be easily installed anywhere while it assures an immerse experience. No initialization phase is needed and the system can handle different face topologies. In addition, the users can keep their eyes open and enjoy the projetion in a mirror. © 2019 Association for Computing Machinery.",7,Interactive computer graphics,Augmented reality - Cameras - Mapping - Optical projectors,Depth sensors - Facial landmark - Makeup augmentation - Procam - Projector-camera system - Real time - System latency,"405.3 Surveying - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 741.3 Optical Devices and Systems - 742.2 Photographic Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Fast depth densification for occlusion-aware augmented reality,"Holynski, Aleksander (1); Kopf, Johannes (2) ","(1) University of Washington, United States (2) Facebook, United States ","SIGGRAPH Asia 2018 Technical Papers, SIGGRAPH Asia 2018",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,4-Dec-18,"SIGGRAPH Asia 2018 Technical Papers, SIGGRAPH Asia 2018",2019,,,,9.78145E+12,10.1145/3272127.3275083,194,"SIGGRAPH Asia 2018 Technical Papers - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH Asia 2018","December 4, 2018 - December 7, 2018",,"Current AR systems only track sparse geometric features but do not compute depth for all pixels. For this reason, most AR effects are pure overlays that can never be occluded by real objects. We present a novel algorithm that propagates sparse depth to every pixel in near realtime. The produced depth maps are spatio-temporally smooth but exhibit sharp discontinuities at depth edges. This enables AR effects that can fully interact with and be occluded by the real scene. Our algorithm uses a video and a sparse SLAM reconstruction as input. It starts by estimating soft depth edges from the gradient of optical flow fields. Because optical flow is unreliable near occlusions we compute forward and backward flow fields and fuse the resulting depth edges using a novel reliability measure. We then localize the depth edges by thinning and aligning them with image edges. Finally, we optimize the propagated depth smoothly but encourage discontinuities at the recovered depth edges. We present results for numerous real-world examples and demonstrate the effectiveness for several occlusion-aware AR video effects. To quantitatively evaluate our algorithm we characterize the properties that make depth maps desirable for AR applications, and present novel evaluation metrics that capture how well these are satisfied. Our results compare favorably to a set of competitive baseline algorithms in this context. © 2018 Copyright held by the owner/author(s).",27,Three dimensional computer graphics,Augmented reality - Flow fields - Image reconstruction - Interactive computer graphics - Mapping - Optical flows - Petroleum reservoir evaluation - Pixels,3D reconstruction - Depth Estimation - Evaluation metrics - Forward-and-backward - Geometric feature - Reliability measure - Simultaneous Localization - Video analysis,"405.3 Surveying - 512.1.2 Petroleum Deposits : Development Operations - 631.1 Fluid Flow, General - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 741.1 Light/Optics",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Evaluating pointing modes and frames of reference for remotely supporting an augmented reality user in a collaborative (virtual) environment,"Brown, Gordon (1); Prilla, Michael (1) ","(1) Human-Centered Information Systems, Department of Informatics, TU Clausthal Clausthal, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 713-717,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,10.1145/3340764.3344896,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"With the availability of powerful and affordable Augmented Reality (AR) devices, scenarios have become popular in which people wearing AR devices are supported by remote experts. These experts often use 2D peripherals to access the video feed of the 3D head mounted device (HMD) and to augment it with verbal or digital information. This raises the question whether tools that work for these scenarios also work for remote consultations. We conducted a study to (re-)evaluate these tools in a furniture sales consultation context. We focused on the consultant side of these settings and explored how the use of different pointing methods and perspectives affect different situations during a consultation. For this, we developed and evaluated a prototype with ten furniture store workers. Initial results show that while most usability and task load scores were even, the participants reported clear favorites for certain settings. We use these results to derive design recommendations for similar future projects. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",34,Augmented reality,Computer applications - Computer programming - Wire pointing,Design recommendations - Digital information - Frames of reference - Perspectives - Pointing methods - Remote collaboration - Remote consultation - Remote experts,"535.2.2 Metal Forming Practice - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality based on stem for supporting science literacy in vocational education,"Agustina, W.W. (1); Sumarto, S. (1); Trisno, B. (1) ","(1) Pendidikan Teknologi Dan Kejuruan, Universitas Pendidikan Indonesia, Jl. Dr. Setiabudi No. 229, Bandung, Indonesia ",Journal of Physics: Conference Series,,Institute of Physics Publishing,"v 1375, n 1",,21-Nov-19,Annual Conference of Science and Technology,2019,,17426588,17426596,,10.1088/1742-6596/1375/1/012088,12088,"Annual Conference of Science and Technology 2018, AnCOSeT 2018",30-Aug-18,,"AR development is becoming very popular in various fields of science. Some researchers have demonstrated that AR has the potential in science, technology, engineering, and mathematics (STEM). Puse of total AR integrated STEM needs to be studied more deeply, how STEM-AR characteristics, and how the potential of AR-based STEM to support the scientific literacy of students. Searches through relevant sources, the online database of which seven ACM Digital Library, ERIC, ieeexplore, ISI Web of Science, ScienceDirect, Scopus, and Springer. The results showed that the use of STEM-AR potentially an effort to improve science literacy. © 2019 Published under licence by IOP Publishing Ltd.",43,"STEM (science, technology, engineering and mathematics)",Augmented reality - Digital libraries,"Ar development - Online database - Science , technology , engineering , and mathematics - Science literacy - Scientific literacy - Vocational education - Web of Science","723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A comparative study of augmented reality SDKs to develop an educational application in chemical field,"Hanafi, Anasse (1); Elaachak, Lotfi (1); Bouhorma, Mohammed (1) ","(1) LIST Laboratory, UAE University, P.O. Box 416, Morocco ",ACM International Conference Proceeding Series,,Association for Computing Machinery,v Part F148154,,2019,,2019,,,,9.78145E+12,10.1145/3320326.3320386,53,"2nd International Conference Networking, Information Systems and Security, NISS 2019","March 27, 2019 - March 29, 2019",,"Augmented Reality 'AR' is an increasingly pervasive newcomer in higher-education; it offers good advantages for learners to learn anywhere and at any time. Working in this new promoter research area can be attractive for researchers, due in one hand, to the lack of such applications used in education, on the other hand to the higher quality of the developed applications. In this paper, we present a comparative study concerning some AR software development kits 'SDKs', this comparison will be based on several significant criteria, in order, to choose the most suitable SDK adapted for the learning process in higher education, more especially the chemical scope. Thereafter, according to the analysis of the obtained results, we will develop an educational AR application based on the chosen SDK. The proposed application combines Augmented Reality techniques, game engine utilities, rendering of both 2D and 3D resources, and objects detection and tracking algorithm, and its main objective is to allow learners to learn new concepts in organic chemistry e.g. 'stereochemistry', and to understand the compositions of both atoms and molecules in 3D environment. © 2019 Association for Computing Machinery.",18,Augmented reality,Learning systems - Object detection - Software design - Three dimensional computer graphics,2D and 3D resources - Comparative studies - Developed applications - Educational Applications - Game Engine - Learning process - Organic Chemistry - Software development kit,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Extracting relevant information using handheld augmented reality,"Haque, Rehnuma (1); Mariful Islam, M.D. (1); Salma, Sarmin (1); Abdullah-Al-Jubair, Md. (2); Weng, Ng Giap (3) ","(1) Computer Science and Engineering, American International University-Bangladesh, Dhaka, Bangladesh (2) Department of Computer Science, American International University-Bangladesh, Dhaka, Bangladesh (3) Faculty of Computing and Informatics, University Malaysia, Sabah Sabah, Malaysia ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,10-Jan-20,"International Conference on Computing Advancements: 'Age of Computing and Augmented Life', ICCA 2020",2020,,,,9.78145E+12,10.1145/3377049.3377069,,"2020 International Conference on Computing Advancements: Age of Computing and Augmented Life, ICCA 2020","January 10, 2020 - January 12, 2020",,"Augmented Reality (AR) technology is being incorporated into education materials to attract students and to make the learning experience more engaging. This study focuses on the development of 3D object, audio-visual and interaction in Handheld AR. This research aims to bridge that gap using Handheld AR for a magazine, which allows students to get an overview and interact with the 3D model of the campus, view general information and events of the university. This magazine also benefits students that live outside Dhaka, who are unable to visit the campus beforehand. The users can use their Android phone camera for real-time video capture and render virtual objects in the augmented environment through Vuforia and Unity engine integration. To evaluate system effectiveness and user satisfaction, a survey is conducted. The survey consists of user background information, functionality tests and a user feedback questionnaire. The outcome of the survey shows satisfactory of the successful implementation of 3D and multimedia modules. This paper also discusses the future scopes and summarizes how to extract relevant information for students to gain knowledge and get entertainment by using handheld AR. © 2020 Association for Computing Machinery.",20,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ChalkboARd: Exploring augmented reality for public displays,"Gruenefeld, Uwe (1); Wolff, Torge (2); Diekmann, Niklas (2); Koelle, Marion (2); Heuten, Wilko (1) ","(1) OFFIS, Institute for IT, Oldenburg, Germany (2) University of Oldenburg, Oldenburg, Germany ","Proceedings - Pervasive Displays 2019 - 8th ACM International Symposium on Pervasive Displays, PerDis 2019",,"Association for Computing Machinery, Inc",,,12-Jun-19,"Proceedings - Pervasive Displays 2019 - 8th ACM International Symposium on Pervasive Displays, PerDis 2019",2019,,,,9.78145E+12,10.1145/3321335.3324929,a19,"8th ACM International Symposium on Pervasive Displays, PerDis 2019","June 12, 2019 - June 14, 2019",,"Augmented Reality (AR) devices and applications are gaining in popularity, and - with recent trends such as Pokemon Go - are venturing into public spaces where they become more and more pervasive. In consequence, public AR displays might soon be part of our cityscapes and may impact on our everyday view of the world. In this work, we present ChalkboARd, a prototype of an AR-enabled public display that seamlessly integrates into its environment. We investigate the influence of our system on the attention of bystanders in a field study (N=20). The field deployment of ChalkboARd provides evidence that AR for public displays needs to be interactive and adaptive to their surroundings, while at the same time taking privacy issues into account. Nevertheless, ChalkboARd was received positively by the participants, which points out the (hidden) potential of public AR displays. © 2019 ACM.",32,Augmented reality,Display devices - Human computer interaction,AR display - Attention shifts - Field deployment - Field studies - Privacy issue - Public display - Public space - Recent trends,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An evaluation of augmented reality music notation,"Liu, Zeruo (1); Adcock, Matt (2); Gardner, Henry (1) ","(1) RS Computer Science, CECS, Australian National University, Canberra; ACT; 2600, Australia (2) Data 61, Canberra; ACT; 2600, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365748,a38,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,We conducted a focus group study of a prototype application to test the opportunities and limitations of augmented reality music notation for musical performance and rehearsal. © 2019 Association for Computing Machinery.,8,Augmented reality,Helmet mounted displays - Interactive computer graphics - Virtual reality,Focus groups - Head mounted displays - Headset - Hololens - Music - Musical notation - Musician,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
SnapChart: An augmented reality analytics toolkit to enhance interactivity in a collaborative environment,"Jing, Allison (1); Xiang, Chenyang (2); Kim, Seungwon (1); Billinghurst, Mark (1); Quigley, Aaron (2) ","(1) University of South Australia, Australia (2) University of St.Andrews, United Kingdom ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365725,a55,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"Collaborative Immersive Analytics (IA) is a tool which enables multiple people to explore the same dataset using immersive technologies, such as Augmented Reality (AR) or Virtual Reality (VR). In this poster, we describe a system which uses AR to provide situated 3D visualisations in a practical agile collaborative setting. Through a preliminary user study we found that our system helps users accept the concept of IA while enhancing engagement and interactivity during AR collaboration. © 2019 Association for Computing Machinery.",3,Augmented reality,Human computer interaction - Interactive computer graphics - Virtual reality - Visualization,Collaborative environments - Collaborative settings - Immersive - Immersive technologies - Interactivity - Multiple people - User study,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Situated storytelling with SLAM enabled augmented reality,"Ketchell, Sarah (1); Chinthammit, Winyu (2); Engelke, Ulrich (3) ","(1) School of Technology, Environments and Design, University of Tasmania, Hobart; TAS; 7001, Australia (2) Human Interface Technology Laboratory, University of Tasmania, Launceston; TAS; 7250, Australia (3) Commonwealth Scientific and Industrial Research Organisation (CSIRO), Data61, Kensington; WA; 6152, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365681,a35,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"This paper addresses the feasibility of situated storytelling using Simultaneous Localisation and Mapping (SLAM) enabled augmented reality (AR) on a mobile phone.We specifically focus on storytelling in the heritage context as it provides a rich environment for stories to be told in.We conducted expert interviews with several museum and heritage sites to identify major themes for storytelling in the heritage context. These themes informed the development of an AR based storytelling application for a mobile phone. We evaluated the application in a user study and gained further insight into the factors that users appreciate in AR based storytelling. From these insights we derive several high level design guidelines that may inform future system development for situated storytelling, especially in the heritage context. © 2019 Association for Computing Machinery.",30,Augmented reality,Cellular telephones - Interactive computer graphics - Mapping - Virtual reality,Heritage - Heritage sites - High-level design - Simultaneous localisation and mappings - Storytelling - System development - User study,"405.3 Surveying - 718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Reducing latency in a collaborative augmented reality service,"He, Wennan (1); Swift, Ben (1); Gardner, Henry (1); Xi, Mingze (2); Adcock, Matt (2) ","(1) Australian National University, Canberra; ACT, Australia (2) Commonwealth Scientific and Industrial Research Organisation, Canberra; ACT, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365699,a1,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"We show how inter-device location tracking latency can be reduced in an Augmented Reality (AR) service that uses Microsoft's HoloLens (HL) devices for multi-user collaboration. Specifically, we have built a collaborative AR system for a research greenhouse that allows multiple users to be able to work collaboratively to process and record information about individual plants in the greenhouse. In this system, we have combined the HL 'world tracking' functionality together with marker-based tracking to develop a onefor- A ll-shared-experience (OFALL-SE) dynamic object localization service. We compare this OFALL-SE service with the traditional Local Anchor Transfer (LAT) method for managing shared experiences and show that latency of data transmission throughout the server and users can be dramatically reduced. Our results indicate that OFALL-SE can support near-real-time collaboration when sharing the physical locations of the plants among users in a greenhouse. © 2019 Association for Computing Machinery.",23,Augmented reality,Greenhouses - Interactive computer graphics - Interfaces (materials) - Virtual reality,Collaborative - Collaborative augmented realities - Latency - Marker-based tracking - Multi-user collaboration - Physical locations - Research greenhouse - Shared experiences,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 821.6 Farm Buildings and Other Structures - 951 Materials Science",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Gaze direction visualization techniques for collaborative wide-Area model-free augmented reality,"Li, Yuan (1); Lu, Feiyu (1); Lages, Wallace S. (2); Bowman, Doug A. (2) ","(1) Center for HCI, Dept. of Computer Science, Virginia Tech, Blacksburg; VA, United States (2) Center for HCI, School of Visual Arts, Virginia Tech, Blacksburg; VA, United States ",Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,10.1145/3357251.3357583,a11,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,"In collaborative tasks, it is often important for users to understand their collaborator's gaze direction or gaze target. Using an augmented reality (AR) display, a ray representing the collaborator's gaze can be used to convey such information. In wide-Area AR, however, a simplistic virtual ray may be ambiguous at large distances, due to the lack of occlusion cues when a model of the environment is unavailable.We describe two novel visualization techniques designed to improve gaze ray effectiveness by facilitating visual matching between rays and targets (Double Ray technique), and by providing spatial cues to help users understand ray orientation (Parallel Bars technique). In a controlled experiment performed in a simulated AR environment, we evaluated these gaze ray techniques on target identification tasks with varying levels of difficulty. The experiment found that, assuming reliable tracking and an accurate collaborator, the Double Ray technique is highly effective at reducing visual ambiguity, but that users found it difficult to use the spatial information provided by the Parallel Bars technique. We discuss the implications of these findings for the design of collaborative mobile AR systems for use in large outdoor areas. © 2019 Association for Computing Machinery.",38,Augmented reality,Visualization,Awareness - Collaboration - Collaborative tasks - Controlled experiment - Novel visualizations - Spatial informations - Target identification - Visualization technique,"723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: ONR, Sponsor: Office of Naval Research; ",The authors would like to thank the anonymous reviewers for their valuable comments and helpful suggestions. This work has been supported by the Office of Naval Research.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Spotlight on off-screen points of interest in handheld augmented reality: Halo-based techniques,"Perea, Patrick (1, 2); Morand, Denis (2); Nigay, Laurence (1) ","(1) Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, Grenoble; F-38000, France (2) Schneider Electric, Carros; F-06510, France ",ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 43-54,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,10.1145/3343055.3359719,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"Navigating Augmented Reality (AR) environments with a handheld device often requires users to access digital contents (i.e. Points of Interests - POIs) associated with physical objects outside the field of view of the device's camera. Halo3D is a technique that displays the location of off-screen POIs as halos (arcs) along the edges of the screen. Halo3D reduces clutter by aggregating POIs but has not been evaluated. The results of a first experiment show that an enhanced version of Halo3D was 18% faster than the focus+context technique AroundPlot* for pointing at a POI, and perceived as 34% less intrusive than the arrow-based technique Arrow2D. The results of a second experiment in more realistic settings reveal that two variants of Halo3D that show the spatial distribution of POIs in clusters (1) enable an effective understanding of the off-screen environment and (2) require less effort than AroundPlot* to find POIs in the environment. © 2019 Association for Computing Machinery.",23,Augmented reality,Flow visualization,Digital contents - Focus+context techniques - Halo - Hand held device - Handheld augmented realities - Physical objects - Point of interest - Points of interest,"631.1 Fluid Flow, General - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Poster: System architecture for progressive augmented reality,"Han, Yunha (1); Lee, Chunggi (1); Kim, Sanghoon (1); Ko, Sungahn (1) ","(1) Ulsan National Institute of Science and Technology, Ulsan, Korea, Republic of ","MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 522-523,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,10.1145/3307334.3328605,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,"In spite of the evolution of Augmented Reality~(AR) technology, it is not wide spread in everyday life. There may be many reasons, but one of the reasons is that it has been developed for very specific users, such as researchers and professionals. To overcome this problem, Grubert et al. proposed the pervasive AR. It is not limited to a specific situation, but is usable in various instances and providing continuous and flexible AR experience. The AR browser is the example of utilizing pervasive AR. The AR browser understands the context of the user and provides corresponding information. However, if the corresponding information to the context of user cannot reach the user in time due to massive data transmission, unexpected network congestion, poor service quality or signal strength, it cannot be guaranteed to be continuous. This leads to a degradation of the user experience, and it cannot support pervasive AR. This paper presents the Progressive Augmented Reality, the way which quickly send incomplete, yet informative, response about the user's current context rather than wait for sending complete information to the user. The concept of Progressive Augmented Reality comes from Progressive Data Science. Our system is aware of network quality by collecting various network health parameters. According to the network status quality, it divides the chunk information to the optimal number and transmits the one that have the highest priority among the divided information. By conducting the above process iteratively, all the divided information is updated. Our client-side system utilizes Android ARCore and has been tested on Google Pixel 2XL. © 2019 Copyright held by the owner/author(s).",6,Augmented reality,,Complete information - Health parameters - Network congestions - Network qualities - Service Quality - Signal strengths - System architectures - User experience,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Analysis of peripheral vision and vibrotactile feedback during proximal search tasks with dynamic virtual entities in augmented reality,"Richards, Kendra (1); Mahalanobis, Nikhil (1); Kim, Kangsoo (1); Schubert, Ryan (1); Lee, Myungho (1); Daher, Salam (1); Norouzi, Nahal (1); Hochreiter, Jason (1); Bruder, Gerd (1); Welch, Gregory F. (1) ","(1) University of Central Florida, Orlando; FL, United States ",Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,10.1145/3357251.3357585,a3,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,"A primary goal of augmented reality (AR) is to seamlessly embed virtual content into a real environment. There are many factors that can affect the perceived physicality and co-presence of virtual entities, including the hardware capabilities, the fidelity of the virtual behaviors, and sensory feedback associated with the interactions. In this paper, we present a study investigating participants' perceptions and behaviors during a time-limited search task in close proximity with virtual entities in AR. In particular, we analyze the effects of (i) visual conflicts in the periphery of an optical see-Through head-mounted display, a Microsoft HoloLens, (ii) overall lighting in the physical environment, and (iii) multimodal feedback based on vibrotactile transducers mounted on a physical platform. Our results show significant benefits of vibrotactile feedback and reduced peripheral lighting for spatial and social presence, and engagement. We discuss implications of these effects for AR applications. © 2019 Association for Computing Machinery.",38,Augmented reality,Helmet mounted displays - Lighting - Sensory feedback,Field of views - Multimodal feedback - Optical see-through head-mounted displays - Peripheral vision - Physical environments - Search tasks - Time-limited search - Vibro-tactile feedbacks,"461.5 Rehabilitation Engineering and Assistive Technology - 723 Computer Software, Data Handling and Applications",,,"Number: 1564065, Acronym: NSF, Sponsor: National Science Foundation; Number: 1852002, Acronym: IIS, Sponsor: Division of Information and Intelligent Systems; Number: Welch, Acronym: ONR, Sponsor: Office of Naval Research; Number: -, Acronym: SU, Sponsor: Stanford University; Number: -, Acronym: UF, Sponsor: University of Florida; Number: -, Acronym: UCF, Sponsor: University of Central Florida; ","This material includes work supported in part by the National Science Foundation under Award Number 1564065 (Dr. Ephraim P. Glinert, IIS), Award Number 1852002 (Dr. Harriet G. Taylor and Dr. Jonathan M. Sprinkle, CNS), and Collaborative Award Numbers 1800961, 1800947, and 1800922 (Dr. Tonya Smith-Jackson, IIS) to the University of Central Florida, University of Florida, and Stanford University respectively; the Office of Naval Research under Award Number N00014-17-1-2927 (Dr. Peter Squire, Code 34); and the AdventHealth Endowed Chair in Healthcare Simulation (Prof. Welch). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the supporting institutions.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
What can we learn from augmented reality (AR)? Benefits and Drawbacks of AR for Inquiry-based Learning of Physics,"Radu, Iulian (1); Schneider, Bertrand (1) ","(1) Graduate School of Education, Harvard University, Cambridge; MA, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300774,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Emerging technologies such as Augmented Reality (AR), have the potential to radically transform education by making challenging concepts visible and accessible to novices. In this project, we have designed a Hololens-based system in which collaborators are exposed to an unstructured learning activity in which they learned about the invisible physics involved in audio speakers. They learned topics ranging from spatial knowledge, such as shape of magnetic fields, to abstract conceptual knowledge, such as relationships between electricity and magnetism. We compared participants’ learning, attitudes and collaboration with a tangible interface through multiple experimental conditions containing varying layers of AR information. We found that educational AR representations were beneficial for learning specific knowledge and increasing participants’ self-efficacy (i.e., their ability to learn concepts in physics). However, we also found that participants in conditions that did not contain AR educational content, learned some concepts better than other groups and became more curious about physics. We discuss learning and collaboration differences, as well as benefits and detriments of implementing augmented reality for unstructured learning activities. © 2019 Association for Computing Machinery.",38,Augmented reality,Human engineering,Collaboration differences - Collaborative learning - Conceptual knowledge - Emerging technologies - Experimental conditions - Inquiry-based learning - Physics education - Unstructured learning,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality views for occluded interaction,"Lilija, Klemen (1); Pohl, Henning (1); Boring, Sebastian (1); Hornbæk, Kasper (1) ","(1) University of Copenhagen, Copenhagen, Denmark ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300676,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"We rely on our sight when manipulating objects. When objects are occluded, manipulation becomes difficult. Such occluded objects can be shown via augmented reality to re-enable visual guidance. However, it is unclear how to do so to best support object manipulation. We compare four views of occluded objects and their effect on performance and satisfaction across a set of everyday manipulation tasks of varying complexity. The best performing views were a see-through view and a displaced 3D view. The former enabled participants to observe the manipulated object through the occluder, while the latter showed the 3D view of the manipulated object offset from the object’s real location. The worst performing view showed remote imagery from a simulated hand-mounted camera. Our results suggest that alignment of virtual objects with their real-world location is less important than an appropriate point-of-view and view stability. © 2019 Copyright held by the owner/author(s).",39,Augmented reality,Cameras - Human engineering,Manipulated objects - Manipulation task - Object manipulation - Occluded objects - Occluder - Real-world - Virtual objects - Visual guidance,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 742.2 Photographic Equipment",,,"Number: 648785, Acronym: ERC, Sponsor: H2020 European Research Council; ",This project has received funding from the European Research Council (ERC) under the European Union&rsquo;s Horizon 2020 research and innovation program (grant agreement 648785).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Exploring virtual agents for augmented reality,"Wang, Isaac (1); Smith, Jesse (1); Ruiz, Jaime (1) ","(1) Department of CISE, University of Florida, Gainesville; FL, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300511,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Prior work has shown that embodiment can benefit virtual agents, such as increasing rapport and conveying nonverbal information. However, it is unclear if users prefer an embodied to a speech-only agent for augmented reality (AR) headsets that are designed to assist users in completing real-world tasks. We conducted a study to examine users’ perceptions and behaviors when interacting with virtual agents in AR. We asked 24 adults to wear the Microsoft HoloLens and find objects in a hidden object game while interacting with an agent that would offer assistance. We presented participants with four different agents: voice-only, non-human, full-size embodied, and a miniature embodied agent. Overall, users preferred the miniature embodied agent due to the novelty of his size and reduced uncanniness as opposed to the larger agent. From our results, we draw conclusions about how agent representation matters and derive guidelines on designing agents for AR headsets. © 2019 Association for Computing Machinery.",62,Augmented reality,Human engineering,Designing agents - Embodied agent - Embodied conversational agent - Hidden objects - Larger agents - Non-verbal information - Real-world task - Virtual agent,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,"Number: IIS-1750840, Acronym: NSB, Sponsor: National Science Board; Number: W911NF-15-1-0459, Acronym: ARO, Sponsor: Army Research Office; ","This work is partially supported by the U.S. Defense Advanced Research Projects Agency (DARPA) and the U.S. Army Research Office (ARO) under contract #W911NF-15-1-0459, and the National Science Foundation Grant Award #IIS-1750840. Opinions, findings, and conclusions expressed in this paper are those of the authors and do not necessarily reflect these agencies&rsquo; views.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Personalising the TV Experience using Augmented Reality An Exploratory Study on Delivering Synchronised Sign Language Interpretation,"Vinayagamoorthy, Vinoba (1); Glancy, Maxine (1); Ziegler, Christoph (2); Schäffer, Richard (2) ","(1) BBC R and D, London, United Kingdom (2) IRT, Munich, Germany ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300762,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Augmented Reality (AR) technology has the potential to extend the screen area beyond the rigid frames of televisions. The additional display area can be used to augment televisions (TVs) with extra information tailored to individuals, for instance, the provision of access services like sign language interpretations. We invited 23 (11 in the UK, 12 in Germany) users of signed content to evaluate three methods of watching a sign language interpreted programme – one traditional in-vision method with signed programme content on TV and two AR-enabled methods in which an AR sign language interpreter (a ‘half-body’ version and a ‘full-body’ version) is projected just outside the frame of the TV presenting the programme. In the UK, participants were split 3-ways in their preferences while in Germany, half the participants preferred the traditional method followed closely by the ‘half-body’ version. We discuss our participants reasoning behind their preferences and implications for future research. © 2019 Copyright held by the owner/author(s).",48,Augmented reality,Binary alloys - Human engineering - Synchronization - Television,Accessibility - Connected experiences - HbbTV 2.0 - HoloLens - Interaction techniques - Personalisation - Second screens - Sign language,"461.4 Ergonomics and Human Factors Engineering - 716.4 Television Systems and Equipment - 723 Computer Software, Data Handling and Applications - 961 Systems Science",,,"Number: 687655, Acronym: RSE, Sponsor: Royal Society of Edinburgh; Number: H2020-ICT-2015, Acronym: EGU, Sponsor: European Geosciences Union; ",This research was funded through the European Union&rsquo;s H2020-ICT-2015 programme under grant agreement number 687655 (2-IMMERSE). We thank our colleagues from BBC R&D and IRT who advised on this project.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Affinity lens data-assisted affinity diagramming with augmented reality,"Subramonyam, Hariharan (1); Drucker, Steven M. (2); Adar, Eytan (1) ","(1) University of Michigan, Ann Arbor; MI, United States (2) Microsoft Research, Redmond; WA, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300628,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Despite the availability of software to support Affinity Diagramming (AD), practitioners still largely favor physical sticky-notes. Physical notes are easy to set-up, can be moved around in space and offer flexibility when clustering unstructured data. However, when working with mixed data sources such as surveys, designers often trade off the physicality of notes for analytical power. We propose Affinity Lens, a mobile-based augmented reality (AR) application for Data-Assisted Affinity Diagramming (DAAD). Our application provides just-in-time quantitative insights overlaid on physical notes. Affinity Lens uses several different types of AR overlays (called lenses) to help users find specific notes, cluster information, and summarize insights from clusters. Through a formative study of AD users, we developed design principles for data-assisted AD and an initial collection of lenses. Based on our prototype, we find that Affinity Lens supports easy switching between qualitative and quantitative ‘views’ of data, without surrendering the lightweight benefits of existing AD practice. © 2019 Association for Computing Machinery.",48,Augmented reality,Economic and social effects - Human engineering,Affinity diagram - Design Principles - Just in time - Mixed data sources - Sticky notes - Trade off - Unstructured data - Visual analytics,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 971 Social Sciences",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Exploring the impact of network impairments on remote collaborative augmented reality applications,"Ahsen, Tooba (1); Dogar, Fahad R. (1); Gardony, Aaron L. (2) ","(1) Tufts University, United States (2) CCDC - Soldier Center, CABCS, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312774,3312774,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Our research explores the impact of network impairments on remote augmented reality (AR) collaborative tasks, and possible strategies to improve user experience in these scenarios. Using a simple AR task, under a controlled network environment, our preliminary user study highlights the impact of network outages on user workload and experience, and how user roles and learning styles play a role in this regard. © 2019 Copyright held by the owner/author(s).",16,Augmented reality,Human engineering,Collaboration - Collaborative augmented realities - Collaborative tasks - Learning Style - Network environments - Network impairments - Network outages - User experience,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,"Research was partially sponsored by NSF (Award: 1618321) and the Combat Capabilities Development Command Soldier Center (under Cooperative Agreement Number W911QY-15-2-0001). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, the Combat Capabilities Development Command Soldier Center, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
What you sketch is what you get: Quick and easy augmented reality prototyping with PINTar,"Gasques, Danilo (1); Sharkey, Thomas (1); Johnson, Janet G. (1); Weibel, Nadir (1) ","(1) University of California, San Diego; CA; 92093, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312847,3312847,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Augmented Reality(AR) tools are currently primarily targeted at programmers, making designing for AR challenging and time-consuming. We developed an interactive prototype, PintAR, that enables the authoring and rapid-prototyping of situated experiences by allowing designers to bring their ideas to life using a digital pen for sketching and a head-mounted display for visualizing and interacting with virtual content. In this paper, we explore the versatility such a tool could provide through case studies of a researcher, an artist, a ballerina, and a clinician. © 2019 Copyright held by the owner/author(s).",10,Augmented reality,Helmet mounted displays - Human engineering - Software prototyping,Case-studies - Digital pens - Head mounted displays - HoloLens - Sketching,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 723.1 Computer Programming",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Virtual objects in the physical world: Relatedness and psychological ownership in augmented reality,"Poretski, Lev (1); Arazy, Ofer (1); Lanir, Joel (1); Shahar, Shalev (1); Nov, Oded (2) ","(1) University of Haifa, Israel (2) New York University, NY, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300921,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"As technology advances, people increasingly interact with virtual objects in settings such as augmented reality (AR) where the virtual layer is superimposed on top of the physical world. Similarly to interactions with physical objects, users may assign virtual objects with value, experience a sense of relatedness, and develop psychological ownership over these objects. The objective of this study is to understand how AR’s unique characteristics influences the emergence of meaning and ownership perceptions amongst users. We conducted a study of users’ interactions with a virtual dog over a three-week period, comparing AR and fully virtual settings. Our findings show that engagement with the application is a key determinant of the relation users develop with virtual objects. However, the effect of the background layer–whether physical or virtual–dominates the development of relatedness and ownership feelings, highlighting the importance of the 'real' physical layer in shaping users’ perceptions. © 2019 Copyright is held by the owner/author(s).",71,Augmented reality,Human engineering - Network layers,Material cultures - Ownership - Qualitative analysis - Relatedness - Virtual possessions,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Urban noises: Augmented reality and urban art workshop,"Casimiro, Giovanna Graziosi (1) ","(1) University of Sao Paulo, Sao Paulo, Brazil ","Proceedings - Pervasive Displays 2019 - 8th ACM International Symposium on Pervasive Displays, PerDis 2019",,"Association for Computing Machinery, Inc",,,12-Jun-19,"Proceedings - Pervasive Displays 2019 - 8th ACM International Symposium on Pervasive Displays, PerDis 2019",2019,,,,9.78145E+12,10.1145/3321335.3329678,a41,"8th ACM International Symposium on Pervasive Displays, PerDis 2019","June 12, 2019 - June 14, 2019",,"This workshop is focused on the use of augmented reality to animate and activate graffitis of the city of Palermo. The main goal is to teach and share knowledge in digital art and augmented reality, using an easy learning process to make digital interfaces accessible to different public, ages and goals. The workshop brings the main discussion of the city as a screen of intangible interventions, using the mobile devices of the participants to overlap their visual creations on the city walls. To make it possible, we will map and photograph some of the main graffiti in the city of Palermo, working with this files to start up the interventions. The workshop outcome consists in a set of animated interactive graffiti, exploiting a free mobile app for passers-by interaction, which allows for a city tour of augmented interactive murals. © 2019 Author.",8,Augmented reality,Display devices - Human computer interaction,Digital art - Digital interfaces - Graffiti - Learning process - Mobile app - Share knowledge - Urban noise,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
PintAR: Sketching spatial experiences in augmented reality,"Gasques, Danilo (1); Johnson, Janet G. (1); Sharkey, Thomas (1); Weibel, Nadir (1) ","(1) UC San Diego, San Diego; CA; 92092, United States ",DIS 2019 Companion - Companion Publication of the 2019 ACM Designing Interactive Systems Conference,Adobe; Google; Sketch; UC San Diego's Design Lab; Virginia Tech,"Association for Computing Machinery, Inc",,p 17-20,18-Jun-19,DIS 2019 Companion - Companion Publication of the 2019 ACM Designing Interactive Systems Conference,2019,,,,9.78145E+12,10.1145/3301019.3325158,,"2019 ACM Conference on Designing Interactive Systems, DIS 2019","June 23, 2019 - June 28, 2019",,"In this demo, we present PintAR: an interactive prototyping tool that explores Augmented Reality for the design of interactive spatial experiences. Our system aims to remedy the lack of tools for rapid-prototyping situated experiences in AR without programming or 3D modeling. PintAR combines a digital paper and pen interface with a head-mounted display to allow users to sketch and interact with digital content in their environment. Users can take PintAR anywhere, leveraging objects and information available in the real world to bring context to their prototypes. Copyright held by the owner/author(s).",5,Augmented reality,3D modeling - Helmet mounted displays - Software prototyping,Digital contents - Digital paper - Head mounted displays - Interactive prototyping - Pen interfaces - PintAR - Real-world - Sketching,"723 Computer Software, Data Handling and Applications - 723.1 Computer Programming",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Mixedux: A mixed prototyping framework for usability testing in augmented reality,"Morozova, Alyona (1); Rheinstädter, Verena (1); Wallach, Dieter (1) ","(1) Ergosign GmbH, Europaallee 20a, Saarbrücken; 66113, Germany ",DIS 2019 Companion - Companion Publication of the 2019 ACM Designing Interactive Systems Conference,Adobe; Google; Sketch; UC San Diego's Design Lab; Virginia Tech,"Association for Computing Machinery, Inc",,p 41-44,18-Jun-19,DIS 2019 Companion - Companion Publication of the 2019 ACM Designing Interactive Systems Conference,2019,,,,9.78145E+12,10.1145/3301019.3325146,,"2019 ACM Conference on Designing Interactive Systems, DIS 2019","June 23, 2019 - June 28, 2019",,"In this paper, we rethink and challenge conventional ways of prototyping. We present a new perspective on how usability testings of digital products may benefit from emerging augmented reality (AR) technologies. We demonstrate a conceptual prototype of an innovative framework that makes it possible to combine 3D models of complex devices, represented by holograms, with 2D user interfaces (UIs) opened in a web browser on a physical touch display. The framework aims to facilitate the processes of UI design and interactions with complex, costly, or even not-yet-existing systems. For demonstration purposes we use the Microsoft HoloLens Development Edition and a conventional touch display of a smartphone. © 2019 Copyright is held by the owner/author(s).",9,Augmented reality,Display devices - Holographic displays - User interfaces,Complex devices - Digital products - Existing systems - HoloLens - MicroSoft - UI designs - Usability testing,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",,,ACM International Conference Proceeding Series,"Macquarie University-Sydney; The University of Western Australia, Department of Electronic Engineering",Association for Computing Machinery,,,23-Feb-19,"Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019",2019,,,,9.78145E+12,,,"3rd International Conference on Virtual and Augmented Reality Simulations, ICVARS 2019","February 23, 2019 - February 25, 2019",,The proceedings contain 17 papers. The topics discussed include: influence of interactive questions on the sense of presence and anxiety in a virtual-reality job-interview; development of chemical incident response training program by applying virtual reality technology; therapeutic virtual reality for nyctophobic disorder; virtual reality for anatomical vocabulary learning; development of dual cognitive task virtual reality game addressing stroke rehabilitation; game cinematography - towards understanding relationship between spatial distortion and game-play; a new method to render virtual walls for haptic systems: ’tracking wall’. application to needle insertion simulation; collaborative hands-on training on haptic simulators; PD pattern recognition in transformers based on greyscale images and affinity propagation algorithm; development of automated platform for image capturing and counting algorithm for viral plaque; and design of an autonomous sentry gun system for the detection of people in restricted zones.,,,,,,,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Dataminerschema: A mobile learning application for analytical modeling primer utilizing augmented reality technique,"Pagaduan, Roxanne A. (1); Caliwag, Jasmin A. (1); Castillo, Reynaldo E. (1); Felonia, Paulo E. (1); Gonzales, Rose Ann C. (1) ","(1) Technological Institute of the Philippines, Quezon City, Philippines ",ACM International Conference Proceeding Series,International Association of Computer Science and Information Technology; Tokai University,Association for Computing Machinery,v Part F148384,p 130-134,2019,,2019,,,,9.78145E+12,10.1145/3322645.3322698,,"2nd International Conference on Information Science and System, ICISS 2019","March 16, 2019 - March 19, 2019",,"DataMinerSchema is a Mobile learning application intended for students who are taking up data mining and warehousing course. The purpose of this study is to inform and give knowledge to the user about data warehousing specifically three types of analytical modeling primer such as star schema, snowflake schema, and fact constellation schema. DataMinerSchema includes discussion, images, graphics, and video presentation. Augmented Reality (AR) technology was implemented in data mining and data warehousing (DMDW) book to display information about the analytical modeling primer. The developers evaluated IT professionals, mobile application developers, and IT students. DataMinerSchema was evaluated using ISO 9126 Standard measuring its quality in terms of functionality, reliability, usability, and portability. Based on Likert’s scale, the users’ evaluation result was strongly agreed. © 2019 Association for Computing Machinery.",19,Data mining,Analytical models - Augmented reality - Computer aided instruction - Data warehouses - E-learning - Quality control - Warehouses,Evaluation results - Iso 9126 - IT professional - Mobile applications - Mobile Learning - Star schema - Video presentations,"694.4 Storage - 723 Computer Software, Data Handling and Applications - 913.3 Quality Assurance and Control - 921 Mathematics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Display methods of projection augmented reality based on deep learning pose estimation,"Ro, Hyocheol (1); Byun, Jung-Hyun (1); Park, Yoon Jung (1); Han, Tack-Don (1) ","(1) Media System Lab, Yonsei University, Seoul, Korea, Republic of ","ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3306214.3338608,,"ACM SIGGRAPH 2019 Posters - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"In this paper, we propose three display methods for projection-based augmented reality. In spatial augmented reality (SAR), determining where information, objects, or contents are to be displayed is a difficult and important issue. We use deep learning models to estimate user pose and suggest ways to solve the issue based on this data. Finally, each method can be appropriately applied according to various the applications and scenarios. © 2019 Copyright held by the owner/author(s).",8,Deep learning,Augmented reality - Interactive computer graphics - Mixed reality,Learning models - Pose estimation - Spatial augmented realities,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: No.NRF-2018R1A2A1A05078628, Acronym: MSIP, Sponsor: Ministry of Science ICT and Future Planning; Number: -, Acronym: NRF, Sponsor: National Research Foundation of Korea; ",This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIP) (No.NRF-2018R1A2A1A05078628).,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
SmAR2T: A models at runtime architecture to interact with the web of things using augmented reality,"Bezerra, Jagni Dasa Horta (1); De Souza, Cidcley Teixeira (1) ","(1) IFCE, Fortaleza, Brazil ",ACM International Conference Proceeding Series,Google; TecnoTRENDS Tecnologia Educacional,Association for Computing Machinery,,p 124-129,23-Sep-19,"Proceedings of the 33rd Brazilian Symposium on Software Engineering, SBES 2019",2019,,,,9.78145E+12,10.1145/3350768.3353818,,"33rd Brazilian Symposium on Software Engineering, SBES 2019","September 23, 2019 - September 27, 2019",,"The Internet of Things has a high heterogeneity of devices, cloud services, communication protocols and comprises different research and development areas. This implies in ease-of-use problems for users who interact with high-density smart environments using common interface solutions, demanding specific applications for each device. There are also situations where things change their locations and networks, bringing even more complexity to this scenery. A great number of recent researches point Augmented Reality(AR) as an interface solution that is capable of providing rich user experiences for high-density IoT environments. W3C's Web of Things standards(WoT) are proving to be a feasible solution for IoT's heterogeneity issues by using well-known web protocols and the Thing Description(TD) formal model. This paper's goal is to propose an architecture called smAR2t that integrates these solutions by managing mobility, heterogeneity and high-density of smart environments, allowing users to browse their surroundings with AR and to interact with things seamlessly according to each of their TDs. This could be done by: representing TDs in a higher level of abstraction with models; transforming these models at runtime to reflect real-time interactions and behaviors; updating an auto-generated AR interface according to these models. © 2019 Association for Computing Machinery.",13,Internet of things,Augmented reality - Internet protocols - Network architecture - Software engineering,High heterogeneity - Interface solution - Level of abstraction - Model-driven Engineering - Models at run time - Models@run.time - Real time interactions - Research and development,"723 Computer Software, Data Handling and Applications - 723.1 Computer Programming",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Reducing perceived waiting time in theme park queues via an augmented reality game,"Zambetta, Fabio (1); Raffe, William (2); Tamassia, Marco (1); Mueller, Florian Floyd (3); Li, Xiaodong (1); Quinten, Niels (4); Patibanda, Rakesh (3); Dang, Daniel (1); Satterley, Jon (5) ","(1) School of Science, RMIT University, Melbourne, Australia (2) School of Software, University of Technology Sydney, Sydney, Australia (3) Exertion Games Lab, RMIT University, Melbourne, Australia (4) Leuven University College of Arts, Leuven, Belgium (5) Village Roadshaw Digital, Melbourne, Australia ",ACM Transactions on Computer-Human Interaction,,Association for Computing Machinery,"v 27, n 1",,22-Jan-20,,2020,,10730516,15577325,,10.1145/3361524,3,,,,"Theme parks visits can be very playful events for families, however, waiting in the ride's queues can often be the cause of great frustration. We developed a novel augmented reality game to be played in the theme park's queue, and an in-the-wild study with X participants using log data and interviews demonstrated that every minute playing was perceived to the same extent of about 5 minutes of not playing the game. We articulate a design space for researchers and strategies for game designers aiming to reduce perceived waiting time in queues. With our work, we hope to extend how we use games in everyday life to make our lives more playful. © 2020 Association for Computing Machinery.",59,Queueing theory,Augmented reality,Design spaces - Entertainment - Game designers - Log data - Perceived waiting time - Theme park - Waiting psychology,"723 Computer Software, Data Handling and Applications - 922.1 Probability Theory",,,"Number: LP1301007439, Acronym: ARC, Sponsor: Australian Research Council; ","This work is supported by the Australian Research Council under grant LP1301007439. Authors&rsquo; addresses: F. Zambetta, M. Tamassia, X. Li, and D. Dang, School of Science, RMIT University, Melbourne, Australia; email: {fabio.zambetta, marco.tamassia, xiaodong.li, daniel.dang}@rmit.edu.au; W. Raffe, School of Software, University of Technology Sydney, Sydney, Australia; email: william.raffe@uts.edu.au; F. &lsquo;Floyd&rsquo; Mueller and R. Patibanda, Exertion Games Lab, RMIT University, Melbourne, Australia; email: {florian.mueller, rakesh.patibanda}@rmit.edu.au; N. Quinten, Leuven University College of Arts, Leuven, Belgium; email: niels@nielsquinten.com; J. Satterley, Village Roadshaw Digital, Melbourne, Australia; email: jon_satterley@vrl.com.au. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. &copy; 2020 Association for Computing Machinery. 1073-0516/2020/01-ART3 $15.00 https://doi.org/10.1145/3361524",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Interactive visualization of painting data with augmented reality,"Yoo, Kyungjin (1); Foster, Dean (1) ","(1) University of Maryland, United States ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3365032,3365032,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Exploration of Augmented Reality technologies has increased substantially and the increase in both popularity and technological maturity has also led to several applications being developed for educational and museum environments. Specifically, a greater focus has been placed upon creating memorable experiences that both attract and educate museum patrons. Attempts to do this involve creating both Virtual Reality and Augmented Reality experiences, such as having users enter into immersive worlds that demonstrate the history of a certain time period, or applications that overlay life-like models of those animals in the very room the user is standing in. Many of these experiences are quite exceptional but begin to lack in variety when moving towards the art gallery, and mainly focus on making painting information more accessible. In an attempt to address this, this project outlines the design and evaluation of a proof-of-concept meant to study if adding interaction through Augmented Reality to paintings themselves would be both technologically feasible and desirable. © 2019 Copyright held by the owner/author(s).",1,Augmented reality,Data visualization - Flow visualization - Painting - Virtual reality - Visualization,Art gallery - Augmented reality technology - Design and evaluations - Interactive - Interactive visualizations - OR applications - Proof of concept - Time-periods,"631.1 Fluid Flow, General - 723 Computer Software, Data Handling and Applications - 813.1 Coating Techniques",,,"Number: -, Acronym: UMD, Sponsor: University of Maryland; ","The authors would like to acknowledge The Phillips Collection in Washington, D.C. for resources regarding artwork and the University of Maryland's FIRE program for the opportunity to pursue this research.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Increasing trust in fully automated driving: Route indication on an augmented reality head-up display,"Von Sawitzky, Tamara (1); Wintersberger, Philipp (2); Riener, Andreas (1); Gabbard, Joseph L. (3) ","(1) Technische Hochschule Ingolstadt, Germany (2) CARISSMA, Technische Hochschule Ingolstadt, Germany (3) COGENT Lab, Virginia Tech, Blacksburg, United States ","Proceedings - Pervasive Displays 2019 - 8th ACM International Symposium on Pervasive Displays, PerDis 2019",,"Association for Computing Machinery, Inc",,,12-Jun-19,"Proceedings - Pervasive Displays 2019 - 8th ACM International Symposium on Pervasive Displays, PerDis 2019",2019,,,,9.78145E+12,10.1145/3321335.3324947,a6,"8th ACM International Symposium on Pervasive Displays, PerDis 2019","June 12, 2019 - June 14, 2019",,"Cooperative, intelligent transportation systems (C-ITSs) have capabilities far beyond what human drivers can achieve. For instance, intelligent systems that plan the exact trajectories of vehicles could increase throughput at intersections, allowing vehicles to pass with high speed and without any need for traffic lights. It is imaginable that C-ITS will manage traffic in fully automated driving (FAD). However, the unpredictable behavior patterns of the automated vehicle (AV) are also likely to make the occupants feel uncomfortable. FAD systems operate based on a vast amount of information. As this information is potentially invisible to individuals, the question is if users trust FAD systems. We hypothesize that users experiencing such scenarios would demand system feedback to anticipate upcoming system decisions and maneuvers. Therefore, we evaluated five augmented reality (AR) user interface (UI) concepts aiming to increase system transparency in a user study (N = 30) in a driving simulation in virtual reality (VR). Our results support the assumption that feedback about the system state (in the form of route information) significantly increases trust in AVs. As trust is highly subjective, we propose to provide experience-based route visualization systems in FAD to meet individuals' needs. © 2019 ACM.",25,Head-up displays,Augmented reality - Automation - Human engineering - Intelligent systems - Intelligent vehicle highway systems - User interfaces - Vehicles - Virtual reality,Amount of information - Driving simulation - Exact trajectories - Fully automated - Intelligent transportation systems - Route indication - Route visualization - Trust,"461.4 Ergonomics and Human Factors Engineering - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731 Automatic Control Principles and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A mobile augmented reality interface for teaching folk dances,"Kico, Iris (1); Liarokapis, Fotis (1) ","(1) HCI Lab, Masaryk University, Brno, Czech Republic ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364752,3364752,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"This paper presents a prototype mobile augmented reality interface for assisting the process of learning folk dances. As a case study, a folk dance was digitized based on recordings from professional dancers. To assess the effectiveness of the technology, it was comparatively evaluated with a large back-projection system in laboratory conditions. Sixteen participants took part in the study, and their movements were captured using motion capture system and then compared with the recordings from the professional dancers. Experimental results indicate that augmented reality has the potential to be used for learning folk dances. © 2019 Copyright held by the owner/author(s).",8,Augmented reality,Virtual reality,Folk dances - Laboratory conditions - Mobile augmented reality - Motion capture system - Motion capturing - Motion tracking - Process of learning - User study,"723 Computer Software, Data Handling and Applications",,,,"This work is supported under the H2020 European Union funded project TERPSICHORE: Transforming Intangible Folkloric Performing Arts into Tangible Choreographic Digital Objects, under the grant agreement 691218.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
V.Ra: An in-situ visual authoring system for robot-IoT task planning with augmented reality,"Cao, Yuanzhi (1); Xu, Zhuangying (1); Li, Fan (2); Zhong, Wentao (1); Huo, Ke (1); Ramani, Karthik (1, 3) ","(1) School of Mechanical Engineering, Purdue University, West Lafayette; IN; 47907, United States (2) School of Mechanical Engineering, Tsinghua University, Beijing, China (3) School of Electrical and Computer Engineering, United States ",DIS 2019 - Proceedings of the 2019 ACM Designing Interactive Systems Conference,Adobe; Google; Sketch; UC San Diego's Design Lab; Virginia Tech,"Association for Computing Machinery, Inc",,p 1059-1070,18-Jun-19,DIS 2019 - Proceedings of the 2019 ACM Designing Interactive Systems Conference,2019,,,,9.78145E+12,10.1145/3322276.3322278,,"2019 ACM Conference on Designing Interactive Systems, DIS 2019","June 23, 2019 - June 28, 2019",,"We present V.Ra, a visual and spatial programming system for robot-IoT task authoring. In V.Ra, programmable mobile robots serve as binding agents to link the stationary IoTs and perform collaborative tasks. We establish an ecosystem that coherently connects the three key elements of robot task planning, the human, robot and IoT, with one single mobile AR device. Users can perform task authoring with the Augmented Reality (AR) handheld interface, then placing the AR device onto the mobile robot directly transfers the task plan in a what-you-do-is-what-robot-does (WYDWRD) manner. The mobile device mediates the interactions between the user, robot, and the IoT oriented tasks, and guides the path planning execution with the embedded simultaneous localization and mapping (SLAM) capability. We demonstrate that V.Ra enables instant, robust and intuitive room-scale navigatory and interactive task authoring through various use cases and preliminary studies. © 2019 ACM.",46,Human robot interaction,Augmented reality - Internet of things - Mobile agents - Mobile robots - Motion planning - Robot programming - Robotics,Authoring systems - Collaborative tasks - Handheld interfaces - Robot navigation - Robotic tasks - Simultaneous localization and mapping - SLAM - Spatial programming,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 731.5 Robotics",,,"Number: 1637961, Acronym: NSF, Sponsor: National Science Foundation; ","This work was partially supported by the NSF under grants FW-HTF 1839971, IIS (NRI) 1637961 and IIP (PFI:BIC) 1632154. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agency.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Using augmented reality and ontologies to co-design assistive technologies in smart homes,"Haidon, Corentin (1); Ngankam, Hubert Kenfack (1); Giroux, Sylvain (1); Pigot, Hélne (1) ","(1) Département d'Informatique, Université de Sherbrooke, Sherbrooke; QC, Canada ","International Conference on Intelligent User Interfaces, Proceedings IUI",ACM Special Interest Group on Artificial Intelligence (SIGAI); Specialist Interest Group in Computer-Human Interaction of the ACM (SIGCHI),Association for Computing Machinery,,p 126-127,17-Mar-20,Proceedings of the 25th International Conference on Intelligent User Interfaces Companion. IUI 2020,2020,,,,9.78145E+12,10.1145/3379336.3381492,,"25th International Conference on Intelligent User Interfaces, IUI 2020","March 17, 2020 - March 20, 2020",,"Smart homes provide alternative means to foster autonomy for frail people living at home. Oral and visual cues are produced to help people carrying out activities. This necessitates to determine which sensors and effectors to choose for monitoring activities, which is is not trivial. A Do-it-Yourself approach is proposed for caregivers who know the frail people habits but needs a user-friendly interaction. Augmented reality and ontologies are aimed to address many of the smart home design issues, via a virtual advisor. The augmented reality interface is linked to an OWL ontology that describes space, sensors and effectors, activities of daily living, monitoring and assistance. First, a semantic 3D model of one's house is constructed. Second, still on augmented reality, a hierarchical model of the assistance and monitoring scenario is specified. A virtual advisor proposes actions, scenarios and corrections of design inconsistencies. © 2020 International Conference on Intelligent User Interfaces, Proceedings IUI. All rights reserved.",5,User interfaces,3D modeling - Augmented reality - Automation - Hierarchical systems - Intelligent buildings - Ontology - Semantics - Virtual addresses,Activities of Daily Living - Assistive technology - Cognitive deficits - Context- awareness - Do it yourself - Hierarchical model - Monitoring activities - Smart homes,"402 Buildings and Towers - 722.1 Data Storage, Equipment and Techniques - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731 Automatic Control Principles and Applications - 961 Systems Science",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Augmented reality for children in a confirmation task: Time, fatigue, and usability","Munsinger, Brita (1); Quarles, John (1) ","(1) University of Texas at San Antonio, San Antonio; TX, United States ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364274,3364274,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"The objective of this paper is to explore three different interaction methods in a confirmation task on a head-mounted Augmented Reality (AR) device with a population of children aged 9-11 years. The three interaction methods we look at are voice recognition, gesture recognition, and controller.We conducted a within-subjects study using a Fitts' Law confirmation task performed by children with a Microsoft HoloLens. We measured elapsed time during the completion of the tasks. Also, we collected usability and fatigue measures using the System Usability Scale and the OMNI RPE (Ratings of Perceived Exertion) scale. We found significant differences between voice and controller for time, fatigue and usability. We also found significant differences between gesture and controller for time, fatigue and usability. We hope to apply the results of this study to improve augmented reality educational tools for children in the future. © 2019 Copyright held by the owner/author(s).",31,Usability engineering,Augmented reality - Virtual reality,Children - Educational tools - Fatigue measures - Fitts' law - Interaction methods - Perceived exertion - System usability - Usability studies,"723 Computer Software, Data Handling and Applications",,,"Number: IIS-1350995, Acronym: NSF, Sponsor: National Science Foundation; ","The authors wish to thank Cerina Zamora of Spicewood Park Elementary and her students. This work was supported by grants from the National Science Foundation (IIS-1648949, IIS-1350995).",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ShareAR: Communication-efficient multi-user mobile augmented reality,"Ran, Xukan (1); Slocum, Carter (1); Gorlatova, Maria (2); Chen, Jiasi (1) ","(1) University of California, Riverside, United States (2) Duke University, United States ",HotNets 2019 - Proceedings of the 18th ACM Workshop on Hot Topics in Networks,ACM SIGCOMM; Facebook; Google; VMWare,"Association for Computing Machinery, Inc",,p 109-116,13-Nov-19,HotNets 2019 - Proceedings of the 18th ACM Workshop on Hot Topics in Networks,2019,,,,9.78145E+12,10.1145/3365609.3365867,,"18th ACM Workshop on Hot Topics in Networks, HotNets 2019","November 14, 2019 - November 15, 2019",,"Augmented reality is an emerging application on mobile devices. However, there is a lack of understanding of the communication requirements and challenges of multi-user AR scenarios. In this position paper, we propose several important research issues that need to be addressed for low-latency, accurate shared AR experiences: (a) Systems tradeoffs of AR communication architectures used today in mobile AR platforms; (b) Understanding AR communication patterns and adapting the AR application layer to dynamically changing network conditions; and (c) Tools and methodologies to evaluate AR quality of experience in real time on mobile devices. We present preliminary measurements of off-the-shelf mobile AR platforms as well as results from our AR system, ShareAR, illustrating performance tradeoffs and indicating promising new research directions. © 2019 ACM.",35,Augmented reality,Commerce - Quality of service - Real time systems,Communication architectures - Communication efficiency - Communication pattern - Emerging applications - Mobile augmented reality - Network condition - Performance trade-off - SLAM,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications",,,"Number: CNS-1817216, Acronym: NSF, Sponsor: National Science Foundation; ",This work has been supported in part by NSF grants CNS-1817216 and CSR-1903136,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality-based procedural task training application for less privileged children and autistic individuals,"Singh, Katyayani (1); Srivastava, Ayushi (1); Achary, Krishnaveni (2); Dey, Arindam (3); Sharma, Ojaswa (1) ","(1) IIIT Delhi, India (2) Tamana NGO, India (3) University of Queensland, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365703,a31,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"In this work, we evaluate the applicability of using Augmented Reality applications in for enhanced learning experiences for children from less privileged backgrounds, with a focus on autistic population. Such an intervention can prove to be very useful to children with reduced cognitive development. In our evaluation, we compare the AR mode of instruction for a procedural task training, using tangram puzzles, with live demonstration and a desktopbased application. First, we performed a within-subjects user study on neurotypical children in the age group of 9-12 years. We asked the children to independently solve a tangram puzzle after being trained through different modes of instruction. Second, we used the same instruction modes to train autistic participants. Our findings indicate that during training, children took the longest time to interact with Desktop-based instruction, and took the shortest time to interact with the live demonstration mode. Children also took the longest time to independently solve the tangram puzzle in the Desktop mode. We also found that autistic participants could use AR-based instructions but required more time to go through the training. © 2019 Association for Computing Machinery.",42,Augmented reality,Interactive computer graphics - Personnel training - Virtual reality,Augmented reality applications - Autism - Children - Cognitive development - Enhanced learning - Instruction - Less privileged - Task trainings,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 912.4 Personnel",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Using augmented reality to improve productivity and safety for heavy machinery operators: State of the art,"Sitompul, Taufik Akbar (1); Wallmyr, Markus (1) ","(1) Malardalen University, CrossControl, Vasteras, Sweden ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365689,a8,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"The machinery used in industrial applications, such as in agriculture, construction, and forestry, are increasingly equipped with digital tools that aim to aid the operator in task completion, improved productivity, and enhanced safety. In addition, as machines are increasingly connected, there are even more opportunities to integrate external information sources. This situation provides a challenge in mediating the information to the operator. One approach that could be used to address this challenge is the use of augmented reality. This enables the system-generated information to be combined with the user's perception of the environment. It has the potential to enhance the operators' awareness of the machine, the surroundings, and the operation that needs to be performed. In this paper, we review the current literature to present the state of the art, discuss the possible benefits, and the use of augmented reality in heavy machinery. © 2019 Association for Computing Machinery.",47,Mixed reality,Accident prevention - Augmented reality - Digital devices - Forestry - Interactive computer graphics - Machinery - Productivity,Digital tools - External informations - Heavy machinery - Heavy vehicle - State of the art - User's perceptions,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 914.1 Accidents and Accident Prevention",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Depth perception in projective augmented reality: An evaluation of advanced visualization techniques,"Heinrich, Florian (1); Bornemann, Kai (1); Lawonn, Kai (2); Hansen, Christian (1) ","(1) University of Magdeburg, Research Campus STIMULATE, Germany (2) University of Jena, Germany ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364245,3364245,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Augmented reality (AR) is a promising tool to convey useful information at the place where it is needed. However, perceptual issues with augmented reality visualizations affect the estimation of distances and depth and thus can lead to critically wrong assumptions. These issues have been successfully investigated for video see-through modalities. Moreover, advanced visualization methods encoding depth information by displaying additional depth cues were developed. In this work, state-of-the-art visualization concepts were adopted for a projective AR setup. We conducted a user study to assess the concepts' suitability to convey depth information. Participants were asked to sort virtual cubes by using the provided depth cues. The investigated visualization concepts consisted of conventional Phong shading, a virtual mirror, depth-encoding silhouettes, pseudo-chromadepth rendering and an illustrative visualization using supporting line depth cues. Besides different concepts, we altered between a monoscopic and a stereoscopic display mode to examine the effects of stereopsis. Consistent results across variables show a clear ranking of examined concepts. © 2019 Association for Computing Machinery.",44,Visualization,Augmented reality - Depth perception - Encoding (symbols) - Flow visualization - Signal encoding - Stereo image processing - Virtual reality,Advanced visualizations - Depth information - Distance estimation - Illustrative visualization - Reality visualization - State of the art - Stereoscopic display - Virtual mirror,"631.1 Fluid Flow, General - 716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,"Number: HA 7819/1-2, Acronym: DFG, Sponsor: Deutsche Forschungsgemeinschaft; Number: ZS/2016/04/78123, Acronym: -, Sponsor: -; Number: -, Acronym: GIF, Sponsor: German-Israeli Foundation for Scientific Research and Development; Number: -, Acronym: FEDER, Sponsor: European Regional Development Fund; Number: -, Acronym: BMBF, Sponsor: Bundesministerium f&uuml;r Bildung und Forschung; Number: 13GW0095A, Acronym: BMBF, Sponsor: Bundesministerium f&uuml;r Bildung und Forschung; ","This work was partially funded by the German Research Foundation (DFG, grant numbers HA 7819/1-2 and LA 3855/1-2). It was also funded by the Federal Ministry of Education and Research (BMBF) within the STIMULATE research campus (grant number 13GW0095A) and the European Regional Development Fund (EFRE) as part of the initiative Sachsen-Anhalt WISSENSCHAFT Schwer-punkte"" (operation number ZS/2016/04/78123).""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
2D/3D mixed interface for furniture placement in Smartphone-based mobile augmented reality,"Han, Bin (1); Kim, Gerard J. (1) ","(1) Dept. of Computer Science and Engineering, Digital Experience Laboratory, Korea University, Seoul, Korea, Republic of ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364715,3364715,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"In this work, we propose to use an approximate 2D map of the environment generated from the latest environment modeling technology and enhance the object manipulation performance for the touch based mobile augmented reality. We validated the advantage of the proposed interface through a pilot experiment and confirmed that the use of the 2D map helps reduce the task completion time almost 2 times and improve the usability as well. © 2019 Association for Computing Machinery.",4,Augmented reality,Smartphones - Virtual reality,3D interaction technique - Environment modeling - Mobile augmented reality - Object manipulation - Pilot experiment - Task completion time,"718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: NRF-2016M3C1B6929724, Acronym: MSIP, Sponsor: Ministry of Science, ICT and Future Planning; Number: 10085589, Acronym: -, Sponsor: -; Number: -, Acronym: NRF, Sponsor: National Research Foundation of Korea; ","This research was supported in part by the National Research Foundation of Korea grant funded by the Ministry of Science, ICT & Future Planning for the Convergent R&D Program for Science and Technology and Liberal Arts (NRF-2016M3C1B6929724), and the Standard Technology Development and Spread Program of KATS/KEIT (10085589 - Standards Development for VR/AR based Virtual Training Systems Integration Technologies)",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality visualisation concepts to support intraoperative distance estimation,"Heinrich, Florian (1); Schmidt, Gerd (1); Hansen, Christian (1); Jungmann, Florian (2) ","(1) University of Magdeburg, Germany (2) University Medical Center of the Johannes Gutenberg, University Mainz, Germany ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364818,3364818,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"The estimation of distances and spatial relations between surgical instruments and surrounding anatomical structures is a challenging task for clinicians in image-guided surgery. Using augmented reality (AR), navigation aids can be displayed directly at the intervention site to support the assessment of distances and reduce the risk of damage to healthy tissue. To this end, four distance-encoding visualisation concepts were developed using a head-mounted optical see-through AR setup and evaluated by conducting a comparison study. Results suggest the general advantage of the proposed methods compared to a blank visualisation providing no additional information. Using a Distance Sensor concept signalising the proximity of nearby structures resulted in the least time the instrument was located below 5mm to surrounding risk structures and yielded the least amount of collisions with them. © 2019 Association for Computing Machinery.",7,Surgical equipment,Augmented reality - Damage detection - Risk assessment - Transplantation (surgical) - Virtual reality - Visualization,Anatomical structures - Comparison study - Distance estimation - Image guided surgery - Medical augmented realities - Optical see-through - Spatial relations - Surgical instrument,"462.1 Biomedical Equipment, General - 462.4 Prosthetics - 723 Computer Software, Data Handling and Applications - 914.1 Accidents and Accident Prevention",,,"Number: HA 7819/1-2, Acronym: DFG, Sponsor: Deutsche Forschungsgemeinschaft; Number: 13GW0095A, Acronym: BMBF, Sponsor: Bundesministerium f&uuml;r Bildung und Forschung; Number: ZS/2016/04/78123, Acronym: FEDER, Sponsor: European Regional Development Fund; ","This work was funded by the DFG (HA 7819/1-2). It was also funded by the Federal Ministry of Education and Research within the Research Campus STIMULATE (13GW0095A) and the European Regional Development Fund as part of the initiative Sachsen-Anhalt WISSENSCHAFT Schwerpunkte"" (ZS/2016/04/78123).""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality approach for position-based service using handheld smartphone,"Park, Jihoon (1); Park, Sangmin (1); Ko, Kwanghee (1) ","(1) Gwangju Institute of Science and Technology, Gwangju, Korea, Republic of ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364712,3364712,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"In this work, we present an augmented reality (AR) approach for position based service using a smartphone in an indoor environment. The AR method, combined with position estimation, provides a user with a smartphone with a service that is specific to a particular position without using a marker or any other hardware device. The position in an indoor environment is estimated using an IMU sensor only in the smartphone. The accuracy of the position and heading direction of the user is improved by integrating the values from the accelerometer and the gyro using Principal Component Analysis(PCA) and Extended Kalman Filter(EKF). Then, a drift noise of the estimated position is reduced by a registration step performed at a specific position. The estimated position is given to the position based service, which is provided to the user on the smartphone screen through AR. The concept of the proposed method is demonstrated with some examples. © 2019 Association for Computing Machinery.",4,Principal component analysis,Augmented reality - Extended Kalman filters - Smartphones - Virtual reality,AR method - Handhelds - Hardware devices - Heading directions - Indoor environment - Position estimation - Position-based service,"718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications - 922.2 Mathematical Statistics",,,,This research was supported by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Science and ICT(2017R1A2B4012124).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ARComposer: Authoring augmented reality experiences through text,"Shekhar, Sumit (1); Maheshwari, Paridhi (1); Monisha, J. (2); Singhal, Amrit (3); Singh, Kush Kumar (4); Krishna, Kundan (5) ","(1) Adobe Research, United States (2) IIT, Madras, India (3) IIT, Kanpur, India (4) IIT, Roorkee, India (5) Carnegie Mellon University, United States ",UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 63-65,14-Oct-19,UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332167.3357116,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Augmented Reality (AR) is rapidly gaining popularity, enhancing human perception of the real world by augmenting digital experiences. Existing tools for authoring AR scenes are either template based or require domain knowledge from experts, and are therefore restrictive. ARComposer is a novel interface that enables easy authoring of AR experiences from free-form text describing the scene. Our proposed interface allows creators to compose varied scenes comprising of multiple objects with diverse relationships to each other as well as human models with animations, starting merely with a textual description. A qualitative evaluation shows that ARComposer provides a good flow experience to its users. Furthermore, a crowd-sourced experiment evaluating various aspects of the rendered AR scenes indicates the viability of the proposed approach. © 2019 Copyright is held by the owner/author(s).",16,User interfaces,3D modeling - Augmented reality - Rendering (computer graphics) - Three dimensional computer graphics,Domain knowledge - Flow experience - Human perception - Multiple objects - Natural languages - Qualitative evaluations - Template-based - Textual description,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
ARCalVR: Augmented reality playground on mobile devices,"Zhang, Menghe (1); Lucknavalai, Karen (1); Liu, Weichen (1); Alipour, Kamran (1); Schulze, Jürgen P. (1) ","(1) University of California, San Diego; CA, United States ","ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3305365.3329732,a2,"ACM SIGGRAPH 2019 Appy Hour - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"With the development of ARKit and ARCore, mobile Augmented Reality (AR) applications have become popular. Our ARCalVR is a lightweight, open-source software environment to develop AR applications on Android devices, and it gives the programmer full control over the phone's resources. With ARCalVR, one can do 60fps marker-less AR on Android devices, including functionalities of more complex environment understanding, physical simulation, virtual object interaction and interaction between virtual objects and real environment. © 2019 Copyright Held by the Owner/Author(s).",3,Android (operating system),Application programs - Augmented reality - Interactive computer graphics - Open source software - Open systems - Virtual reality,Android - ARCore - Mobile Development - Realistic Lighting - Spherical harmonics,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
PlayGAMI: Augmented reality origami creativity platform,"Grandhi, Uttam (1); Chang, Ina Yosun (2) ","(1) PlayGAMI, Brooklyn; NY, United States (2) PlayGAMI, Orange County; CA, United States ","ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3305365.3329729,a4,"ACM SIGGRAPH 2019 Appy Hour - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"PlayGAMI is an augmented reality origami creativity platform. It has the fun of designing, folding origami and the magic of AR all in a single experience! Our platform lets a user draw on real origami paper and turn their creation into a virtual origami action figure/game character! Further, we use GANs that interpret certain drawn symbols to interactive game elements. The final customized design can be posted to an online 3D Gallery for viewing and sharing on social media. © 2019 Copyright Held by the Owner/Author(s).",3,Interactive computer graphics,Augmented reality,Craft - Creativity - Games - GANs - Origami - Tangible interfaces,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Optimizing augmented reality outcomes in a gamified place experience application through design science research,"Vasilevski, Nikolche (1); Brand, Jeffrey (1); Birt, James (1) ","(1) Faculty of Society and Design, Bond University, Gold Coast; QLD, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365747,a52,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"Nearly ubiquitous smartphone use invites research and development of augmented reality experiences promoting knowledge and understanding. However, there is a lack of design science research dissemination about developing these solutions. This paper adds to the information systems body of knowledge by presenting the second iteration of Design Science Research Methodology artefact and the process of its development in the form of a gamified place experience application about indigenous art, focusing on the optimization of AR integration and user interface enhancements. In testing the usability, we illustrate how the application was optimized for successful outcomes. The qualitative analysis results revealed the high level of usability of the mobile application leading to further testing of efficacy in creating Sense of Place where the art is curated and displayed. © 2019 Association for Computing Machinery.",4,Design,Argon - Arts computing - Augmented reality - Interactive computer graphics - Iterative methods - User interfaces - Virtual reality,Design-science researches - Gamification - Interface enhancement - Qualitative analysis - Research and development - Sense of place - Service marketing - Tours,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 804 Chemical Products Generally - 921.6 Numerical Methods",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A web-based augmented reality plat-form using pictorial QR code for educational purposes and beyond,"Nguyen, Minh (1); Le, Huy (1); Lai, Phu Minh (1); Yan, Wei Qi (1) ","(1) Auckland University of Technology, New Zealand ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364793,3364793,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Augmented Reality (AR) provides the capability to overlay virtual 3D information onto a 2D printed flat surface; for example, displaying a 3D model on a single flat card that accompanies with the diagram shown in a learning text-book. The student can zoom in and out, rotate, and perceive the animation of the figure in real-time. This will make the educational theory more attractive; hence, motivates students to learn. AR is a great tool; however, the setup and display are not straight-forward (there are many different AR markers with different encryption, decryption methods, and displaying flat-forms). In this paper, we proposed a portable browser-based platform which uses the advantages of AR along with scan-able QR Code on mobile phones to enhance instant 3D visualisation. The user only needs a smart-phone (Apple iPhone or Android) with Internet-enabled; no specific Apps are needed to install. The user scans the QR Code embedded in a colour image, the code will link to a public website, and the website will produce AR Experience right on top of the browser. As a result, it provides a stress-free, low-cost, portable, and promising solution for not only educational purposes but also many other fields such as gaming, property selling, ecommerce, reporting. The set up is convenient: the user uploads a picture (e.g. a racing car), and what actions to be related to it (a 3D model to display, or a movie to play). The system will add on the picture one small colour QR code (to redirect to an online URL) and a thin black border. The user also uploads the 3D model (GLTF files) that he wants to display on top of the card to finish the set-up. At the display, the user can print the AR card, point their smart-phone towards the card, and pre-setup AR models or actions will appear on it. To students, these 3D graphics or animations will allow them to learn and understand the lessons in a much more intuitive way. © 2019 Copyright held by the owner/author(s).",5,Three dimensional displays,3D modeling - Augmented reality - Costs - Cryptography - E-learning - Education - Education computing - Smartphones - Students - Virtual reality - Websites,3D information - 3D Visualisation - Colour image - Educational theory - Flat surfaces - Learning text - QR codes - Web-based augmented realities,"718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications - 911 Cost and Value Engineering; Industrial Economics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Adjustable adaptation for spatial augmented reality workspaces,"Lages, Wallace S. (1); Bowman, Doug A. (1) ","(1) Center for Human-Computer Interaction, Blacksburg; VA, United States ",Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,10.1145/3357251.3358755,a20,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,Many cases in which augmented reality would be useful in everyday life requires the ability to access to information on the go. This means that interfaces should support user movement and also adjust to different physical environments. Prior research has showed that spatial adaptation can reduce the effort required to manage windows when walking and moving to different spaces. We designed and implemented an unified interaction system for AR windows that allow users to quickly switch and fine tune spatial adaptation. Our study indicates that a small number of adaptive behaviors is sufficient to facilitate information access in variety of conditions. © 2019 Association for Computing Machinery.,3,User interfaces,Augmented reality,Adaptive behavior - Adaptive interface - Information access - Interaction systems - Physical environments - Spatial adaptation - Spatial augmented realities - Wearable,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: ONR, Sponsor: Office of Naval Research; ",The authors gratefully acknowledge funding support from the Immersive Sciences program in the Office of Naval Research and from the Brazilian National Council for Scientific and Technological Development.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Optical-reflection type 3D augmented reality mirrors,"Lee, Gun A. (1); Park, Hye Sun (2); Billinghurst, Mark (1) ","(1) University of South Australia, Adelaide; SA, Australia (2) ETRI, Daejeon, Korea, Republic of ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364782,3364782,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Augmented Reality (AR) mirrors can show virtual objects overlaid onto the physical world reflected in the mirror. Optical-reflection type AR mirror displays use half-silvered mirrors attached in front of a digital display. However, prior work suffered from visual depth mismatch between the optical reflection of the 3D physical space and 2D images displayed on the surface of the mirror. In this research, we use 3D visualisation to overcome this problem and improve the user experience by providing better depth perception for watching and interacting with the content displayed on an AR mirror. As a proof of concept, we developed two prototype optical-reflection type 3D AR mirror displays, one using glassesfree multi-view 3D display and another using a head tracked 3D stereoscopic display that supports hand gesture interaction. © 2019 Copyright held by the owner/author(s).",6,Three dimensional displays,Augmented reality - Depth perception - Mirrors - Stereo image processing - Three dimensional computer graphics - Virtual reality - Visualization,3D Visualisation - 3D Visualization - Depth mismatch - Digital display - Half-silvered mirrors - Optical reflection - Proof of concept - Stereoscopic display,"723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 741.3 Optical Devices and Systems",,,,"This work was supported by Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2017-0-01849, Development of Core Technology for Real-Time Image Composition in Unstructured In-outdoor Environment).",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An adaptive interface for spatial augmented reality workspaces,"Lages, Wallace S. (1); Bowman, Doug A. (1) ","(1) Center for Human-Computer Interaction, Blacksburg; VA, United States ",Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,10.1145/3357251.3360005,a32,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,"A promising feature of wearable augmented reality devices is the ability to easily access information on the go. However, designing AR interfaces that can support user movement and also adjust to different physical environments is a challenging task. We present an interaction system for AR windows that uses adaptation to automatically perform level window movement while allowing high-level user control. © 2019 Association for Computing Machinery.",8,User interfaces,Augmented reality - Wearable computers,Adaptive interface - Interaction systems - On The Go - Physical environments - Spatial augmented realities - User control - User movement - Wearable,"722.2 Computer Peripheral Equipment - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: ONR, Sponsor: Office of Naval Research; ",The authors gratefully acknowledge funding support from the Immersive Sciences program in the Office of Naval Research and from the Brazilian National Council for Scientific and Technological Development.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Extending virtual reality displaywall environments using augmented reality,"Nishimoto, Arthur (1); Johnson, Andrew (1) ","(1) University of Illinois at Chicago, Chicago; IL, United States ",Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,10.1145/3357251.3357579,a7,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,"Two major form factors for virtual reality are head-mounted displays and large display environments such as CAVE and the LCDbased successor CAVE2. Each of these has distinct advantages and limitations based on how they're used. This work explores preserving the high resolution and sense of presence of CAVE2 environments in full stereoscopic mode by using a see-Though augmented reality HMD to expand the user's field of regard beyond the physical display walls. In our explorative study, we found that in a visual search task in a stereoscopic CAVE2, the addition of the HoloLens to expand the field of regard did not hinder the performance or accuracy of the participant, but promoted more physical navigation which in post-study interviews participants felt aided in their spatial awareness of the virtual environment. © 2019 Association for Computing Machinery.",17,Helmet mounted displays,Augmented reality - Stereo image processing - Virtual reality,Field of regard - Head mounted displays - High resolution - Immersive display - Physical navigation - Presence - Sense of presences - Spatial awareness,"723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,"Number: DE-SC005067, Acronym: USDOE, Sponsor: U.S. Department of Energy; Number: CNS-0959053, Acronym: NSF, Sponsor: National Science Foundation; ",We thank Dr. Debaleena Chattopadhyay and students in her Empirical Methods class for their feedback in the design of the user study. This work used the CAVE2 hybrid-reality environment which was supported by the National Science Foundation (award # CNS-0959053) and the Department of Energy (award # DE-SC005067).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Enhancement of pointing towards non-haptic augmented reality interfaces by increasing the arm position sense,"Markov-Vetter, Daniela (1); Staadt, Oliver (1) ","(1) University of Rostock, Germany ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364728,3364728,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Interactive user interfaces in head-mounted Augmented Reality environments are not always projected onto a physical surface. However, operating such free-floating interfaces by touch gestures is challenging because they do not provide haptic feedback. Considering a pointing gesture, in this work we present a user study evaluating the benefits of increasing the arm position sense for operating non-haptic interface. Our findings confirm that haptic feedback is required and show that an increased arm sense compensates for the lack of haptic feedback. The results suggest that applying 0.3 times of the pointing arm's weight significantly speeds up direct object selection for free-floating interfaces. We also show that the correction phase of the underlying pointing movement is affected by boosting the arm sense. © 2019 Association for Computing Machinery.",7,Haptic interfaces,Augmented reality - Virtual reality,Direct objects - Haptic feedbacks - Interaction - Interactive user interfaces - Pointing gestures - Pointing movement - Reality interface - Selection,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
HoloCity-exploring the use of augmented reality cityscapes for collaborative understanding of high-volume urban sensor data,"Lock, Oliver (1); Bedanrz, Tomasz (2); Pettit, Christopher (3) ","(1) UNSW Built Environment/Art and Design (EPICentre), Sydney; NSW, Australia (2) CSIRO Data61, UNSW Art and Design, EPICentre, Sydney; NSW, Australia (3) City Futures Research Centre, UNSW Built Environment, Sydney; NSW, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365734,a45,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"This research presents an application for visualizing the real-world cityscapes and massive transport network performance data sets in Augmented Reality (AR) using the Microsoft HoloLens, or any equivalent hardware. This runs in tandem with numerous emerging applications in the growing worldwide Smart Cities movement and industry. Specifically, this application seeks to address visualization of both real-time and aggregated city data feeds-such as weather, traffic and social media feeds. The software is developed in extensible ways, and it able to overlay various historic and live data sets coming from multiple sources. Advances in computer graphics, data processing and visualization now allow us to tie these visual tools in with much more detailed, longitudinal, massive performance data sets to support comprehensive and useful forms of visual analytics for city planners, decision makers and citizens. Further, it allows us to show these in new interfaces such as the HoloLens and other head-mounted displays to enable collaboration and more natural mappings with the real world. Using this toolkit, this visualization technology allows a novel approach to explore hundreds of millions of data points in order to find insights, trends, patterns over significant periods of time and geographic space. The focus of our development uses open data sets, which maximizes applications to assessing the performance of networks of cities worldwide. The city of Sydney, Australia is used as our initial application. It showcases a real-world example of this application enabling analysis of the transport network performance over the past twelve months. © 2019 Association for Computing Machinery.",7,Data visualization,Augmented reality - Decision making - Electronic data interchange - Flow visualization - Helmet mounted displays - Interactive computer graphics - Network performance - Open Data - Smart city - Transportation - Urban transportation - Virtual reality - Visualization,"Decision makers - Emerging applications - Head mounted displays - Performance data - Sydney , Australia - Urban data - Visual analytics - Visualization technologies","432 Highway Transportation - 433 Railroad Transportation - 631.1 Fluid Flow, General - 723 Computer Software, Data Handling and Applications - 912.2 Management",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
LightAnchors: Appropriating point lights for spatially-anchored augmented reality interfaces,"Ahuja, Karan (1); Pareddy, Sujeath (1); Xiao, Robert (1); Goel, Mayank (1); Harrison, Chris (1) ","(1) Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh; PA; 15213, United States ",UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 189-196,17-Oct-19,UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332165.3347884,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Augmented reality requires precise and instant overlay of digital information onto everyday objects. We present our work on LightAnchors, a new method for displaying spatially-anchored data. We take advantage of pervasive point lights - such as LEDs and light bulbs - for both in-view anchoring and data transmission. These lights are blinked at high speed to encode data. We built a proof-of-concept application that runs on iOS without any hardware or software modifications. We also ran a study to characterize the performance of LightAnchors and built eleven example demos to highlight the potential of our approach. © 2019 Association of Computing Machinery.",36,User interfaces,Application programs - Augmented reality - Incandescent lamps - Light - Smartphones - Visible light communication,Digital information - Light bulbs - Markers - Mobile interaction - Proof of concept - Reality interface - Software modification - Tags,"707.2 Electric Lamps - 717.1 Optical Communication Systems - 718.1 Telephone Systems and Equipment - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 741.1 Light/Optics",,,"Number: -, Acronym: DARPA, Sponsor: Defense Advanced Research Projects Agency; Number: SRC, Acronym: SRC, Sponsor: Semiconductor Research Corporation; ","This research was generously supported with funds from the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. We are also grateful to Anthony Rowe and his lab for early brainstorming on this effort.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Designar: Immersive 3D-modeling combining augmented reality with interactive displays,"Reipschläger, Patrick (1); Dachselt, Raimund (1) ","(1) Interactive Media Lab Dresden, Technische Universität Dresden, Germany ",ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 29-41,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,10.1145/3343055.3359718,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"We present DesignAR, an augmented design workstation for creating 3D models. Our approach seamlessly integrates an interactive surface displaying 2D views with head-mounted, stereoscopic Augmented Reality (AR). This creates a combined output space that expands the screen estate and enables placing 3D objects beyond display borders. For the effective combination of 2D and 3D views, we define different levels of proximity and alignment. Regarding input, multi-touch and pen mitigate issues of precision and ergonomics commonly found in mid-air VR/AR interaction. For creating and refining 3D models, we propose a set of pen and touch techniques with immediate AR feedback, including sketching of rotational solids or tracing physical objects on the surface. To further support a designer's modeling process, we additionally propose orthographic model views and UI offloading in AR as well as freely placeable model instances with real-world reference. Based on our DesignAR prototype, we report on challenges and insights regarding this novel type of display augmentation. The combination of high-resolution, high-precision interactive surfaces with carefully aligned AR views opens up exciting possibilities for future work and design environments, a vision we call Augmented Displays. © 2019 Copyright held by the owner/author(s).",58,Three dimensional displays,3D modeling - Augmented reality - Design - Ergonomics - Stereo image processing,Design environment - High resolution - Interactive display - Interactive surfaces - Modeling process - Pen and touches - Pen interactions - Physical objects,"723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,"Number: 389792660, Acronym: DFG, Sponsor: Deutsche Forschungsgemeinschaft; ",This work was partially funded by DFG grant 389792660 as part of TRR 248 (see https://perspicuous-computing.science). We would like to thank Severin Engert for his support in creating the figures and videos for this publication.,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"An end-to-end augmented reality solution to support aquaculture farmers with data collection, storage, and analysis","Xi, Mingze (1); Adcock, Matt (1); McCollouch, John (2) ","(1) CSIRO's, Data61, Canberra, Australia (2) CSIRO's, Data61, Hobart, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365721,a37,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"Augmented reality (AR) is being rapidly adopted by industries, such as logistics, manufacturing and military. However, one of the extremely under-explored yet significant areas is the primary production industry, and as a major source of food and nutrition, seafood production has always been a priority for many countries. Aquaculture farming is a highly dynamic, unpredictable and labour-intensive process. In this paper, we discuss the challenges in aquaculture farm operation based on our field studies with leading Australian fisheries. We also propose an 'AR + Cloud' system design to tackle the delayed in-situ water quality data collection and query, as well as aquaculture pond stress monitoring and analysis. © 2019 Association for Computing Machinery.",11,Data acquisition,Aquaculture - Augmented reality - Digital storage - Interactive computer graphics - Quality control - Search engines - Virtual reality - Visualization - Water quality,Aquaculture ponds - Data collection - Field studies - Immersive - Labour-intensive - Primary production - Stress monitoring - Water quality data,"445.2 Water Analysis - 722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications - 821.3 Agricultural Methods - 913.3 Quality Assurance and Control",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Portal-ble: Intuitive free-hand manipulation in unbounded smartphone-based augmented reality,"Qian, Jing (1); Ma, Jiaju (1); Li, Xiangyu (2); Attal, Benjamin (1); Lai, Haoming (1); Tompkin, James (1); Hughes, John F. (1); Huang, Jeff (1) ","(1) Brown University, Providence; RI, United States (2) Southeast University, Nanjing, China ",UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 133-145,17-Oct-19,UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332165.3347904,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"Smartphone augmented reality (AR) lets users interact with physical and virtual spaces simultaneously. With 3D hand tracking, smartphones become apparatus to grab and move virtual objects directly. Based on design considerations for interaction, mobility, and object appearance and physics, we implemented a prototype for portable 3D hand tracking using a smartphone, a Leap Motion controller, and a computation unit. Following an experience prototyping procedure, 12 researchers used the prototype to help explore usability issues and define the design space. We identified issues in perception (moving to the object, reaching for the object), manipulation (successfully grabbing and orienting the object), and behavioral understanding (knowing how to use the smartphone as a viewport). To overcome these issues, we designed object-based feedback and accommodation mechanisms and studied their perceptual and behavioral effects via two tasks: picking up distant objects, and assembling a virtual house from blocks. Our mechanisms enabled significantly faster and more successful user interaction than the initial prototype in picking up and manipulating stationary and moving objects, with a lower cognitive load and greater user preference. The resulting system - Portal-ble - improves user intuition and aids free-hand interactions in mobile situations. © 2019 Copyright is held by the owner/author(s).",52,User interfaces,Augmented reality - Palmprint recognition - Smartphones,3D hand tracking - Behavioral effects - Design considerations - Experience prototyping - Hand manipulation - Motion controller - Object appearance - User interaction,"718.1 Telephone Systems and Equipment - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: -, Acronym: NSF, Sponsor: National Science Foundation; ","This work is supported by the National Science Foundation under Grant No. IIS-1552663 and by a gift from Pixar. We thank Fumeng Yang, Sara Gramley, Meng Kun, Andries van Dam, Leslie Welch, Neille-Ann Tan, Valerie Nguon, Nediyana Daskalova, Shaun Wallace, and Arielle Chapin for helping with editing and intellectual support. We also thank student researchers in the CSCI 2300 course at Brown University for conducting the experience prototyping sessions.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Visualizing and interacting with hierarchical menus in immersive augmented reality,"Pourmemar, Majid (1); Poullis, Charalambos (1) ","(1) Immersive and Creative Technologies Lab, Department of Computer Science and Software Engineering, Concordia University, Montreal; QC, Canada ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365693,a30,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"Graphical User Interfaces (GUIs) have long been used as a way to inform the user of the large number of available actions and options. GUIs in desktop applications traditionally appear in the form of two-dimensional hierarchical menus due to the limited screen real estate, the spatial restrictions imposed by the hardware e.g. 2D, and the available input modalities e.g. mouse/keyboard point-and-click, touch, dwell-time etc. In immersive Augmented Reality (AR), there are no such restrictions and the available input modalities are different (i.e. hand gestures, head pointing or voice recognition), yet the majority of the applications in AR still use the same type of GUIs as with desktop applications. In this paper we focus on identifying the most efficient combination of (hierarchical menu type, input modality) to use in immersive applications using AR headsets.We report on the results of a withinsubjects study with 25 participants who performed a number of tasks using four combinations of the most popular hierarchical menu types with the most popular input modalities in AR, namely: (drop-down menu, hand gestures), (drop-down menu, voice), (radial menu, hand gestures), and (radial menu, head pointing). Results show that the majority of the participants (60%, 15) achieved a faster performance using the hierarchical radial menu with head pointing control. Furthermore, the participants clearly indicated the radial menu with head pointing control as the most preferred interaction technique due to the limited physical demand as opposed to the current de facto interaction technique in AR i.e. hand gestures, which after prolonged use becomes physically demanding leading to arm fatigue known as 'Gorilla arms'. © 2019 Association for Computing Machinery.",25,Graphical user interfaces,Augmented reality - Drops - Interactive computer graphics - Mammals - Palmprint recognition - Speech recognition - Touch screens - Virtual reality,Desktop applications - Gestural input - Graphical user interface (GUIs) - Immersive application - Immersive augmented realities - Interaction paradigm - Interaction techniques - Spatial restrictions,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 751.5 Speech",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Effects of shared gaze parameters on visual target identification task performance in augmented reality,"Norouzi, Nahal (1); Erickson, Austin (1); Kim, Kangsoo (1); Schubert, Ryan (1); LaViola, Joseph J. (1); Bruder, Gerd (1); Welch, Gregory F. (1) ","(1) University of Central Florida, United States ",Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,19-Oct-19,Proceedings - SUI 2019: ACM Conference on Spatial User Interaction,2019,,,,9.78145E+12,10.1145/3357251.3357587,a12,"7th ACM Symposium on Spatial User Interaction, SUI 2019","October 19, 2019 - October 20, 2019",,"Augmented reality (AR) technologies provide a shared platform for users to collaborate in a physical context involving both real and virtual content. To enhance the quality of interaction between AR users, researchers have proposed augmenting users' interpersonal space with embodied cues such as their gaze direction. While beneficial in achieving improved interpersonal spatial communication, such shared gaze environments suffer from multiple types of errors related to eye tracking and networking, that can reduce objective performance and subjective experience. In this paper, we conducted a human-subject study to understand the impact of accuracy, precision, latency, and dropout based errors on users' performance when using shared gaze cues to identify a target among a crowd of people. We simulated varying amounts of errors and the target distances and measured participants' objective performance through their response time and error rate, and their subjective experience and cognitive load through questionnaires. We found some significant differences suggesting that the simulated error levels had stronger effects on participants' performance than target distance with accuracy and latency having a high impact on participants' error rate.We also observed that participants assessed their own performance as lower than it objectively was, and we discuss implications for practical shared gaze applications. © 2019 Association for Computing Machinery.",33,Eye tracking,Augmented reality - Errors - Surveys,Cognitive loads - Human subjects - Quality of interaction - Shared gazes - Simulated error - Subjective experiences - Target identification - Task performance,"723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: UCF, Sponsor: University of Central Florida; Number: -, Acronym: UF, Sponsor: University of Florida; Number: 1564065, Acronym: NSF, Sponsor: National Science Foundation; Number: 1800947, Acronym: IIS, Sponsor: Division of Information and Intelligent Systems; Number: Welch, Acronym: ONR, Sponsor: Office of Naval Research; Number: -, Acronym: SU, Sponsor: Stanford University; ","This material includes work supported in part by the National Science Foundation under Award Number 1564065 (Dr. Ephraim P. Glinert, IIS) and Collaborative Award Numbers 1800961, 1800947, and 1800922 (Dr. Tonya Smith-Jackson, IIS) to the University of Central Florida, University of Florida, and Stanford University respectively; the Office of Naval Research under Award Number N00014-17-1-2927 (Dr. Peter Squire, Code 34); and the AdventHealth Endowed Chair in Healthcare Simulation (Prof. Welch). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the supporting institutions.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Supporting older adults in using complex user interfaces with augmented reality,"Kong, Junhan (1); Guo, Anhong (1); Bigham, Jeffrey P. (1) ","(1) Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh; PA, United States ",ASSETS 2019 - 21st International ACM SIGACCESS Conference on Computers and Accessibility,ACM SIGACCESS,"Association for Computing Machinery, Inc",,p 661-663,24-Oct-19,ASSETS 2019 - 21st International ACM SIGACCESS Conference on Computers and Accessibility,2019,,,,9.78145E+12,10.1145/3308561.3354593,,"21st International ACM SIGACCESS Conference on Computers and Accessibility, ASSETS 2019","October 28, 2019 - October 30, 2019",,"Using complex interfaces has been shown to be challenging for older adults. Existing tutorial systems can be cumbersome, and sometimes difficult to use. To solve this problem, we present a system to support older adults in using visual interfaces by providing step-by-step visual guidance with augmented reality. Using the Apple ARKit platform, our system detects the interface in a phone camera view, and provides visual guidance for users to access the interface following a generated sequence of interactions based on pre-specified tasks and prior knowledge of the interface. © 2019 Copyright is held by the owner/author(s).",9,User interfaces,Augmented reality - Interfaces (materials),Camera view - Cognitive support - Complex interface - Older adults - Prior knowledge - Tutorial system - Visual guidance - Visual Interface,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 951 Materials Science",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The Q*bird level designer: User-assisted procedural level design in augmented reality,"Cheng, Yi Fei (1); Normoyle, Aline (1) ","(1) Swarthmore College, United States ","Proceedings - MIG 2019: ACM Conference on Motion, Interaction, and Games",ACM SIGGRAPHGeneral,"Association for Computing Machinery, Inc",,,28-Oct-19,"Proceedings - MIG 2019: ACM Conference on Motion, Interaction, and Games",2019,,,,9.78145E+12,10.1145/3359566.3364686,a46,"2019 ACM Conference on Motion, Interaction, and Games, MIG 2019","October 28, 2019 - October 30, 2019",,"Augmented reality (AR) gaming is becoming widely available thanks to improvements in hand-held devices such as phones and tablets. In this work, we describe our system for generating levels for the AR game, Q*bird. In Q*bird, the player must visit every cell in the level while avoiding bees and cannon balls, similarly to the 1982 arcade game, Q*bert. To create a new level, designers place game elements using virtual cards. The system then generates the remainder of the level, ensuring that it’s navigable. Designers can edit these levels by dragging and dropping the created geometry. To test, the designer can drop a character into the level and play it. This system aids playtesting and level design by allowing levels to be quickly specified and tested in the same environment in which the game is played. Furthermore, this system offers an example of how the design of AR levels can also be performed in AR. © 2019 Copyright held by the owner/author(s).",4,Birds,Augmented reality,Design tool - Game elements - Hand held device - Level design - Procedural content generations,"723 Computer Software, Data Handling and Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Poster– ProMar: Practical reference object-based multi-user augmented reality,"Li, Tengpeng (1); Nguyen, Nam Son (1); Zhang, Xiaoqian (1); Wang, Teng (1); Sheng, Bo (1) ","(1) Department of Computer Science, University of Massachusetts, Boston, United States ","MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 531-532,12-Jun-19,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",2019,,,,9.78145E+12,10.1145/3307334.3328610,,"17th ACM International Conference on Mobile Systems, Applications, and Services, MobiSys 2019","June 17, 2019 - June 21, 2019",,"Mobile Augmented Reality (MAR) represents an emerging category of applications that bring users interactive experiences with the physical experiencesnvironment. In such applications, users can place virtual objects in real-world space, and view them through a camera view. In this work, we develop a framework that supports multi-users interactions for MAR apps, where one user places a virtual object that can be recognized by other users. © 2019 Copyright held by the owner/author(s).",4,Augmented reality,,Camera view - Mobile augmented reality - Multi-user - Real-world - Reference objects - Virtual objects,"723 Computer Software, Data Handling and Applications",,,,This work is supported by National Science Foundation grant CNS-1527336 and UMass Boston Healey Grant.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Supporting abstraction skills using augmented reality?,"Reuter, Rebecca (1); Knietzsch, Marco (1); Hauser, Florian (1); Mottok, Jürgen (1) ","(1) OTH Regensburg, Regensburg, Germany ","Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE",ACM SIGCSE,Association for Computing Machinery,,p 320,2-Jul-19,ITiCSE 2019 - Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education,2019,,1942647X,,9.78145E+12,10.1145/3304221.3325562,,"2019 ACM Conference on Innovation and Technology in Computer Science Education, ITiCSE 2019","July 15, 2019 - July 17, 2019",,We investigated the potential of augmented reality (AR) to enable visualization of abstract concepts and present the first iteration of a teaching experiment that evaluates the use of AR as support for abstraction skills. Students were confronted with the task to present and explain information to different groups of stakeholders at the example of a coffee machine. Results show that students find it helpful to have a visual app-prototype and especially one that can be disassembled in different levels. The main goal was to sensitize students for the need to think about and to abstract information for certain roles and perspectives. © 2019 Copyright held by the owner/author(s).,3,Students,Abstracting - Augmented reality - Education computing - Engineering education - Engineering research - Software engineering,Abstract concept - Abstraction skills,"723 Computer Software, Data Handling and Applications - 723.1 Computer Programming - 901.2 Education - 901.3 Engineering Research - 903.1 Information Sources and Analysis",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Can mobile augmented reality stimulate a honeypot efect? Observations from Santa’s Lil helper,"Kelly, Ryan M. (1); Ferdous, Hasan Shahid (1); Wouters, Niels (1); Vetere, Frank (1) ","(1) University of Melbourne, Melbourne, Australia ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300515,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"In HCI, the honeypot efect describes a form of audience engagement in which a person’s interaction with a technology stimulates passers-by to observe, approach and engage in an interaction themselves. In this paper we explore the potential for honeypot efects to arise in the use of mobile augmented reality (AR) applications in urban spaces. We present an observational study of Santa’s Lil Helper, a mobile AR game that created a Christmas-themed treasure hunt in a metropolitan area. Our study supports a consideration of three factors that may impede the honeypot efect: the presence of people in relation to the game and its interactive components; the visibility of gameplay in urban space; and the extent to which the game permits a shared experience. We consider how these factors can inform the design of future AR experiences that are capable of stimulating honeypot efects in public space. © 2019 Copyright held by the owner/author(s).",61,Augmented reality,Human engineering - Urban planning,Audience - Honeypots - Metropolitan area - Mobile augmented reality - Observational study - Public space - Shared experiences - Urban spaces,"403.1 Urban Planning and Development - 461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
SIG: Spatiality of augmented reality user interfaces,"Vovk, Alla (1); Wild, Fridolin (1); Rodrigues, Danilo Gasques (2); Weibel, Nadir (2) ","(1) Oxford Brookes University, Oxford, United Kingdom (2) UC San Diego, San Diego, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3311756,3311756,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Augmented reality and spatial information manipulation is being increasingly used as part of environment integrated form factors and wearable devices such as head-mounted displays. The integration of this exciting technology in many aspects of peoples' lives is transforming the way we understand computing, pushing the boundaries of Spatial Interfaces into virtual but embedded environments. We think that the HCI community is ready for a renewed discussion about the role of Augmented Reality within Spatial Interfaces. With this SIG we want to expand the discussion related to Spatial Interfaces and the way they impact interaction with the world in two areas. First, we aim to critically discuss the definition of Spatial Interfaces and outline the common components that build such interfaces in today's world. Second, we would like the community to reflect on the path ahead and focus on the potential of what kind of experiences can Spatial Interfaces achieve today. © 2019 Copyright held by the owner/author(s).",5,User interfaces,Augmented reality - Helmet mounted displays - Human engineering,Community IS - Embedded environment - Head mounted displays - Impact interaction - Interaction - Spatial computing - Spatial informations - Wearable devices,"461.4 Ergonomics and Human Factors Engineering - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Jumpar - Augmented reality platformer,"Klimm, Maren (1); Walczak, Dominik (1); Ayen, Daniel (1) ","(1) Reutlingen University, Reutlingen, Germany ",CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 261-266,17-Oct-19,CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play,2019,,,,9.78145E+12,10.1145/3341215.3358249,,"6th ACM SIGCHI Annual Symposium on Computer-Human Interaction in Play, CHI PLAY 2019","October 22, 2019 - October 25, 2019",,"The puzzle and dexterity mobile game JumpAR combines well-known Jump ‘n’ Run elements with the opportunities of Augmented Reality (AR). The goal of the game is to complete a parkour of platforms with a pcharacter and to collect rewards on the way. But with one trick – the construction of the level is done by the player themselves. The bases of the different platforms and obstacles are placed in the real-world surroundings of the player. These then get augmented on the mobile device and form a parkour through which the player must maneuver the character to successfully reach the exit platform. This highly interactive game leads to individual creative game areas wherever the player starts to build the parkour: on the floor, on a desk or on even on the lawn. It can be played either alone or in groups and allows both, a collaborative and competitive gameplay by creating challenging parkours for oneself or others. © 2019 Copyright is held by the owner/author(s).",10,Human computer interaction,Augmented reality - Interactive computer systems,Creative games - Gameplay - Interactive games - Mobile games - Platformer - Real-world,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Robot Teleoperation with Augmented Reality Virtual Surrogates,"Walker, Michael E. (1); Hedayati, Hooman (1); Szafir, Daniel (2) ","(1) Dept. of Computer Science, University of Colorado Boulder, Boulder; CO, United States (2) Dept. of Computer Science, ATLAS Institute, University of Colorado Boulder, Boulder; CO, United States ",ACM/IEEE International Conference on Human-Robot Interaction,ACM; ACM SIGAI; ACM SIGCHI; IEEE; IEEE Robotics and Automation Society,IEEE Computer Society,v 2019-March,p 202-210,22-Mar-19,HRI 2019 - 14th ACM/IEEE International Conference on Human-Robot Interaction,2019,,,21672148,9.78154E+12,10.1109/HRI.2019.8673306,8673306,"14th Annual ACM/IEEE International Conference on Human-Robot Interaction, HRI 2019","March 11, 2019 - March 14, 2019",,"Teleoperation remains a dominant control paradigm for human interaction with robotic systems. However, teleoperation can be quite challenging, especially for novice users. Even experienced users may face difficulties or inefficiencies when operating a robot with unfamiliar and/or complex dynamics, such as industrial manipulators or aerial robots, as teleoperation forces users to focus on low-level aspects of robot control, rather than higher level goals regarding task completion, data analysis, and problem solving. We explore how advances in augmented reality (AR) may enable the design of novel teleoperation interfaces that increase operation effectiveness, support the user in conducting concurrent work, and decrease stress. Our key insight is that AR may be used in conjunction with prior work on predictive graphical interfaces such that a teleoperator controls a virtual robot surrogate, rather than directly operating the robot itself, providing the user with foresight regarding where the physical robot will end up and how it will get there. We present the design of two AR interfaces using such a surrogate: one focused on real-time control and one inspired by waypoint delegation. We compare these designs against a baseline teleoperation system in a laboratory experiment in which novice and expert users piloted an aerial robot to inspect an environment and analyze data. Our results revealed that the augmented reality prototypes provided several objective and subjective improvements, demonstrating the promise of leveraging AR to improve human-robot interactions. © 2019 IEEE.",41,Human robot interaction,Antennas - Augmented reality - Drones - Graphical user interfaces - Industrial manipulators - Machine design - Man machine systems - Manipulators - Mixed reality - Real time control - Remote control - Robots,Aerial robots - ARHMD - Graphical interface - Interface designs - Laboratory experiments - Robot teleoperation - Teleoperation interface - Teleoperation systems,"601 Mechanical Design - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731 Automatic Control Principles and Applications",,,"Number: NNX16AR58G, Acronym: -, Sponsor: -; Number: 1764092, Acronym: NSF, Sponsor: National Science Foundation; ",ACKNOWLEDGMENTS This work was supported by an Early Career Faculty grant from NASA&rsquo;s Space Technology Research Grants Program under award NNX16AR58G and the NSF under award 1764092.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Hypercubes: A playful introduction to computational thinking in augmented reality,"Fuste, Anna (1); Schmandt, Chris (1) ","(1) MIT Media lab, 75 Amherst St., Cambridge; MA; 02142, United States ",CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 379-387,17-Oct-19,CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play,2019,,,,9.78145E+12,10.1145/3341215.3356264,,"6th ACM SIGCHI Annual Symposium on Computer-Human Interaction in Play, CHI PLAY 2019","October 22, 2019 - October 25, 2019",,"We present HyperCubes, an Augmented Reality (AR) platform to foster computational literacy. Using paper cubes as AR markers and spatial tracking, the user becomes familiar with sequences of instructions as coding blocks. We leverage spatial cognition as a means to improve understanding of procedural and sequential models. We have performed two pilot studies for an iterative and user centered design of the platform. With a final qualitative user study we address engagement levels and the educational potential of the platform. We argue that by using spatial cognition and the flexibility of the AR medium, a playful introduction to basic computational thinking concepts can be presented in late elementary school and middle school children. Copyright is held by the author/owner(s).",37,Human computer interaction,Augmented reality - Geometry - Interactive computer systems - Mathematical programming,Computational thinkings - Educational potential - Elementary schools - Engagement levels - K-12 education - Paper crafts - Spatial cognition - Spatial tracking,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 921 Mathematics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
DMove: Directional Motion-based Interaction for Augmented Reality Head-Mounted Displays,"Xu, Wenge (1); Liang, Hai-Ning (1); Zhao, Yuxuan (1); Yu, Difeng (1); Monteiro, Diego (1) ","(1) Xi’an Jiaotong-Liverpool University, Suzhou, Jiangsu, China ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300674,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"We present DMove, directional motion-based interaction for Augmented Reality (AR) Head-Mounted Displays (HMDs) that is both hands- and device-free. It uses directional walking as a way to interact with virtual objects. To use DMove, a user needs to perform directional motions such as moving one foot forward or backward. In this research, we first investigate the recognition accuracy of the motion directions of our method and the social acceptance of this type of interactions together with users’ comfort rating for each direction. We then optimize its design and conduct a second study to compare DMove in task performance and user preferences (workload, motion sickness, user experience), with two approaches—Hand interaction (Meta 2-like) and Head+Hand interaction (HoloLens-like) for menu selection tasks. Based on the results of these two studies, we provide a set of guidelines for DMove and further demonstrate two applications that utilize directional motions. © 2019 Association for Computing Machinery.",60,Helmet mounted displays,Augmented reality - Human engineering - Street traffic control - User interfaces,Comfort ratings - Directional motion - Head mounted displays - Menu selection - Motion direction - Recognition accuracy - Social acceptance - Task performance,"406.2 Roads and Streets - 461.4 Ergonomics and Human Factors Engineering - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: #KSF-A-03, Acronym: XJTU, Sponsor: Xi&rsquo;an Jiaotong University; ",The authors would like to thank the anonymous reviewers for their valuable comments and helpful suggestions. The work is supported in part by Xi&rsquo;an Jiaotong-Liverpool University (XJTLU) Key Special Fund (#KSF-A-03) and XJTLU Research Development Fund.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Brick: Toward a model for designing synchronous colocated augmented reality games,"Bhattacharyya, Po (1); Nath, Radha (1); Jo, Yein (1); Jadhav, Ketki (1); Hammer, Jessica (1) ","(1) Carnegie Mellon University, Pittsburgh; PA, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300553,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"Augmented reality (AR) games have been growing in popularity in recent years. However, current AR games ofer limited opportunities for a synchronous multiplayer experience. This paper introduces a model for designing AR experiences in which players inhabit a shared, real-time augmented environment and can engage in synchronous and collaborative interactions with other players. We explored the development of this model through the creation of Brick, a two-player mobile AR game at the room scale. We refned Brick over multiple rounds of iteration, and we used our playtests to investigate a range of issues involved in designing shared-world AR games. Our fndings suggest that there are fve major categories of interactions in a shared-world AR system: single-player, intrapersonal, multiplayer, interpersonal, and environmental. We believe that this model can support the development of collaborative AR games and new forms of social gameplay. © 2019 Copyright held by the owner/author(s).",31,Brick,Augmented reality - Human engineering,Augmented environments - Co-located - Collaborative - Collaborative interaction - Mobile Ar - Multiplayers - New forms - Synchronous,"414.2 Brick Materials - 461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Auditory augmented reality for cyber physical production systems,"Iber, Michael (1); Lechner, Patrik (1); Jandl, Christian (1); Mader, Manuel (1); Reichmann, Michael (1) ","(1) Media and Digital Technologies, St. Pölten University of Applied Sciences, St. Pölten, Austria ",ACM International Conference Proceeding Series,beLa; et al.; Holon; Routledge; Volvo; xln audio,Association for Computing Machinery,,p 53-60,18-Sep-19,"Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound, AM 2019",2019,,,,9.78145E+12,10.1145/3356590.3356600,,"14th International Audio Mostly Conference: A Journey in Sound, AM 2019","September 18, 2019 - September 20, 2019",,"We describe a proof-of-concept approach on the sonification of estimated operation states of 3D printing processes. The results of this study form the basis for the development of an 'intelligent' noise protection headphone as part of Cyber Physical Production Systems, which provides auditorily augmented information to machine operators and enables radio communication between them. Further application areas are implementations in control rooms (equipped with multichannel loudspeaker systems) and utilization for training purposes. The focus of our research lies on situation-specific acoustic processing of conditioned machine sounds and operation related data with high information content, considering the often highly auditorily influenced working knowledge of skilled workers. As a proof-of-concept the data stream of error probability estimations regarding partly manipulated 3D printing processes was mapped to three sonification models, giving evidence about momentary operation states. The neural network applied indicates a high accuracy (>93%) concerning error estimation distinguishing between normal and manipulated operation states. None of the manipulated states could be identified by listening. An auditory augmentation, respectively sonification of these error estimations provides a considerable benefit to process monitoring. © 2019 ACM.",42,Cyber Physical System,3D printers - Audio acoustics - Augmented reality - Errors - Printing presses - Process control - Process monitoring - Radio communication,Acoustic processing - Auditory augmentation - Auditory display - Error prediction - Error probabilities - Information contents - Multichannel loudspeaker systems - Production system,"716.3 Radio Systems and Equipment - 723 Computer Software, Data Handling and Applications - 745.1.1 Printing Equipment - 751.1 Acoustic Waves - 913.1 Production Engineering",,,"Number: 866856, Acronym: -, Sponsor: -; Number: -, Acronym: MOEA, Sponsor: Ministry of Economic Affairs; ",Our research is funded by the Austrian Ministry of Digital and Economic Affairs within the FFG COIN project Immersive Media Lab (866856). We further wish to than? our colleagues Matthias Zeppelzauer and Djordje Slijep&#269;evi&#263; for their feedbac? on deep learning methodologies as well as Franzis?a Bruc?ner and Georg Vogt for their support within the project.,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An augmented reality based strategy for base station maintenance,"Lai, ChinLun (1) ","(1) Communication Engineering Department, Oriental Institute of Technology, No. 58, Sec. 2, Sichuan Rd., Banqiao Dist., New Taipei City, Taiwan ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 57-61,25-Oct-19,ICDTE 2019 - 2019 the 3rd International Conference on Digital Technology in Education,2019,,,,9.78145E+12,10.1145/3369199.3369230,,"3rd International Conference on Digital Technology in Education, ICDTE 2019","October 25, 2019 - October 27, 2019",,"In this paper, a skill training strategy for base station maintenance is proposed thus the engineers of telecomm operators can be well self-trained and solve the problems at the first line without the limitations of time, space, manpower, and other expensive equipment. From this way, the basic knowledge of maintenance skill for mobile base station can be accessed immediately in 24x7 hours thus keep the good service quality of mobile communication. This learning strategy fulfills the concept of learning by practice while reducing the related cost and effort significantly. It can be expected that the proposed system will give great benefit and help the first-line engineers to make up the shortage of manpower while provides high availability and reliability of the base stations via the good effect of self problem-solving. Furthermore, it is also easily to be applied to other skill training fields thus is practical for the future education and training purpose. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",13,E-learning,Augmented reality - Base stations - Educational technology - Maintenance - Mobile telecommunication systems - Personnel training,Education and training - Expensive equipments - High availability - Learning strategy - Mobile base station - Mobile communications - Self-learning - Skills training,"716.3 Radio Systems and Equipment - 723 Computer Software, Data Handling and Applications - 901.2 Education - 912.4 Personnel - 913.5 Maintenance",,,,Thanks for the Far EasTone Telecommunications members in this project who allowing and helping us to build and test the prototype system. Also thanks to all the related OITCE students who practiced in the Far EasTone Telecommunications during the project duration.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented Reality for Quick and Intuitive Robotic Packing Re-Programming,"Araiza-Illan, Dejanira (1); De San Bernabe, Alberto (1); Hongchao, Fang (1); Shin, Leong Yong (1) ","(1) Agency for Science, Technology and Research (STAR), Advanced Remanufacturing and Technology Centre (ARTC), Singapore, Singapore ",ACM/IEEE International Conference on Human-Robot Interaction,ACM; ACM SIGAI; ACM SIGCHI; IEEE; IEEE Robotics and Automation Society,IEEE Computer Society,v 2019-March,p 664,22-Mar-19,HRI 2019 - 14th ACM/IEEE International Conference on Human-Robot Interaction,2019,,,21672148,9.78154E+12,10.1109/HRI.2019.8673327,8673327,"14th Annual ACM/IEEE International Conference on Human-Robot Interaction, HRI 2019","March 11, 2019 - March 14, 2019",,"Current manufacturing applications are subject to constant changes in production orders for their robotic systems to adapt to the dynamic nature of the market. Hence, reprogramming robots needs to be a fast, easy and effective process. In this demonstration, we present an augmented reality (AR) interface using HoloLens. Our interface provides an intuitive platform to re-program a robotic packing application through simple hand gestures and the information gathered by the HoloLens' spatial mapping functionality. © 2019 IEEE.",3,Human robot interaction,Application programs - Augmented reality - Man machine systems - Manufacture - Robot programming - Robotics,Dynamic nature - Hand gesture - Intuitive interfaces - Manufacturing applications - Production order - Re-programming - Robotic systems - Spatial mapping,"537.1 Heat Treatment Processes - 723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
V.Ra: An in-situ visual authoring system for robot-Iot task planning with augmented reality,"Cao, Yuanzhi (1); Li, Fan (2); Huo, Ke (1); Xu, Zhuangying (1); Zhong, Wentao (1); Ramani, Karthik (1) ","(1) Purdue University, West Lafayette; IN, United States (2) Tsinghua University, Beijing, China ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312797,3312797,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"We present V.Ra, a visual and spatial programming system for robot-IoT task authoring. In V.Ra, programmable mobile robots serve as binding agents to link the stationary IoTs and perform collaborative tasks. We establish an ecosystem that coherently connects the three key elements of robot task planning (human-robot-IoT) with one single AR-SLAM device. Users can perform task authoring in an analogous manner with the Augmented Reality (AR) interface. Then placing the device onto the mobile robot directly transfers the task plan in a what-you-do-is-what-robot-does (WYDWRD) manner. The mobile device mediates the interactions between the user, robot and IoT oriented tasks, and guides the path planning execution with the SLAM capability. © 2019 Copyright held by the owner/author(s).",8,Robot programming,Augmented reality - Human engineering - Internet of things - Mobile agents - Mobile robots - Motion planning - Robotics,Authoring systems - Binding agent - Collaborative tasks - Human robots - Robotic tasks - SLAM - Spatial programming - Task planning,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 731.5 Robotics",,,"Number: FW-HTF 1839971, Acronym: NSF, Sponsor: National Science Foundation; Number: IIP (PFI:BIC) 1632154, Acronym: NSF, Sponsor: National Science Foundation; Number: IIS (NRI) 1637961, Acronym: NSF, Sponsor: National Science Foundation; ","This work was partially supported by the NSF under grants FW-HTF 1839971, IIS (NRI) 1637961 and IIP (PFI:BIC) 1632154. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agency.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Tourmar: Designing Tourism Mobile Augmented Reality Architecture with Data Integration to improve User Experience,"Ocampo, Ardee Joy T. (1) ","(1) Lorma Colleges Carlatan, San Fernando City La Union; 2500, Philippines ",ACM International Conference Proceeding Series,Shenzhen University; Sun Yat-Sen University,Association for Computing Machinery,,p 79-83,10-May-19,ICMSSP 2019 - 2019 4th International Conference on Multimedia Systems and Signal Processing,2019,,,,9.78145E+12,10.1145/3330393.3330428,,"4th International Conference on Multimedia Systems and Signal Processing, ICMSSP 2019","May 10, 2019 - May 12, 2019",,"The importance of tourism industry as a major contributor to the economic state of countries all around the world has significantly increased. Tourism is not only a major source of employment, but also a catalyst for urban and rural development. The design architecture of the study would integrate tourism data based on the needs of tourist or travelers to mobile augment reality application. The design model of this application is important to make the app more interactive and the user experience is also considered. The use of this technology allows travelers or tourist to retrieve interactive multimodal data about the different tourist destinations. Two major components of the TourMAR application were determined which are the Content Management and Design principle of the MAR. These components are essential to study in this research in order to produce an effective MAR application. The content management enables to address the common problem on tourism data thru the implementation of data integration. The design principles enhanced the development considerations which is anchored on the user perspective that would improve user experience. © 2019 Association for Computing Machinery.",31,Data integration,Augmented reality - Information management - Knowledge engineering - Multimedia signal processing - Multimedia systems - Regional planning,Content management - Design architecture - Design Principles - Mobile augmented reality - Tourism - Tourism industry - Tourist destinations - User perspectives,"403.2 Regional Planning and Development - 723 Computer Software, Data Handling and Applications",,,,The author would like to acknowledge the Communication and Tourism Office of the Province of La Union as the target respondent of the implementation of the study. Also to the author's family and colleagues in the academe.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Adopting conversational interfaces for exploring OSGi-based software architectures in augmented reality,"Seipel, P. (1); Stock, A. (1); Santhanam, S. (1); Baranowski, A. (1); Hochgeschwender, N. (1); Schreiber, A. (1) ","(1) German Aerosp. Center, Simulation & Software Technol., Cologne, Germany ",2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering (BotSE). Proceedings,,"IEEE, Piscataway, NJ, USA",,20-1,2019,,,,,,978-1-7281-2262-5,10.1109/BotSE.2019.00013,,2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering (BotSE),28-May-19,"Montreal, QC, Canada","We propose conversational interfaces as a convenient and complementary way for users to explore OSGi-based software architectures in immersive Augmented Reality (AR). By providing a conversational interface we aim to remedy some peculiarities of AR devices, but also enhancing the exploration task at hand. We exemplify a use case and sketch how different user utterances can be used to retrieve information about the to-be-explored OSGi-based software architecture. We identify crucial components such as natural language generation and intent recognition which are required to implement the user story and we outline the status of our implementation.",5,,augmented reality - software architecture - user interfaces,conversational interface - OSGi-based software architecture - augmented reality - AR devices,C6130V Virtual reality - C6180 User interfaces - C6110B Software engineering techniques,G06F9/44,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Blocks: Collaborative and Persistent Augmented Reality Experiences,"Anhong Guo (1); Canberk, I. (2); Murphy, H. (3); Monroy-Hernandez, A. (4); Vaish, R. (5) ","(1) Human-Comput. Interaction Inst., Carnegie Mellon Univ., Pittsburgh, PA, United States (2) Snap Inc., Venice, FL, United States (3) Wellesley Coll., Wellesley, MA, United States (4) Snap Inc., Seattle, WA, United States (5) Snap Inc., Santa Monica, CA, United States ","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",,"ACM, USA","v 3, n 3",83 (24 pp.),Sept. 2019,,,,2474-9567,,,10.1145/3351241,,,,,"We introduce Blocks, a mobile application that enables people to co-create AR structures that persist in the physical environment. Using Blocks, end users can collaborate synchronously or asynchronously, whether they are colocated or remote. Additionally, the AR structures can be tied to a physical location or can be accessed from anywhere. We evaluated how people used Blocks through a series of lab and field deployment studies with over 160 participants, and explored the interplay between two collaborative dimensions: space and time. We found that participants preferred creating structures synchronously with colocated collaborators. Additionally, they were most active when they created structures that were not restricted by time or place. Unlike most of today's AR experiences, which focus on content consumption, this work outlines new design opportunities for persistent and collaborative AR experiences that empower anyone to collaborate and create AR content.",,,augmented reality - groupware - mobile computing - user interfaces,AR structures - persistent augmented reality experiences - mobile application - collaborative AR experiences - AR content,"C6130V Virtual reality - C6180 User interfaces - C6190V Mobile, ubiquitous and pervasive computing - C6130G Groupware",G06F9/44,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Development of mobile augmented-reality and virtual-reality simulated training systems for marine ecology education,"Chang, Yu-Lien (1); Tien, Chia-Ling (2) ","(1) Department of Travel Management, National Kaohsiung University of Hospitality and Tourism, Kaohsiung, Taiwan (2) Department of Tourism and Leisure, Lunghwa University of Science and Technology, Taiwan, Taoyuan City, Taiwan ",Proceedings - Web3D 2019: 24th International ACM Conference on 3D Web Technology,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,26-Jul-19,Proceedings - Web3D 2019: 24th International ACM Conference on 3D Web Technology,2019,,,,9.78145E+12,10.1145/3329714.3338142,,"24th International ACM Conference on 3D Web Technology, Web3D 2019","July 26, 2019 - July 28, 2019",,"The paper is a case study for the effectiveness of hand-held and head-mounted AR interaction for increasing teaching and learning, in the specific field of marine ecology education. This system will serve as an assisting tool for the teaching activity of marine ecology guide to improve the learning effectiveness of the learners. A total of 191 questionnaires were collected, there were 161 valid questionnaires, among which including 70 AR questionnaires, 91 AR2VR questionnaires. There were 30 incomplete invalid questionnaires. This study adopts the pre-test and post-test variance design of the quasi-experimental research method to explore the learners' flow experience effect, technology acceptance model, activity involvement and learning effect under different guidance modes, as well as their attitudes towards the use and acceptance of the guidance system. The experiments are performed and analyzed in detail. The study results indicate that (a) the learning achievements of the students differed between before and after the AR and VR mobile guidance activities, (b) both the AR and VR guidance modes enhanced the flow experience of visitors, (c) all aspects of the technology acceptance models in both groups had positive effects on the flow experience, (d) all aspects of the activity involvement of both groups had positive effects on flow experience, and (e) most learners exhibited positive attitudes toward and acceptance of using AR and VR mobile guidance systems. © 2019 Association for Computing Machinery.",7,Learning systems,Acceptance tests - Augmented reality - E-learning - Ecology - Education computing - Marine education - Remote control - Surveys - Virtual reality - Web services,Experimental research - Flow experience - Learning achievement - Learning effectiveness - Mobile augmented reality - Simulated trainings - Teaching and learning - Technology acceptance model,"454.3 Ecology and Ecosystems - 723 Computer Software, Data Handling and Applications - 731.1 Control Systems - 913 Production Planning and Control; Manufacturing",,,,"The research was supported by grants from the Taiwan Ministry ofScience and Technology Project MOST-107-2813-C-262-001-H, and by the Higher Education Sprout Project, Ministry of Education, Taiwan. We also thank Mr. Xiang Ming Tsai, Ms. Yu hen Wu, and Ms. Xi Yu Su for their assistance.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Switching between augmented reality and a manual-visual task: A preliminary study,"Fereydooni, Nadia (1); Kun, Andrew L. (1); Shaer, Orit (2) ","(1) University of New Hampshire, Durham; NH; 03824, United States (2) Wellesley College, Wellesley; MA; 02481, United States ","Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",cerence; Helmholtz Instituut; here; Rijkswaterstaat - Ministry of Infrastructure and Water Management; Uber ARG; Utrecht University - Faculty of Social and Behavioral Sciences,"Association for Computing Machinery, Inc",,p 99-103,21-Sep-19,"Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",2019,,,,9.78145E+12,10.1145/3349263.3351502,,"11th ACM International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019","September 21, 2019 - September 25, 2019",,"In this project we designed an Augmented Reality working space for drivers in a semi-automated vehicle, and evaluated users visual behavior in this space specifically during switching from the AR non-driving task to a Manual-Visual task. The results of this preliminary study show that users do not switch between the two provided tasks immediately after they are asked to. One possible explanation for this gradual transition is the timing of the interruption during the sub task. These results also provide a potential path for how to proceed with the study. © 2019 Copyright is held by the owner/author(s).",14,User interfaces,Augmented reality - Automation - Eye tracking,Automated driving - Automated vehicles - Driving tasks - Gradual transition - Human-centered computing - Mixed/augmented reality - Visual behavior - Working space,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731 Automatic Control Principles and Applications",,,"Number: 1840085, Acronym: -, Sponsor: -; ",Andrew Kun and Orit Shaer were supported in part by NSF CMMI grant 1840085.,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Incarar: A design space towards 3D augmented reality applications in vehicles,"Wiegand, Gesa (1); Mai, Christian (2); Holländer, Kai (2); Hussmann, Heinrich (2) ","(1) fortiss Munich, Germany (2) LMU Munich, Munich, Germany ","Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",cerence; Helmholtz Instituut; here; Rijkswaterstaat - Ministry of Infrastructure and Water Management; Uber ARG; Utrecht University - Faculty of Social and Behavioral Sciences,"Association for Computing Machinery, Inc",,p 1-13,21-Sep-19,"Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",2019,,,,9.78145E+12,10.1145/3342197.3344539,,"11th ACM International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019","September 21, 2019 - September 25, 2019",,"Advances in vehicle automation and the resulting change of the interior of cars lead to new challenges for user interface concepts. Augmented reality (AR) is a promising solution for the emerging design needs due to its diverse opportunities for user interaction and presenting information. This paper supports the development of novel AR applications. We describe a corresponding use case set consisting of 98 examples from a literature review and two focus groups. Based on these samples we present a design space for in-car AR applications. To demonstrate the benefit thereof, we show a fictional design process including our proposed design space to derive a custom AR system. This work supports designers and engineers by providing a systematic approach for integrating 3D AR interfaces in a vehicle, excluding windshields and windows. © 2019 Association for Computing Machinery.",70,User interfaces,Augmented reality - Design - Vehicles,AR application - Augmented reality applications - Automotive - Design spaces - In-vehicle automation - Literature reviews - Presenting informations - User interaction,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented Reality Based Therapy System for Social Skill Deficits,"Sha, K. (1); Zhandong Liu (2); Dempsey, J. (3) ","(1) Univ. of Houston-Clear Lake, Houston, TX, United States (2) Baylor Coll. of Med., Houston, TX, United States (3) Children's Hosp. Colorado, Univ. of Colorado, Aurora, CO, United States ","2019 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE). Proceedings",,"IEEE, Piscataway, NJ, USA",,19-20,2019,,,,,,978-1-7281-4687-4,10.1109/CHASE48038.2019.00015,,"2019 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",25-27 Sept. 2019,"Arlington, VA, USA","Treating social skill deficits caused by Autism Spectrum Disorder (ASD) has been a significant challenge. Applied Behavior Analysis (ABA) has shown to be an effective treatment; however, many patients have to leave ABA because of the high cost, lack of insurance coverage, and shortage of ABA-skilled providers. This paper proposes and designs an augmented reality (AR) based system that aims to ease and improve the effectiveness of ABA, as well as to reduce its cost as the ABA can be performed in a clinic or at home with the support of the system. The system consists of three major components, including a web-based module, a HoloLens-based AR module, and a central control module. These modules work together to achieve an efficient treatment system. Ten therapy scenarios are designed and deployed in the prototype system to test the effectiveness of the system.",7,,augmented reality - Internet - medical computing - medical disorders - patient treatment - virtual reality,therapy system - social skill deficits - autism spectrum disorder - applied behavior analysis - insurance coverage - ABA-skilled providers - augmented reality based system - web-based module - HoloLens-based AR module - central control module,C7330 Biology and medical computing - C6130V Virtual reality - C7210N Information networks,G06F19/00,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Augmented reality and museum accessibility: A case study to support hard of hearing people,"Barbosa, Priscyla (1); Amorim, Patrícia (1); Leal Ferreira, Simone Bacellar (1) ","(1) Universidade Federal Do Estado Do, Rio de Janeiro - UNIRIO, Rio de Janeiro, Brazil ",IHC 2019 - Proceedings of the 18th Brazilian Symposium on Human Factors in Computing Systems,Brazilian Computer Society (SBC); CAPES; CNPq,"Association for Computing Machinery, Inc",,,22-Oct-19,IHC 2019 - Proceedings of the 18th Brazilian Symposium on Human Factors in Computing Systems,2019,Portuguese,,,9.78145E+12,10.1145/3357155.3358434,3358434,"18th Brazilian Symposium on Human Factors in Computing Systems, IHC 2019","October 21, 2019 - October 25, 2019",,"This article presents the results of a first analysis of the construction process of an assistive augmented reality technology to support the spontaneous visit of deaf and hard of hearing to museums, in order to identify all barriers and benefits that may assist the developers in the design of artifacts for deaf pre-linguistic people. For this purpose, it was built an application prototype, based on user-centered design, able to present the museum's exhibition content in different formats. © 2019 ACM.",38,Audition,Augmented reality - Exhibitions - Human engineering - User centered design,Accessiblity - Assistive - Augmented reality technology - Construction process - Hard of hearings - Human information processing,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Use of virtual and augmented reality as tools for visualization of information: A systematic review,"Cardoso, Alexandre (1); Cyrino, Gabriel F. (1); Viana, Jose C. (1); Junior, Mauricio J. A. (1); Almeida, Pedro A. M. T. (1); Lamounier, Edgard A. (1); Lima, Gerson F. M. (1) ","(1) Federal University of Uberlandia, Uberlandia, Brazil ",Advances in Intelligent Systems and Computing,,Springer Verlag,v 903,p 407-417,2019,"Intelligent Human Systems Integration 2019 - Proceedings of the 2nd International Conference on Intelligent Human Systems Integration IHSI 2019: Integrating People and Intelligent Systems, 2019",2019,,21945357,21945365,9.78303E+12,10.1007/978-3-030-11051-2_62,,"2nd International Conference on Intelligent Human Systems Integration, IHSI 2019","February 7, 2019 - February 10, 2019",,"Visualization of Information aims to present methodologies to optimize the cognition of the agent that seeks to identify, segment and learn from information that can be presented in various forms. Based on that, this study aims to identify the availability of information through virtual environments with a focus on Virtual Reality and Augmented Reality as a support for Visualization of Information. Thus, a Systematic Literature Review (SLR) at IEEE Xplore, ScienceDirect and ACM Digital Library databases, from September 20, 2016 to November 18, 2016. Of the 174 studies surveyed, 22 met the inclusion criteria. As an analysis, this article briefly presents the contributions of each of the articles, and a discussion is made of the applicability and research opportunities that can still be made in this area. © 2019, Springer Nature Switzerland AG.",28,Digital libraries,Augmented reality - Information systems - Integration - Intelligent systems - Virtual reality - Visualization,Information visualization - Library database - Research opportunities - Systematic literature review (SLR) - Systematic Review - Virtual and augmented reality - Visualization of information,"723 Computer Software, Data Handling and Applications - 903.2 Information Dissemination - 921.2 Calculus",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Towards in-situ process tomography data processing using augmented reality technology.,"Nowak, Adam (1); Woniak, Mikolaj (1); Rowiska, Zdzislawa (1); Grudzie, Krzysztof (1); Romanowski, Andrzej (1) ","(1) Institute of Applied Computer Science, Lodz University of Technology, Lodz, Poland ",UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,p 168-171,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,10.1145/3341162.3343782,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,"Nowadays, various branches of industry are based on continuous processes. Therefore, efficient and accurate data analysis became crucial for maintaining control and optimizing the monitored trials. This paper presents a novel solution for in-situ analysis of complex numerical data. The proposed system employs mixed-reality technology to visualize data and enable collaborative analysis in remote locations. The system was implemented using Microsoft HoloLens and tested in laboratory environment, with its proof-of-concept version applied to solve real expert analysis task. After the experiment NASA TLX and SUS questionnaires were filled and demonstrated improvement performance and enabling more extensive analysis, including their spatio-temporal features. © 2019 Copyright held by the owner/author(s).",12,Data handling,Augmented reality - Data reduction - Data visualization - Flow visualization - In situ processing - Information analysis - Mixed reality - NASA - Surveys - Tomography - Ubiquitous computing - Wearable computers,Augmented reality technology - Collaborative analysis - Continuous process - CSCW - In-situ analysis - Laboratory environment - Mixed reality technologies - Spatio temporal features,"631.1 Fluid Flow, General - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 746 Imaging Techniques - 903.1 Information Sources and Analysis",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Towards interactions with augmented reality systems in hyper-connected cars,"Schipor, Ovidiu-Andrei (1); Vatavu, Radu-Daniel (1) ","(1) MintViz Lab, MANSiD Research Center, University Stefan cel Mare of Suceava, Romania ",CEUR Workshop Proceedings,,CEUR-WS,v 2503,p 76-82,2019,"HCI Engineering 2019 - Joint Proceedings HCI Engineering 2019 - Methods and Tools for Advanced Interactive Systems and Integration of Multiple Stakeholder Viewpoints, co-located with 11th ACM SIGCHI Symposium on Engineering Interactive Computing Systems, EICS 2019",2019,,16130073,,,,,"2019 Joint HCI Engineering - Methods and Tools for Advanced Interactive Systems and Integration of Multiple Stakeholder Viewpoints, HCI Engineering 2019",18-Jun-19,,"Hyper-connected cars can store, process, and share a large amount and variety of digital content, which creates opportunities for using high-definition Augmented Reality (AR) and live video streaming to enhance current in-vehicle driving assistance and navigation systems. However, several challenges must be overcome to make such systems viable and efficient, such as dealing effectively with a variety of smart devices, platforms, and in-vehicle standards and technologies or delivering dynamic digital content to users in interactive time. In this paper, we propose a solution to these challenges by modeling the smart car as a distinct type of a smart environment. This model enables us to introduce a five-layer software architecture proposal based on Euphoria, a recent high-performing event-driven software architecture design for supporting effective communications between heterogeneous I/O devices in generic smart environments. We discuss the ways in which Euphoria can provide effective solutions to our identified challenges and hope that our contributions will stimulate interesting discussions towards defining a practical roadmap of engineering interactions with AR systems and high-definition video for hyper-connected cars. Copyright © 2019 for this paper by its authors.",28,Digital devices,Augmented reality - Navigation systems - Software architecture,Augmented reality systems - Challenges - Driving assistance - Effective communication - Event-driven softwares - High definition video - Live video streaming - Smart devices,"723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: MRIS, Sponsor: Ontario Ministry of Research, Innovation and Science; ","This work was supported by a grant of the Romanian Ministry of Research and Innovation, CCCDI-UEFISCDI, Complex Project no. PN-III-P1-1.2-PCCDI-2017-0917, contract no. 21PCCDI/2018 (project P2), within PNCDI III.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Local foodie: Experience design of a mobile augmented reality application for tourists to encourage local food consumption,"Lee, Jeongeun (1); Kaipainen, Kirsikka (1); Väänänen, Kaisa (1) ","(1) Tampere University, Tampere, Finland ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 110-119,29-Jan-20,AcademicMindtrek 2020 - Proceedings of the 23rd International Academic Mindtrek Conference,2020,,,,9.78145E+12,10.1145/3377290.3377298,,"23rd International Academic Mindtrek Conference, AcademicMindtrek 2020","January 29, 2020 - January 30, 2020",,"Food is an essential part of travel experience. Consumption of locally produced food while traveling has the two-fold benefit of providing insight into the local culture and increasing the sustainability of tourism. However, finding local food often requires motivation and effort, as information about food ingredients' origins and supply chains is not easy to discover. This paper presents the three-phase experience design process of a prototype of a mobile augmented reality (MAR) application 'Local Foodie' designed to encourage tourists in Finland to consume local food. Adventure, autonomy, and competence were determined as experience goals for the application, and an interactive MAR prototype was created through iterative design. The results of a user evaluation (n=10) of the prototype suggest that the use of the application was intrinsically motivating, and the MAR elements contributed especially to the fulfillment of adventure and autonomy experience goals. Future work could leverage context-Awareness and personalization to further enhance the experience of adventure. © 2020 ACM.",36,Augmented reality,Design - Food supply - Motivation - Supply chains - Sustainable development - Tourism,Context- awareness - Experience design - Food ingredients - Iterative design - Local foods - Mobile augmented reality - Travel experiences - User evaluations,"723 Computer Software, Data Handling and Applications - 822.3 Food Products - 912 Industrial Engineering and Management - 912.4 Personnel - 913 Production Planning and Control; Manufacturing",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented-reality-enhanced product comparison in physical retailing,"Álvarez Márquez, Jesús Omar (1); Ziegler, Jürgen (1) ","(1) Universty of Duisburg-Essen, Duisburg, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 55-65,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,10.1145/3340764.3340800,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"Augmented reality technology has experienced great improvement in recent years and it has been successfully applied to industry and entertainment settings. However, its application in everyday contexts such as shopping is still very limited. One of the requirements to seamlessly incorporate augmented reality into everyday tasks is to find intuitive, natural methods to make use of it. Due to the inherent capabilities of augmented reality to work as a visual aid to explore and extend the knowledge a user has of the surroundings, this paper proposes the combination of AR technology and product advisors in a novel approach for product comparison. The user’s awareness of the differences between multiple physically present objects is enhanced through virtual augmentations, supporting an intuitive way of comparing two or more products while shopping. To assess the validity of the concept, a prototype for an AR-based shopping assistant for comparing vacuum cleaners has been implemented and evaluated in a user study, testing different methods of visual comparison and interaction. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",29,Augmented reality,Computer applications - Computer programming,Augmented reality technology - ITS applications - Natural interactions - Physical objects - User study - Visual aids - Visual comparison,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Factory Maintenance Application Using Augmented Reality,"Coscetti, Simone (1); Moroni, Davide (1); Pieri, Gabriele (1); Tampucci, Marco (1) ","(1) Institute of Information Science and Technologies, National Research Council of Italy, Pisa, Italy ",ACM International Conference Proceeding Series,University of Groningen; University of Las Palmas de Gran Canaria; University of Twente; Wolfram Research,Association for Computing Machinery,,,7-Jan-20,Proceedings of APPIS 2020 - 3rd International Conference on Applications of Intelligent Systems,2020,,,,9.78145E+12,10.1145/3378184.3378218,3378218,"3rd International Conference on Applications of Intelligent Systems, APPIS 2020","January 7, 2020 - January 9, 2020",,"Tissue converting lines represent one of the key plant in the paper production field: with them, paper tissue is converted into its final form for domestic and sanitary usage. One of the key points of the tissue converting lines is the productivity and the possibility to follow conversion process at relativity low cost. Despite the actual lines have yet an high productivity, the study of the state of the art has shown that choke points still exist, caused by inadequate automation. In this paper, we present the preliminary results of a project which aims at removing such obstacle towards complete automation, by introducing a set of innovations based on ICT solutions applied to advanced automation. In detail, advanced computer vision and video analytics methods will be applied to pervasively monitor converting lines and to automatically extract process information in order to self-regulate specific machine and global parameters. Big data analysis methodologies will be also integrated to obtain new knowledge and infer optimal management models which could be used for the predictive maintenance. Augmented reality interfaces are being designed and developed to support converting line monitoring and maintenance, both ordinary and extraordinary. An Artificial Intelligence module provides suggestions and instructions to the operators in order to guarantee production level even in case of unskilled staff. The automation of such processes will improve factory safety, decrease manual interventions and, thus, will increase production line up-time and efficiency. © 2020 ACM.",8,Tissue,Advanced Analytics - Artificial intelligence - Augmented reality - Automation - Intelligent systems - Predictive maintenance - Productivity,Conversion process - Global parameters - High productivity - Increase productions - Manual intervention - Optimal management - Process information - Tissue converting,"461.2 Biological Materials and Tissue Engineering - 723 Computer Software, Data Handling and Applications - 723.4 Artificial Intelligence - 731 Automatic Control Principles and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
AR-ia: Volumetric opera for mobile augmented reality,"Kelly, Sean (1); Cordingley, Samantha (1); Nolan, Patrick (2); Rhemann, Christoph (1); Fanello, Sean (1); Tang, Danhang (1); Osborn, Jude (1); Busch, Jay (1); Davidson, Philip (1); Debevec, Paul (1); Denny, Peter (1); Fyffe, Graham (1); Guo, Kaiwen (1); Harvey, Geoff (1); Izadi, Shahram (1); Lincoln, Peter (1); Ma, Wan-Chun Alex (1); Taylor, Jonathan (1); Yu, Xueming (1); Whalen, Matt (1); Dourgarian, Jason (1); Blanchett, Genevieve (2); French, Narelle (2); Sillitoe, Kirstin (1); Uglow, Tea (1); Spiteri, Brenton (2); Pearson, Emma (2); Kernot, Wade (2); Richards, Jonathan (1) ","(1) Google Inc. (2) Opera Queensland, Australia ","SIGGRAPH Asia 2019 XR, SA 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,p 1-3,17-Nov-19,"SIGGRAPH Asia 2019 XR, SA 2019",2019,,,,9.78145E+12,10.1145/3355355.3361890,,"SIGGRAPH Asia 2019 XR - International Conference on Computer Graphics and Interactive Techniques, SA 2019","November 17, 2019 - November 20, 2019",,"Motivated by recent availability of augmented and virtual reality platforms, we tackle the challenging problem of immersive storytelling experiences on mobile devices. In particular, we show an end-to-end system to generate 3D assets that enable real-time rendering of an opera on high end mobile phones. We call our system AR-ia and in this paper we walk through the main components and technical challenges of such a system, showing how to deliver an immersive mixed reality experience in every user's living room. © 2019 Association for Computing Machinery.",10,Mixed reality,Augmented reality - Interactive computer graphics - Mobile telecommunication systems - Three dimensional computer graphics,Augmented and virtual realities - End-to-end systems - Immersive - Living room - Mobile augmented reality - Real-time rendering - Technical challenges,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The audible artefact: Promoting cultural exploration and engagement with audio augmented reality,"Cliffe, Laurence (1); Mansell, James (2); Cormac, Joanne (3); Greenhalgh, Chris (1); Hazzard, Adrian (1) ","(1) School of Computer Science, University of Nottingham, United Kingdom (2) Department of Cultural, Media and Visual Studies, University of Nottingham, United Kingdom (3) Department of Music, University of Nottingham, United Kingdom ",ACM International Conference Proceeding Series,beLa; et al.; Holon; Routledge; Volvo; xln audio,Association for Computing Machinery,,p 176-182,18-Sep-19,"Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound, AM 2019",2019,,,,9.78145E+12,10.1145/3356590.3356617,,"14th International Audio Mostly Conference: A Journey in Sound, AM 2019","September 18, 2019 - September 20, 2019",,"This paper introduces two ongoing projects where audio augmented reality is implemented as a means of engaging museum and gallery visitors with audio archive material and associated objects, artworks and artefacts. It outlines some of the issues surrounding the presentation and engagement with sound based material within the context of the cultural institution, discusses some previous and related work on approaches to the cultural application of audio augmented reality, and describes the research approach and methodology currently engaged with in developing an increased understanding in this area. Additionally, it discusses the project within the context of related cultural and sound studies literature, presents some initial conclusions as a result of a practice-based approach, and outlines the next steps for the project. © 2019 ACM.",27,Audio acoustics,Augmented reality,Audio - Augmented - Cultural - Experience - Locative - Soundscapes,"723 Computer Software, Data Handling and Applications - 751.1 Acoustic Waves",,,"Number: -, Acronym: RCUK, Sponsor: Research Councils UK; ",The author is supported by the Horizon Centre for Doctoral Training at the University of Nottingham (RCUK Grant No. EP/L015463/1) and Fusing Audio and Semantic Technologies (FAST).,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Ethnographic study of a commercially available augmented reality hmd app for industry work instruction,"Pringle, Andrew (1); Van Esch, Robin (2); Hutka, Stefanie (3); Heffernan, Niall (4); Mom, Jesse (4); Chen, Paul (5) ","(1) School of Psychology, Trinity College Dublin, Dublin, Ireland (2) BAM Advies and Engineering, Royal BAM Group Utrecht Area, Netherlands (3) Adobe Design Research and Strategy Adobe Inc., San Francisco; CA, United States (4) Sales DAQRI, Curtiss-Wright Dublin, Ireland (5) Product Management DAQRI and Komprise, Los Angeles; CA, United States ",ACM International Conference Proceeding Series,The Department of Computer Science and Engineering at UTA; The Human Centered Computing Laboratory (Heracleia) at UTA; The iPerform Industry-University NSF Center at UTA; The National Center for Scientific Research (NCSR)-Demokritos; The National Science Foundation (NSF),Association for Computing Machinery,,p 389-397,5-Jun-19,"Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019",2019,,,,9.78145E+12,10.1145/3316782.3322752,,"12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019","June 5, 2019 - June 7, 2019",,"Industrial applications of Augmented Reality (AR) are becoming increasingly commonplace but there are only a small number of published user studies examining the use of commercially available AR technologies, like AR HMDs, with real workers in real industry settings. This paper presents ethnographic research of an industry task that includes the context of the industry procedure, pain-points with current methods and a user experience study of an HMD-delivered AR application for delivering work instructions to support engineers performing the procedure. The AR application is delivered to engineers with different levels of experience through a commercially-available AR HMD (the DAQRI Smart Glasses®). Engineers (users) were observed and video recorded by researchers as they performed the procedure in the real-world setting of a sprinkler room of a hospital in the Netherlands. Engineers who used AR were found to deviate less from the correct procedure in comparison to an engineer who performed sprinkler maintenance using the current industry method, without AR instruction. Errors made by engineers on the procedure, together with semi-structured interview responses, shed light on customer pain points that AR can alleviate, useful UX/UI design considerations, barriers to adoption and insights for informing larger scale user evaluations of industry AR from maintenance to manufacturing. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",25,Helmet mounted displays,Augmented reality - Engineers - Maintenance,Barriers to adoption - Design considerations - Ethnographic study - Ethnography - Head mounted displays - Providing instructions - Qualitative method - Semi structured interviews,"723 Computer Software, Data Handling and Applications - 913.5 Maintenance",,,"Number: -, Acronym: SFI, Sponsor: Sustainable Forestry Initiative; ","The authors wish to thank DAQRI, Royal BAM group and the engineers who participated in the ethnographic study. The work of the first author was supported by a Science Foundation Ireland (SFI) Industry Fellowship, Grant #16/IFA/4331.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Evaluating learning approaches for product assembly Using chunking of instructions, spatial augmented reality and display based work instructions.","Wilschut, Ellen S. (1); Van Rhijn, Gu J.W. (1); Könemann, Reinier (1); Bosch, Tim (1); Murphy, Molly S. (1) ","(1) TNO Sustainable Productivity and Employability, Leiden, Netherlands ",ACM International Conference Proceeding Series,The Department of Computer Science and Engineering at UTA; The Human Centered Computing Laboratory (Heracleia) at UTA; The iPerform Industry-University NSF Center at UTA; The National Center for Scientific Research (NCSR)-Demokritos; The National Science Foundation (NSF),Association for Computing Machinery,,p 376-381,5-Jun-19,"Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019",2019,,,,9.78145E+12,10.1145/3316782.3322750,,"12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019","June 5, 2019 - June 7, 2019",,"Augmented Reality (AR) as an assistive technology is a promising tool for novice operators to learn assembly processes. This experiment compared an AR instruction method to display based electronic working instructions (EWI) for product assembly, to assess learning during the first repetitions of the products. In addition, two types of work instructions were used, i.e., standard and chunk instructions. In this experiment a chunk instruction consists of six assembly steps. Effects of the instruction method and type on the learning phase were evaluated with 24 novice operators building two products i.e.. Operators were then asked to build the same products without instructions in order to assess learned skills and establish effects on the recall phase, also as a result of instruction method and type. Task completion time (TCT), product quality, operator workload and learning curve were measured. The learning curve, as indicated by the TCT, took place during the first three repetitions of product assembly. Instruction method and instruction type had no effect on the learning curve. Product quality was high and no differences were found between learning conditions. Operator workload revealed that chunking of the instruction increased workload during the learning phase. During the recall phase, the AR group's TCT increased 19.2%, but only for the first product's repetition without instruction. Product quality remained the same during the recall phase, however operator workload was reduced for chunk learned products. This study indicates that chunking of instructions should be avoided for novice workers. Both EWI and AR can be used for teaching new assembly procedures. While AR and EWI are useful during the learning phase, there are indications that these methods might hinder the operator once they required the necessary skills and knowledge to assemble the product. A possible solution is making instructions more adaptive to fit the skill proficiency of the operator. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",14,Assembly,Augmented reality - E-learning - Industry 4.0 - Learning systems - Manufacture - Quality control,Assistive technology - Electronic working instructions - Instruction methods - Learning curves - Operator 4.0 - Operator workload - Spatial augmented realities - Task completion time,"723 Computer Software, Data Handling and Applications - 913.3 Quality Assurance and Control - 913.4 Manufacturing",,,"Number: -, Acronym: EZ, Sponsor: Ministerie van Economische Zaken; ","This research was supported by the Dutch Ministry of Economic Affairs. We would like to thank Festo (Delft, The Netherlands) for the use of their products in this experiment and the participants in this study for their valuable contribution.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An empirical study of incorporation of augmented reality into civic education,"Zheng, Wei (1); Zhou, Yan (1); Qin, Yi (2) ","(1) China University of Geosciences, Wuhan; 430074, China (2) Silk Road Institute, China University of Geosciences, Wuhan; 430074, China ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 30-34,28-Jun-19,"Proceedings of the 2019 International Conference on Modern Educational Technology, ICMET 2019",2019,,,,9.78145E+12,10.1145/3341042.3341054,,"2019 International Conference on Modern Educational Technology, ICMET 2019","June 28, 2019 - June 30, 2019",,"This paper attempts to examine the effectiveness of educational application of augmented reality into civic education class. 166 college freshmen students were investigated in a randomized field teaching experiment to determine the statistical difference between the traditional teaching mode and new one which connects AR course materials to civic education course with academic emotions, learning strategies and environment as research variables. As suggested by the results of Independent t-test and Pearson correlation, we found that students' positive academic emotions in the class with AR based teaching materials increased with more self-regulated learning strategies adopted and improved classroom environment. The study has implications for the development of civic education curriculum, which encourages the incorporation of today's AR technology into civic education. © 2019 ACM.",9,Students,Argon - Augmented reality - Correlation methods - Curricula - Educational technology - Learning systems - Teaching,Classroom environment - Education curriculums - Educational Applications - Empirical studies - Pearson correlation - Self-regulated learning strategies - Statistical differences - Teaching materials,"723 Computer Software, Data Handling and Applications - 804 Chemical Products Generally - 901.2 Education - 922.2 Mathematical Statistics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Enhancing subject matter assessments utilizing augmented reality and serious game techniques,"Holtkamp, Brian (1); Alshair, Mohammed (1); Biediger, Daniel (1); Wilson, Michael (1); Yun, Chang (1); Kim, Kyungki (1) ","(1) University of Houston, Houston; TX, United States ",ACM International Conference Proceeding Series,Cal Poly CSSE; iFixit; Zynga,Association for Computing Machinery,,,26-Aug-19,"Proceedings of the 14th International Conference on the Foundations of Digital Games, FDG 2019",2019,,,,9.78145E+12,10.1145/3337722.3337743,46,"14th International Conference on the Foundations of Digital Games, FDG 2019","August 26, 2019 - August 30, 2019",,"In this paper, we utilize the Microsoft HoloLens, a wearable augmented-reality (AR) device, to investigate how well an AR-based assessment tool measures a student's comprehension of, skill in, and aptitude for a given subject matter. We added assessment capabilities to a serious game prototype built in collaboration with Construction Management faculty for their Occupational Safety and Health Administration (OSHA) safety course. In an effort to verify if these assessment elements are effective, we hosted a trial with sixteen university students who were enrolled in the OSHA safety course. The trial consisted of a traditional pen-and-paper exam and an AR-based assessment. The AR-based assessment required the students to identify unsafe situations of virtually simulated workers, construction equipment, and/or vehicles in an AR diorama of an active construction site. We selected OSHA topics from eight categories ranging from proper ladder usage on the work site to workers wearing the correct protective equipment for where they are and what they're doing. We find that 1.) students who performed well on a paper-based assessment also performed well within the AR-based assessment tool, 2.) students and faculty considered the AR trial more comprehensive and representative of knowledge, and 3.) user feedback reinforced the visual and interactive benefits of the AR over the paper-based exam. This trial shows promising results to adapt serious games and AR assessments into Computer Science and interdisciplinary subject matters that rely on visualization and spatial understanding. © 2019 ACM.",16,Serious games,Augmented reality - Construction equipment - Education - Occupational risks - Project management - Students - Wearable technology,Assessment - Construction management - Construction sites - Game prototypes - Occupational safety and health administrations - Protective equipment - Subject matters - University students,"405.1 Construction Equipment - 723 Computer Software, Data Handling and Applications - 912.2 Management",,,,"We would like to thank Carlos Puerta, Erick Ramirez, and Vranda Vijay for their help during the trial. We would like to thank Dr. Ahmed Senouci and Uday Vitthalbhai Anghan for their help while designing the prototype. We would like to thank Steve Mitchell for doing the modeling and animating for the assessment tool. We would like to thank Lee Hart and Charlene Rasmussen for putting together the paper assessment for the trial and help facilitate the trial during their summer course. We would like to thank the participants for taking their time out of the lecture to participate in the trial.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Automatic object recognition in a light-weight augmented reality-based vocabulary learning application for children with autism,"Tang, Tiffany Y. (1); Xu, Jiasheng (1); Winoto, Pinata (1) ","(1) Media Lab and Innovative Technology for Autism Research Center, Wenzhou-Kean University, China ",ACM International Conference Proceeding Series,University of Texas-Dallas; Xi'an Jiaotong-Liverpool University,Association for Computing Machinery,v Part F148152,p 65-68,2019,,2019,,,,9.78145E+12,10.1145/3319921.3319945,,"3rd International Conference on Innovation in Artificial Intelligence, ICIAI 2019","March 15, 2019 - March 18, 2019",,"A number of previous controlled studies have underscored the importance of early diagnosis and intervention in autism. However, despite the technological advances, augmented reality-based (AR) intervention for Chinese autistic children is still rare, which motivates our study. In particular, in this paper, we present a mobile vocabulary-learning application for Chinese autistic children by creating authentic opportunities in outdoor and home use. The core object recognition module is implemented in the deep learning platform, TensorFlow, on one hundred training models; unlike other sophisticated systems, the algorithm has to run in an offline fashion. A pilot study aiming at investigating the system's feasibility and usability had been conducted with typically developing children and their parents with very promising and satisfying results. We also further tested performance of the offline learning algorithm using seven animal toys with very satisfying results. Since the current literature of AR-technology on Chinese word-learning for children with special needs is still in its infancy and arguably lacks rigor in especially design and assessment, which thus offers limited insights into its therapeutic efficacy, feasibility and applicability of individualized intervention for autistic individuals, particularly children. It is our hope that this preliminary study adds to our understanding towards the usability and usefulness of such AR-based mobile learning application. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",22,Learning algorithms,Augmented reality - Computer aided instruction - Deep learning - Diagnosis - Diseases - E-learning - Object recognition,Autism - Lightweight - Offline - TensorFlow - Vocabulary learning,"461.6 Medicine and Pharmacology - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: LGJ19F020001, Acronym: -, Sponsor: Natural Science Foundation of Zhejiang Province; ",The authors gratefully acknowledge financial support from Zhejiang Provincial Natural Science Foundation of China (LGJ19F020001).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Perspectives on how to evaluate augmented reality technology tools for education: a systematic review,"da Silva, Manoela M. O. (1); Teixeira, João Marcelo X. N. (1, 2); Cavalcante, Patrícia S. (3); Teichrieb, Veronica (1) ","(1) Voxar Labs, Centro de Informática, Universidade Federal de Pernambuco, Recife, Brazil (2) Departamento de Eletrônica e Sistemas, Universidade Federal de Pernambuco, Recife, Brazil (3) EDUMATEC, Centro de Educação, Universidade Federal de Pernambuco, Recife, Brazil ",Journal of the Brazilian Computer Society,,Springer London,"v 25, n 1",,1-Dec-19,,2019,,1046500,16784804,,10.1186/s13173-019-0084-8,3,,,,"Education has benefited from augmented reality’s (AR) potential to promote interactive experiences both inside and outside the classroom. A systematic review was conducted on how AR’s impact in the learning process has been evaluated. We selected papers from 2009 to 2017 in three databases, IEEE, ACM, and Science Direct, using an open-source crawler, and in one Brazilian Conference, SBIE. We followed the PRISMA protocol. Forty-five works were selected and used to extract data for our research. They were also analyzed according to quantitative and qualitative criteria. The results from all the papers are available in an online database. Results evidenced an increase in the number of papers evaluating the AR’s impact in education. They also showed that AR has been applied in different areas and contexts. Most papers reported positive outcomes as a result of AR insertion. However, most studies lacked the involvement of the teacher and the use of multiple metrics to evaluate educational gains. © 2019, The Author(s).",83,Learning systems,Augmented reality - Paper,Augmented reality technology - Educational systems - Evaluation - Learning process - Online database - Open sources - Qualitative criteria - Systematic Review,"723 Computer Software, Data Handling and Applications - 811.1 Pulp and Paper",,,"Number: IBPG-0605-1.03/15, Acronym: FACEPE, Sponsor: Funda&Atilde;&sect;&Atilde;&pound;o de Amparo &Atilde;&nbsp; Ci&Atilde;&ordf;ncia e Tecnologia do Estado de Pernambuco; ",The authors would like to thank Funda&ccedil;&atilde;o de Amparo a Ci&ecirc;ncia e Tecnologia de Pernambuco (FACEPE) (processes IBPG-0605-1.03/15) for partially funding this research.,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Protecting visual information in augmented reality from malicious application developers,"Jensen, Jk (1); Hu, Jinhan (1); Rahmati, Amir (2); Likamwa, Robert (1) ","(1) Arizona State University, Tempe; AZ, United States (2) Stony Brook University, Stony Brook; NY, United States ","WearSys 2019 - Proceedings of the 5th ACM Workshop on Wearable Systems and Applications, co-located with MobiSys 2019",ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 23-28,12-Jun-19,"WearSys 2019 - Proceedings of the 5th ACM Workshop on Wearable Systems and Applications, co-located with MobiSys 2019",2019,,,,9.78145E+12,10.1145/3325424.3329659,,"5th ACM Workshop on Wearable Systems and Applications, WearSys 2019, co-located with MobiSys 2019",21-Jun-19,,"Visual applications - those that use camera frames as part of the application - allows for a rich, context-aware experience. The continuing development of mixed and augmented reality (MR/AR) on head-mounted displays (HMDs) furthers the richness of this experience by providing users a continuous vision experience, where visual information continuously provides context, and the real world is augmented by the virtual. However, these visual applications raise serious privacy concerns because they can capture private user information. To understand user privacy concerns in continuous vision computing environments, we study three MR/AR applications (augmented markers, augmented faces, and text capture). We show that in modern mobile visual applications, typical users are exposed to potential mass collection of sensitive information. To address such deficiencies, we develop a framework that provides resource isolation between user information contained in camera frames and application access to the network. We implement the design as a proof of concept on the Android operating system and demonstrate its performance and usability with a modern state-of-the-art augmented reality library and several augmented reality applications. By comparing the applications from our case study with modified versions which better protect user privacy, results show that our design efficiently protects users against data collection in MR/AR applications with less than 0.7% performance overhead. © 2019 ACM.",22,Augmented reality,Cameras - Helmet mounted displays - Wearable technology,Application developers - Augmented reality applications - Mixed and augmented realities - Operating system - Resource isolation - Sensitive informations - User privacy - Visual information,"723 Computer Software, Data Handling and Applications - 742.2 Photographic Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Creating accessible interactive audio-tactile drawings using spatial augmented reality,"Thevin, Lauren (1, 2); Jouffrais, Christophe (3, 4); Rodier, Nicolas (5); Palard, Nicolas (6); Hachet, Martin (7); Brock, Anke M. (8) ","(1) Inria Bordeaux, Talence, France (2) LMU München, Munich, Germany (3) CNRS, IPAL, UMI2955, Singapore, Singapore (4) CNRS, IRIT, UMI5505, Toulouse, France (5) Université Paul Sabatier, IRIT, Toulouse, France (6) RealityTech, Pessac, France (7) Inria Bordeaux., Talence, France (8) ENAC, Université de Toulouse, Toulouse, France ",ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 17-28,10-Nov-19,ISS 2019 - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces,2019,,,,9.78145E+12,10.1145/3343055.3359711,,"14th ACM International Conference on Interactive Surfaces and Spaces, ISS 2019","November 10, 2019 - November 13, 2019",,"Interactive tactile graphics have shown a true potential for people with visual impairments, for instance for acquiring spatial knowledge. Until today, however, they are not well adopted in real-life settings (e.g. special education schools). One obstacle consists in the creation of these media, which requires specific skills, such as the use of vector-graphic software for drawing and inserting interactive zones, which is challenging for stakeholders (social workers, teachers, families of people with visual impairments, etc.). We explored how a Spatial Augmented Reality approach can enhance the creation of interactive tactile graphics by sighted users. We developed the system using a participatory design method. A user study showed that the augmented reality device allowed stakeholders (N=28) to create interactive tactile graphics more efficiently than with a regular vector-drawing software (baseline), independently of their technical background. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",37,Drawing (graphics),Augmented reality,Accessibility - Accessible graphics - Content creation - Spatial augmented realities - Tactile drawings - Visual impairment,"723 Computer Software, Data Handling and Applications - 902.1 Engineering Graphics",,,,"This work was founded by the Erasmus+ Program of the European Union Pr. no 2016-1-EL01-KA201-023731, and AccessiMap ANR-14-CE17-0018. We thank the lab Cherchons pour Voir""", special education centers IJA and IRSA, the transcription center CTEB," and professionals and people with visual impairment participating in the studies.""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village
Personalized augmented reality via fog-based imitation learning,"Ahn, Surin (1); Gorlatova, Maria (2); Naghizadeh, Parinaz (3); Chiang, Mung (3) ","(1) Stanford University, Department of Electrical Engineering, Stanford; CA, United States (2) Duke University, Department of Electrical and Computer Engineering, Durham; NC, United States (3) Purdue University, Department of Electrical and Computer Engineering, West Lafayette; IN, United States ",IoT-Fog 2019 - Proceedings of the 2019 Workshop on Fog Computing and the IoT,ACM,"Association for Computing Machinery, Inc",,p 11-15,15-Apr-19,IoT-Fog 2019 - Proceedings of the 2019 Workshop on Fog Computing and the IoT,2019,,,,9.78145E+12,10.1145/3313150.3313219,,"2019 Workshop on Fog Computing and the IoT, IoT-Fog 2019",15-Apr-19,,"Augmented reality (AR) technologies are rapidly gaining momentum in society and are expected to play a critical role in the future of cities and transportation. In such dynamic settings with a heterogeneous population of AR users, it is important for holograms to be placed in the surrounding environment with regard to the users' preferences. However, the area of AR personalization remains largely unexplored. This paper proposes to use behavioral cloning, an algorithm for imitation learning, as a means of automatically generating policies that capture user preferences of hologram positioning. We argue in favor of employing the fog computing paradigm to minimize the volume of data sent to the cloud, and thereby preserve user privacy and increase both communication efficiency and learning efficiency. Through preliminary results obtained with a custom, Unity-based AR simulator, we demonstrate that user-specific policies can be learned quickly and accurately. © 2019 ACM.",27,Fog computing,Augmented reality - Clone cells - Cloning - Data privacy - Efficiency - Fog - Holograms - Internet of things,Behavioral cloning - Communication efficiency - Computing paradigm - Heterogeneous populations - Imitation learning - Learning efficiency - ML at the edge - Surrounding environment,"443.1 Atmospheric Properties - 461.2 Biological Materials and Tissue Engineering - 461.8.1 Genetic Engineering - 723 Computer Software, Data Handling and Applications - 743 Holography - 913.1 Production Engineering",,,"Number: HR001117C0052, Acronym: DARPA, Sponsor: Defense Advanced Research Projects Agency; Number: CSR-1812797, Acronym: NSF, Sponsor: National Science Foundation; ","This work was supported in part by the Comcast Innovation Fund Research Grant, NSF CSR-1812797 grant, and Defense Advanced Research Projects Agency (DARPA) under contract No. HR001117C0052 and No. HR001117C0048. The opinions, findings and conclusions expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. We thank the reviewers for their helpful feedback.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmenting television with augmented reality,"Saeghe, Pejman (1); Weir, Bruce (2); Vinayagamoorthy, Vinoba (3); Clinch, Sarah (1); Glancy, Maxine (3); Pattinson, Ollie (3); Golds, Paul (2); Stevens, Robert (1); Pettifer, Stephen Robert (1) ","(1) University of Manchester, Manchester, United Kingdom (2) BBC R and D Salford, United Kingdom (3) BBC R and D, London, United Kingdom ",TVX 2019 - Proceedings of the 2019 ACM International Conference on Interactive Experiences for TV and Online Video,ACM SIGCHI,"Association for Computing Machinery, Inc",,p 255-261,4-Jun-19,TVX 2019 - Proceedings of the 2019 ACM International Conference on Interactive Experiences for TV and Online Video,2019,,,,9.78145E+12,10.1145/3317697.3325129,,"6th ACM International Conference on Interactive Experiences for TV and Online Video, TVX 2019","June 5, 2019 - June 7, 2019",,"This paper explores the effects of adding augmented reality (AR) artefacts to an existing TV programme. A prototype was implemented augmenting a popular nature documentary. Synchronised content was delivered over a Microsoft HoloLens and a TV. Our preliminary findings suggest that the addition of AR to an existing TV programme can result in creation of engaging experiences. However, presenting content outside the traditional TV window challenges traditional storytelling conventions and viewer expectations. Further research is required to understand the risks and opportunities presented when adding AR artefacts to TV. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",12,Interactive television,Augmented reality - Television,Human-Media Interaction - MicroSoft - User engagement - User experience - User study,"716.4 Television Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,,The authors acknowledge funding from the UK EPSRC under grant numbers EP/N028228/1 (PACT-MAN) and EP/R512394/1 (BBC iCASE studentship).,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
GhostAR: A time-space editor for embodied authoring of human-robot collaborative task with augmented reality,"Cao, Yuanzhi (1); Wang, Tianyi (1); Qian, Xun (1); Rao, Pawan S. (1); Wadhawan, Manav (1); Huo, Ke (1); Ramani, Karthik (1) ","(1) School of Mechanical Engineering, Purdue University, West Lafayette; IN; 47907, United States ",UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 521-534,17-Oct-19,UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332165.3347902,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"We present GhostAR, a time-space editor for authoring and acting Human-Robot-Collaborative (HRC) tasks in-situ. Our system adopts an embodied authoring approach in Augmented Reality (AR), for spatially editing the actions and programming the robots through demonstrative role-playing. We propose a novel HRC workflow that externalizes user's authoring as demonstrative and editable AR ghost, allowing for spatially situated visual referencing, realistic animated simulation, and collaborative action guidance. We develop a dynamic time warping (DTW) based collaboration model which takes the real-time captured motion as inputs, maps it to the previously authored human actions, and outputs the corresponding robot actions to achieve adaptive collaboration. We emphasize an in-situ authoring and rapid iterations of joint plans without an offline training process. Further, we demonstrate and evaluate the effectiveness of our workflow through HRC use cases and a three-session user study. © 2019 ACM.",69,Human robot interaction,Augmented reality - Robot programming - User interfaces,Adaptive collaboration - Animated simulation - Collaboration models - Dynamic time warping - Embodied authoring - Embodied interaction - Human-robot collaboration - Time-space,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,"Number: 1637961, Acronym: -, Sponsor: -; Number: -, Acronym: IIS, Sponsor: Division of Information and Intelligent Systems; Number: FW-HTF 1839971, Acronym: NSF, Sponsor: National Sleep Foundation; Number: -, Acronym: -, Sponsor: Pfizer; ","This work was partially supported by the NSF under grants FW-HTF 1839971, IIS (NRI) 1637961 and IIP (PFI:BIC) 1632154. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agency.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Brick: A synchronous multiplayer augmented reality game for mobile phones,"Bhattacharyya, Po (1); Jadhav, Ketki (1); Hammer, Jessica (1); Jo, Yein (1); Nath, Radha (1) ","(1) Carnegie Mellon University, Pittsburgh; PA, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3313257,3313257,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Multiplayer augmented reality (AR) games allow players to inhabit a shared physical environment populated with interactive digital objects. However, currently available games fall short because of either limited synchronicity or limited opportunities for player movement. Here, we present Brick, a synchronous multiplayer AR game at the room scale. Brick's players collaborate to fill in a pattern of empty slots using digital 'bricks' scattered about the room. This paper provides an overview of Brick from a design and technical perspective. It also discusses how Brick extends the current scope of AR games to include collaborative gameplay. © 2019 Copyright held by the owner/author(s). ACM",14,Brick,Augmented reality - Human engineering,Collaborative - Collaborative gameplay - Digital Objects - Multiplayers - Physical environments - Synchronous,"414.2 Brick Materials - 461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,"We thank Verizon for their generous support of our research. We thank Mengqi Wu and Yujin Ariza for contributing toward developing Brick in high fidelity. We also thank Yujin Ariza for contributing the background score for Brick. Finally, we are immensely grateful to our playtesters for their time, openness, and honest feedback, without which Brick would not have become a reality.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
End-user robot programming case study: Augmented reality vs. teach pendant,"Kapinus, Michal (1); Materna, Zdeniek (1); Bambuek, Daniel (1); Beran, Vitiezslav (1) ","(1) Faculty of Information Technology, Brno University of Technology, Czech Republic ",ACM/IEEE International Conference on Human-Robot Interaction,ARM; Cambridge Consultants; et al.; FN Robotics; Furhat Robotics; Halodi,IEEE Computer Society,,p 281-283,23-Mar-20,HRI 2020 - Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction,2020,,,21672148,9.78145E+12,10.1145/3371382.3378266,,"15th Annual ACM/IEEE International Conference on Human Robot Interaction, HRI 2020","March 23, 2020 - March 26, 2020",,"The work presents a preliminary experiment aimed for comparing a traditional method of programming an industrial collaborative robot using a teach pendant, with a novel method based on augmented reality and interaction on a high-level of abstraction. In the experiment, three participants programmed a visual inspection task. Subjective and objective metrics are reported as well as selected usability-related issues of both interfaces. The main purpose of the experiment was to get initial insight into the problematic of comparing highly different user interfaces and to provide a basis for a more rigorous comparison, that is going to be taken out. © 2020 ACM.",11,Human robot interaction,Agricultural robots - Augmented reality - Man machine systems - Robot programming - User interfaces,Collaborative robots - End users - High level of abstraction - Objective metrics - Visual inspection task,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731.5 Robotics",,,"Number: LQ1602, Acronym: NPU, Sponsor: Northwestern Polytechnical University; ","The work was supported by Czech Ministry of Education, Youth and Sports from the National Programme of Sustainability (NPU II) project IT4Innovations excellence in science &ndash; LQ1602"".""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Rapid prototyping of augmented reality & virtual reality interfaces,"Nebeling, Michael (1) ","(1) University of Michigan, School of Information Information, Interaction Lab, Ann Arbor; MI, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300826,3300826,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"This course introduces participants to rapid prototyping techniques for augmented reality and virtual reality interfaces. Participants will learn about both physical prototyping with paper and Play-Doh as well as digital prototyping via new visual authoring tools for AR/VR. The course is structured into four sessions. After an introduction to AR/VR prototyping principles and materials, the next two sessions are hands-on, allowing participants to practice new physical and digital prototyping techniques. These techniques use a combination of new paper-based AR/VR design templates and smartphone-based capture and replay tools, adapting Wizard of Oz for AR/VR design. The fourth and final session will allow participants to test and critique each other's prototypes while checking against emerging design principles and guidelines. The instructor has previously taught the techniques to broad student audiences with a wide variety of non-technical backgrounds, including design, architecture, business, medicine, education, and psychology, who shared a common interest in user experience and interaction design. The course is targeted at non-technical audiences including HCI practitioners, user experience researchers, and interaction design professionals and students. A useful byproduct of the course will be a small portfolio piece of a first AR/VR interface designed iteratively and collaboratively in teams. © 2019 Copyright held by the owner/author(s). ACM",6,Curricula,Augmented reality - Human engineering - Iterative methods - mHealth - Rapid prototyping - Smartphones - User interfaces - Virtual reality,Digital prototyping - Interaction design - Physical prototyping - Play-Doh - Technical background - Virtual reality interfaces - Visual authoring tools - Wizard of Oz,"461.4 Ergonomics and Human Factors Engineering - 718.1 Telephone Systems and Equipment - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 901.2 Education - 921.6 Numerical Methods",,,,"Thanks to my former postdoc, Max Speicher, and current MSI student, Katy Madier, for their contributions to some of the prototyping techniques and materials used in this course. I also thank Loren Heubert-Aubry for his initial implementation of the platform we will be using during the course.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
PerfVis: Pervasive Visualization in Immersive Augmented Reality for Performance Awareness,"Merino, Leonel (1); Hess, Mario (2); Bergel, Alexandre (3); Nierstrasz, Oscar (2); Weiskopf, Daniel (1) ","(1) VISUS, University of Stuttgart, Stuttgart, Germany (2) SCG, University of Bern, Switzerland (3) ISCLab, DCC, University of Chile, Santiago, Chile ",ICPE 2019 - Companion of the 2019 ACM/SPEC International Conference on Performance Engineering,ACM SIGMETRICS; ACM SIGSOFT; SPEC,"Association for Computing Machinery, Inc",,p 13-16,4-Apr-19,ICPE 2019 - Companion of the 2019 ACM/SPEC International Conference on Performance Engineering,2019,,,,9.78145E+12,10.1145/3302541.3313104,,"10th ACM/SPEC International Conference on Performance Engineering, ICPE 2019","April 7, 2019 - April 11, 2019",,"Developers are usually unaware of the impact of code changes to the performance of software systems. Although developers can analyze the performance of a system by executing, for instance, a performance test to compare the performance of two consecutive versions of the system, changing from a programming task to a testing task would disrupt the development flow. In this paper, we propose the use of a city visualization that dynamically provides developers with a pervasive view of the continuous performance of a system. We use an immersive augmented reality device (Microsoft HoloLens) to display our visualization and extend the integrated development environment on a computer screen to use the physical space. We report on technical details of the design and implementation of our visualization tool, and discuss early feedback that we collected of its usability. Our investigation explores a new visual metaphor to support the exploration and analysis of possibly very large and multidimensional performance data. Our initial result indicates that the city metaphor can be adequate to analyze dynamic performance data on a large and non-trivial software system. © 2019 ACM.",19,Visualization,Augmented reality - Computer software - Display devices,Design and implementations - Dynamic performance - Immersive augmented realities - Integrated development environment - Performance engineering - Programming tasks - Software visualization - Visualization tools,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Foveated AR: Dynamically-foveated augmented reality display,"Kim, Jonghyun (2); Jeong, Youngmo (1, 2); Stengel, Michael (2); Akit, Kaan (2); Albert, Rachel (2); Boudaoud, Ben (2); Greer, Trey (2); Kim, Joohwan (2); Lopes, Ward (2); Majercik, Zander (2); Shirley, Peter (2); Spjut, Josef (2); McGuire, Morgan (2); Luebke, David (2) ","(1) Seoul National University, Korea, Republic of (2) NVIDIA, 2788 San Tomas Expressway, Santa Clara; CA; 95051, United States ",ACM Transactions on Graphics,,Association for Computing Machinery,"v 38, n 4",,Jul-19,,2019,,7300301,15577368,,10.1145/3306346.3322987,99,,,,"We present a near-eye augmented reality display with resolution and focal depth dynamically driven by gaze tracking. The display combines a traveling microdisplay relayed off a concave half-mirror magnifier for the high-resolution foveal region, with a wide field-of-view peripheral display using a projector-based Maxwellian-view display whose nodal point is translated to follow the viewer’s pupil during eye movements using a traveling holographic optical element. The same optics relay an image of the eye to an infrared camera used for gaze tracking, which in turn drives the foveal display location and peripheral nodal point. Our display supports accommodation cues by varying the focal depth of the microdisplay in the foveal region, and by rendering simulated defocus on the 'always in focus' scanning laser projector used for peripheral display. The resulting family of displays significantly improves on the field-of-view, resolution, and form-factor tradeoff present in previous augmented reality designs. We show prototypes supporting 30, 40 and 60 cpd foveal resolution at a net 85 × 78 field of view per eye. © 2019 Association for Computing Machinery.",71,Holographic displays,Augmented reality - Display devices - Eye movements - Eye tracking,And Phrases: foveated - Augmented - Infra-red cameras - Maxwellian view - Peripheral displays - Scanning lasers - Varifocal - Wide field of view,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Imagineering gamification using interactive augmented reality to develop digital literacy skills,"Choolarb, Tunyaboon (1); Premsmith, Jakkrit (2); Wannapiroon, Panita (2) ","(1) Business Computer, Siam Business Administration, Nonthaburi Technological College, Thailand (2) Information and Production, Technology Management, College of Industrial Technology, King Mongkut's University of Technology North Bangkok, Thailand ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 39-43,25-Oct-19,ICDTE 2019 - 2019 the 3rd International Conference on Digital Technology in Education,2019,,,,9.78145E+12,10.1145/3369199.3369222,,"3rd International Conference on Digital Technology in Education, ICDTE 2019","October 25, 2019 - October 27, 2019",,"The purpose of this research is to develop Imagineering Gamification using Interactive Augmented Reality, to be used for evaluating Digital Literacy skill of learners from their Interactive Augmented Reality learning through Imagineering Gamification model; to compare learners' learning achievements before and after learning through Imagineering Gamification using Interactive Augmented Reality with those who learn through normal classroom; and to survey learners' satisfaction of using Interactive Augmented Reality to develop Digital Literacy skills. The research composed of 80 students from Vocational Certificate of Education year 1 of Siam Business Administration Nonthaburi Technological College and were divided into two groups with a group of 40 each through simple random sampling method: experimental group and control group. The tools of the research were Imagineering Gamification model and the system of Interactive Augmented Reality for developing Digital Literacy skills, which were qualified by the field experts. The research result showed that learning through Imagineering Gamification using Interactive Augmented Reality was suitable with a very good level of overall outcome. The mean of Imagineering Gamification model was 4.96 (S.D. = 0.06) and the mean of the system of Interactive Augmented Reality for developing Digital Literacy skills was 4.98 (S.D. = 0.03). The mean of Digital Literacy skills evaluation, before and after learning, was 4.65 (S.D. = 1.57) and 17.78 (S.D. = 1.60) respectively, and the t-test result was 36.82 with a level of statistical significance of .01. The mean of evaluation of the experimental group's learning achievements after learning was 18.43 (S.D. = 1.61); the mean of evaluation of the control group's learning achievements after learning was 14.40 (S.D. = 1.74), and the t-test result was 10.40 with a level of statistical significance of .01. The survey of students' satisfaction from using the material has a very good level of overall outcome with the mean of 4.89 (S.D. = 0.32). © 2019 Association for Computing Machinery",9,E-learning,Augmented reality - Education computing - Educational technology - Students - Surveys,Business administration - Digital literacies - Gamification - Imagineering - Learning achievement - Simple random sampling - Statistical significance - Students' satisfaction,"723 Computer Software, Data Handling and Applications - 901.2 Education",,,"Number: -, Acronym: KMUTNB, Sponsor: King Mongkut's University of Technology North Bangkok; ","Thank you very much to Siam Business Administration Nonthaburi Technological College with many appreciations for supports of locations and samples Thank you very much to research supervisors, field experts, and teachers of Department of Information and Communication Technology for Education, Faculty of Technical Education, King Mongkut's University of Technology North Bangkok for advices in developing the materials that the model that was proven to be very successful in the study.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Use of augmented reality to teach basic computing concepts,"Rivera-Loaiza, Cuauhtémoc (1); Domínguez-Mota, Francisco J. (1); López-Huerta, María Isabel (1) ","(1) Facultad de Ciencias Físico-Matemáticas, Universidad Michoacana de San Nicolás de Hidalgo, Morelia, Mexico ",ACM International Conference Proceeding Series,ACM SIGCHI; ACM SIGCHI Latin American HCI Community,Association for Computing Machinery,,,30-Sep-19,CLIHC 2019 - Proceedings of the 9th Latin American Conference on Human Computer Interaction,2019,Spanish,,,9.78145E+12,10.1145/3358961.3358981,a20,"9th Latin American Conference on Human Computer Interaction, CLIHC 2019","September 30, 2019 - October 4, 2019",,"Being able to create a computer program is one of the main skill that the younger generations must be capable to have, beginning in their earliest stages of education. We believe that this will be a determinant factor in accessing to more rewarding professional careers and, hopefully, a better quality of life. However, it is noticeable how despite having ubiquitous access to computer equipment they often are unable to produce digital content. One of the main reasons is the lack of familiarity with the basic concepts required to structure a computer program. Our approach aims to curve that problem by offering our users with an introduction to computer programming that uses their own physical space as a testing ground, via their smartphones. We employ the Karel programming language and augmented reality to help students master the basic skills required for creating their own code. In this paper we present the earliest results of our basic prototyping tests and a scope of the following implementation stages. © 2019 Copyright held by the owner/author(s).",9,Human computer interaction,Augmented reality - Computer programming - Education - Employment - Students,Computer equipments - Determinant factors - Digital contents - Paper prototyping - Professional careers - Quality of life - Ubiquitous access - Younger generations,"723 Computer Software, Data Handling and Applications - 723.1 Computer Programming",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The use of augmented reality to represent gamification theory in user story training,"Tsai, Wei-Te (1); Chen, Chien-Hsu (1) ","(1) Department of Industrial Design, National Cheng Kung University, No.1, University Road, Tainan; 701, Taiwan ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 265-268,22-Jul-19,ICEMT 2019 - 2019 3rd International Conference on Education and Multimedia Technology,2019,,,,9.78145E+12,10.1145/3345120.3345131,,"3rd International Conference on Education and Multimedia Technology, ICEMT 2019","July 22, 2019 - July 25, 2019",,"The purpose of this study is to train students in the university's design department to organize and demonstrate the story of a user's situational intentions in the design process. The user context investigation is an integral step in any design activity, and it helps to better understand the target of the design, meet the users' needs, and provide the correct design direction in order to reduce the failure rate after a product goes on the market. Students in the design department have often been found to lack complete product design thinking, and the students' design is often out of touch with reality and cannot meet the needs of end users. The aim of this research is to investigate how to combine physical objects and augmented reality by randomly generating three-dimensional objects combining people, objects and scenes, and having students tell the user stories based on the results produced. Through logic and insight, the future is then applied to the observation of actual scenes. In this study, augmented reality and 3D printing techniques were used to create three teaching aids, each presented in a hexahedral physical form. Each teaching aid contained six randomly generated 3D objects. The random control was thus the students who had a total of 216 permutations. The results of the application are more integrated and more effective in the classroom, and students were found to be highly interested in interacting with the system. With the application of augmented reality and the uncertainty caused by random probability, the effect of gamification was found to increase the desire to learn, and strengthened cognitive and observational powers which were helpful in describing the user's story. © 2019 Association for Computing Machinery.",7,Students,3D printers - Augmented reality - Failure analysis - Multimedia systems - Personnel training - Product design,Complete products - Design activity - Design departments - Design thinking - Gamification - Physical objects - Three-dimensional object - User context,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 745.1.1 Printing Equipment - 912.4 Personnel - 913.1 Production Engineering",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Learning calculus with augmented reality and virtual environments,"Herrera, Linda Margarita Medina (1); Abalo, Marlene Aguilar (2); Ordóñez, Saúl Juárez (3) ","(1) Tecnológico de Monterrey, CGDL, Mexico (2) Tecnológico de Monterrey, CCM, Mexico (3) Tecnológico de Monterrey, CSF, Mexico ",ACM International Conference Proceeding Series,International Association of Computer Science and Information Technology,Association for Computing Machinery,,p 18-22,28-Oct-19,"Proceedings of the 2019 11th International Conference on Education Technology and Computers, ICETC 2019",2019,,,,9.78145E+12,10.1145/3369255.3369271,,"11th International Conference on Education Technology and Computers, ICETC 2019","October 28, 2019 - October 31, 2019",,"In this paper, we present how spatial visualization skills can be developed in engineering students, using augmented reality and remote virtual environments in calculus courses. Two tools have been specifically developed for this purpose: AVRAM (Remote Virtual Environments for the Learning of Mathematics), which allows the visualization and manipulation of surfaces in a virtual three-dimensional space and ARC (Augmented Reality in Calculus), which uses activity cards for each multivariable calculus topic. This paper describes the type of skills that can be developed with the use of these two apps in the dynamic visualization framework and presents some activities that have the purpose of developing logical thinking and problem solving and spatial abstraction skills. More than 1200 students have used these apps. The results of their use in calculus classes indicate an increase in students' engagement, visualization skills and a significant improvement in final grades. © 2019 Association for Computing Machinery.",19,Students,Augmented reality - Calculations - E-learning - Flow visualization - Three dimensional computer graphics - Virtual reality - Visualization,Dynamic visualization - Educational innovations - Mathematics education - Multivariable calculus - Spatial visualization skills - Students' engagements - Three dimensional space - Visualization skills,"631.1 Fluid Flow, General - 723 Computer Software, Data Handling and Applications - 921 Mathematics",,,"Number: -, Acronym: IPEP, Sponsor: Foundation for Education and European Culture; ","The authors would like to acknowledge the financial and technical support of Writing Lab, Tec Labs, Tecnol&oacute;gico de Monterrey in the production of this work. We want to acknowledge the Financial support of Novus Grant with PEP no. PHHT032-17CX00005, TecLabs, Tecnol&oacute;gico de Monterrey, in the production of this work. Our deepest appreciation to professors Gerardo Aguilar, Jaime Castro and Mart&iacute;n P&eacute;rez, responsible professors of the Mathematics III courses where part of the tests and measurements have been conducted, to Alejandro Flores for his technological support throughout this research and to all the members of the project, Touching Math: From Concepts to Reality through 3D Tools."" www.3dtouchingmath.com.""",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Visualization of the cosmic rays using 3D and augmented reality techniques,"Ferro, Diego (1); Flores, Julián (1); Garzón, Juan A. (2) ","(1) CITIUS Universidad de Santiago de Compostela, Spain (2) Universidad de Santiago de Compostela, Facultad de Física, Spain ",ACM International Conference Proceeding Series,"Asociacion para la Interaccion Persona - Ordenador (AIPO); CHISpa, Spanish chapter of ACM SIGCHI; Eusko Jaurlaritza - Gobierno Vasco; Fundacion ONCE; Universidad del Pas Vasco - Euskal Herriko Unibertsitatea",Association for Computing Machinery,,,25-Jun-19,"Proceedings of the 22nd International Conference on Human-Computer Interaction, INTERACCION 2019",2019,,,,9.78145E+12,10.1145/3335595.3335611,25,"22nd International Conference on Human-Computer Interaction, INTERACCION 2019","June 25, 2019 - June 28, 2019",,"The cosmic rays are formed mainly by protons and atomic nuclei that originate from extraterrestrial sources and continuously reach Earth's atmosphere. When they reach the atmosphere, they are fragmented into smaller pieces producing cascades of secondary particles. The analysis of secondary particles that reach the surface, by specific detectors, can provide very valuable information about different events such as; the activity of the sun, changes in the geomagnetic field or data on the atmosphere. The measures developed are influenced by the environment in which the detector is installed, since various elements of the building or environment in which it is located can interact with the rays. In this document, we present the novel application to the visualization of the cosmic rays measured in a high-resolution detector by 2D, 3D and augmented reality techniques. The detector used is located at the Faculty of Physics of the University of Santiago de Compostela, Spain and is known as TRAGALDABAS. © 2019 Authors.",4,Human computer interaction,Augmented reality - Cosmic rays - Cosmology - Data visualization - Earth atmosphere - Extraterrestrial atmospheres - Geomagnetism - Three dimensional computer graphics - Visualization,Atomic nuclei - Augment reality - Geomagnetic fields - High resolution detector - Novel applications - Secondary particles,"443.1 Atmospheric Properties - 481.3.2 Earth Magnetism and Terrestrial Electricity - 657 Space Physics - 657.2 Extraterrestrial Physics and Stellar Phenomena - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Augmented reality in science classroom: Perceived effects in education, visualization and information processing","Virata, Rholeo O. (1); Castro, Johan Daryll L. (1) ","(1) St. Scholastica's College, Manila Leon Guinto st., Malate, Manila; 2560, Philippines ",ACM International Conference Proceeding Series,Faculty of Science and Engineering; IEDRC.org,Association for Computing Machinery,,p 85-92,10-Jan-19,"IC4E 2019 - 2019 10th International Conference on E-Education, E-Business, E-Management and E-Learning",2019,,,,9.78145E+12,10.1145/3306500.3306556,,"10th International Conference on E-Education, E-Business, E-Management and E-Learning, IC4E 2019","January 10, 2019 - January 13, 2019",,"Technology in education has evolved over the decades and has provided more opportunities for technologies such as Augmented Reality to find its way into the pedagogy of teachers in science. In this paper, an AR app was used in facilitating a lesson on chemical bonding and simple compounds. The effects of such innovation were studied using observation and various data gathering methods in an action research design. Results show that AR is a tool that enhances visualization of concepts through elements of virtual and real images, allows students to map physical characteristics easily and aids them in developing mental images for further discourse. Results also show that AR does not only improve motivation but triggers more student-student interaction and teacher-student interaction as well. Students learning attitudes towards chemistry improved in terms of their appreciation of the subject, their perception of its relation to real life and of their meaning-making processes. The teacher indicated that AR apps can innovate the classroom and can provide opportunities for implementation as they are easy to use. However, findings also show that teachers may have a hard time looking for free resources of apps that will meet their needs, and that there is challenge in ensuring that misconceptions are avoided from AR apps' oversimplification of visualization. This study also presents some action plans for further use of AR apps in teaching science. © 2019 Association for Computing Machinery.",18,Students,Augmented reality - Chemical bonds - E-learning - Electronic commerce - Engineering education - Image enhancement - Visualization,Chemistry education - Data-gathering methods - K12 science - Physical characteristics - Science education - Student interactions - Student-student interaction - Technology in educations,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 801.4 Physical Chemistry - 901.2 Education",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Authorable augmented reality instructions for assistance and training in work environments,"Blattgerste, Jonas (1); Renner, Patrick (2); Pfeiffer, Thies (1) ","(1) University of Applied Sciences, Emden / Leer, Emden, Germany (2) CITEC - Cluster of Excellence Cognitive Interaction Technology, Bielefeld, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,10.1145/3365610.3365646,3365646,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"Augmented Reality (AR) is a promising technology for assistance and training in work environments, as it can provide instructions and feedback contextualised. Not only, but especially impaired workers can benefit from this technology. While previous work mostly focused on using AR to assist or train specific predefined tasks, 'general purpose' AR applications, that can be used to intuitively author new tasks at run-time, are widely missing. The contribution of this work is twofold: First we develop an AR authoring tool on the Microsoft HoloLens in combination with a Smartphone as an additional controller following considerations based on related work, guidelines and focus group interviews. Then, we evaluate the usability of the authoring tool itself and the produced AR instructions on a qualitative level in realistic scenarios and gather feedback. As the results reveal a positive reception, we discuss authorable AR as a viable form of AR assistance or training in work environments. © 2019 Copyright held by the owner/author(s).",49,Mixed reality,Augmented reality - Employment - Feedback - Human engineering - Personnel training,Annotation - AR application - Assistance - Authoring - Authoring tool - Cognitive impairment - Realistic scenario - Work environments,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 731.1 Control Systems - 912.4 Personnel",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Effects of augmented reality mobile apps on early childhood education students' achievement,"Jamiat, Nurullizam (1); Othman, Noor Fatin Nadia (1) ","(1) Universiti Sains Malaysia, USM, Centre for Instructional Technology and Multimedia, Penang; 11800, Malaysia ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 30-33,25-Oct-19,ICDTE 2019 - 2019 the 3rd International Conference on Digital Technology in Education,2019,,,,9.78145E+12,10.1145/3369199.3369203,,"3rd International Conference on Digital Technology in Education, ICDTE 2019","October 25, 2019 - October 27, 2019",,"Research trends on augmented reality (AR) showed that limited studies have been conducted on early education. The main objective of this study was to investigate the effects of an AR mobile apps on early childhood education students' achievement of learning alphabets. In this study, a total of 60 children at the age of five and six years old from a rural kindergarten were involved. A quasi-experimental research was applied in this study. The results showed that the children in the AR mobile apps group had a statistically significant higher mean score than the regular or non-AR mobile apps group. In conclusion, children learn alphabets better by using AR mobile apps than using the regular mobile apps. Therefore, it is recommended that more studies on AR mobile apps should be conducted in early childhood education as the technology helped children to learn at the early age. © 2019 Association for Computing Machinery.",12,Students,Augmented reality - E-learning - Educational technology,Early age - Early childhood educations - Experimental research - Learning alphabets - Mobile apps - Research trends - Student achievement,"723 Computer Software, Data Handling and Applications - 901.2 Education",,,"Number: A20190125, Acronym: -, Sponsor: -; Number: -, Acronym: -, Sponsor: Universiti Sains Malaysia; ","We would like to thank Research Creativity and Management Office (RCMO), Universiti Sains Malaysia for the financial assistance provided under the Fund for Overseas Conference: TPLN (A20190125).",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Care: An augmented reality support system for geriatric inpatients with mild cognitive impairment,"Wolf, Dennis (1); Besserer, Daniel (1); Sejunaite, Karolina (1); Schuler, Anja (2); Riepe, Matthias (1); Rukzio, Enrico (1) ","(1) Ulm University, Ulm, Germany (2) BKH Guenzburg, Guenzburg, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,10.1145/3365610.3365612,3365612,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"Cognitive impairment such as memory loss, an impaired executive function and decreasing motivation can gradually undermine instrumental activities of daily living (IADL). With an older growing population, previous works have explored assistive technologies (ATs) to automate repetitive components of therapy and thereby increase patients’ autonomy and reduce dependence on carers. While most ATs were built around screens and projection-based augmented reality (AR), the potential of head-mounted displays (HMDs) for therapeutic assistance is still under-explored. As a contribution to this effort we present cARe, an HMD-based AR framework that uses in-situ instructions and a guidance mechanism to assist patients with manual tasks. In a case study with six geriatric patients, we investigated the prototype’s feasibility during a cooking task in comparison to a regular paper-based recipe. Qualitative and quantitative results indicate that cARe has potential to offer assistance to older individuals with declining cognitive function in their day-to-day tasks and increase their independence in an enjoyable way. © 2019 Association for Computing Machinery.",67,Mixed reality,Augmented reality - Brain - Helmet mounted displays - Patient treatment,Activities of Daily Living - Assistive technology - Cognitive impairment - Dementia - Head mounted displays - IADL - Mild cognitive impairments - Repetitive components,"461.1 Biomedical Engineering - 461.6 Medicine and Pharmacology - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Projection-based augmented reality interface for robot grasping tasks,"Gong, L.L. (1); Ong, S.K. (1); Nee, A.Y.C. (1) ","(1) Mechanical Engineering Department, National University of Singapore, 9 Engineering Drive 1, Singapore; 117576, Singapore ",ACM International Conference Proceeding Series,China University of Geosciences; Guangdong University of Technology; University of Electronic Science and Technology of China; Xihua University,Association for Computing Machinery,,p 100-104,26-Jul-19,"Proceedings of the 2019 4th International Conference on Robotics, Control and Automation, ICRCA 2019 - Workshop 2019 the 4th International Conference on Robotics and Machine Vision, ICRMV 2019",2019,,,,9.78145E+12,10.1145/3351180.3351204,,"2019 4th International Conference on Robotics, Control and Automation, ICRCA 2019 and its Workshop of 2019 4th International Conference on Robotics and Machine Vision, ICRMV 2019","July 26, 2019 - July 28, 2019",,"This paper presents an augmented reality (AR) interface for robot programming of pick-and-place tasks as well as assembly operations. The aim of the AR interface is to increase the intuitiveness and ease of robot programming. Marker tracking is used to automate sensor registration for easier workspace setup. Object recognition is employed to transform robot programming from an absolute coordinate system to an object-based system. This transformation provides flexibility to robot programming as the AR interface can be applied to workspaces that are configured differently to accomplish the same task. A spatially immersive display method is adopted to provide a projection-based direct overlay virtual workspace, so as to give users a better frame of reference in relation to the real workspace. © 2019 Association for Computing Machinery.",13,Robot programming,Augmented reality - Computer vision - Human robot interaction - Object recognition - Robotics,Absolute coordinate - Assembly operations - Frame of reference - Immersive display - Marker tracking - Object-based systems - Sensor registration - Virtual workspaces,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 731.5 Robotics",,,,"The authors acknowledge contributions from Mr Koh Dong Koon. This research is supported by the Singapore A*STAR Agency for Science, Technology and Research Public Sector Research Funding Programme, Project No. 1521200081.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Audiencear - Utilising augmented reality and emotion tracking to address fear of speech,"Hartl, Philipp (1); Fischer, Thomas (1); Hilzenthaler, Andreas (1); Kocur, Martin (1); Schmidt, Thomas (1) ","(1) University of Regensburg, Regensburg, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 913-916,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,10.1145/3340764.3345380,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"With Augmented Reality (AR) we can enhance the reality by computer-generated information about real entities projected in the user’s field of view. Hence, the user’s perception of a real environment is altered by adding (or subtracting) information by means of digital augmentations. In this demo paper we present an application where we utilise AR technology to show visual information about the audience’s mood in a scenario where the user is giving a presentation. In everyday life we have to talk to and in front of people as a fundamental aspect of human communication. However, this situation poses a major challenge for many people and may even go so far as to lead to fear and and avoidance behaviour. Based on findings in previous work about fear of speech, a major cause of anxiety is that we do not know how the audience judges us. To eliminate this feeling of uncertainty, we created an AR solution to support the speaker while giving a speech by tracking the audience’s current mood and displaying this information in real time to the speaker’s view: AudienceAR. By doing so we hypothesise to reduce the speaker’s tension before and during presentation. Furthermore, we implemented a small web interface to analyse the presentation based on the audience mood after the speech is given. Effects will be tested in future work. © 2019 Copyright held by the owner/author(s).",18,Speech recognition,Augmented reality - Technology transfer,Affective Computing - Emotion - Facial - Hololens - Recognition,"723 Computer Software, Data Handling and Applications - 751.5 Speech",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Interactive spatial augmented reality system for Chinese opera,"Zhang, YanXiang (1); Shen, YiRun (1); Zhang, WeiWei (1); Zhu, ZiQiang (2); Ma, PengFei (1) ","(1) Department of Communication of Science and Technology, University of Science and Technology of China, China (2) Giant Interactive Group Inc, China ","ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3306214.3338566,,"ACM SIGGRAPH 2019 Posters - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"In this research, the authors designed an interactive spatial augmented reality system for stage performance based on the technologies of UWB positioning and Bluetooth triggering. The position of the actor is obtained through the antenna tag carried by the actor and the signal base station placed on the stage. Special effects can be triggered through the Bluetooth module according to the actor. The system has a higher degree of freedom in practical applications, which can present an interactive spatial augmented reality effect, and therefore provide new possibilities for the application of spatial augmented reality in the stage performance. The system could bring better immersive experience to the audiences, and it also brings new possibilities for the aesthetic creation of opera. © 2019 Copyright held by the owner/author(s).",3,Ultra-wideband (UWB),Antennas - Augmented reality - Bluetooth - Degrees of freedom (mechanics) - Interactive computer graphics,Antenna tags - Blue-tooth module - Chinese operas - Degree of freedom - Spatial ar - Spatial augmented realities - Stage performance - UWB positioning,"716.3 Radio Systems and Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 931.1 Mechanics",,,"Number: 2014BAH15F02, Acronym: -, Sponsor: -; ",The work is supported by China National Key Technology Support Program under Grant No.: 2014BAH15F02,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A vision of augmented reality for urban search and rescue,"LaLone, Nicolas (1); Alharthi, Sultan A. (2); Toups, Z.O. (2) ","(1) University of Nebraska at Omaha, Omaha; NE, United States (2) New Mexico State University, Las Cruces; NM, United States ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,19-Nov-19,HTTF 2019 - Proceedings of the Halfway to the Future Symposium 2019,2019,,,,9.78145E+12,10.1145/3363384.3363466,3363466,"2019 Halfway to the Future Symposium: Exploring the Past, Present, and Future of HCI and Design-Based Research, HTTF 2019","November 19, 2019 - November 20, 2019",,"Search and rescue (SAR) operations are often nearly computer-technology-free due to the fragility and connectivity needs of current information communication technology (ICT). In this design fiction, we envision a world where SAR uses augmented reality (AR) and the surplus labor of volunteers during crisis response efforts. Unmanned aerial vehicles, crowdsourced mapping platforms, and concepts from video game mapping technologies can all be mixed to keep SAR operations complexity-free while incorporating ICTs. Our scenario describes a near-future SAR operation with currently available technology being assembled and deployed without issue. After our scenario, we discuss socio-technical barriers for technology use like technical fragility and overwhelming complexity. We also discuss how to work around those barriers and how to use video games as a testbed for SAR technology. We hope to inspire more resilient ICT design that is accessible without training. © 2019 Copyright held by the owner/author(s).",30,Human computer interaction,Antennas - Augmented reality - Crowdsourcing - Drones - Interactive computer graphics - Mapping - Unmanned aerial vehicles (UAV),Design fictions - Disaster response - Search and rescue - Video game - Volunteers,"405.3 Surveying - 652.1 Aircraft, General - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,"Number: IIS-1651532, Acronym: NSF, Sponsor: National Science Foundation; ",We thank the creativity of game designers and emergency responders for always pushing use scenarios beyond everyday use. These worlds always give us important somethings to think about. This material is based upon work supported by the National Science Foundation under Grant No. IIS-1651532.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Healthcare augmentation: Social adoption of augmented reality glasses in medicine,"Adenuga, Kayode I. (1); Adenuga, Rahmat O. (2); Ziraba, Abdallah (1); Mbuh, Penn E. (3) ","(1) School of ICT, ICT University Messasi, Yaoundé, Cameroon (2) University Hospital Southampton, Hampshire, United Kingdom (3) School of Management, ICT University Messasi, Yaoundé, Cameroon ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 71-74,9-Apr-19,ICSIE 2019 - Proceedings of 2019 8th International Conference on Software and Information Engineering,2019,,,,9.78145E+12,10.1145/3328833.3328840,,"8th International Conference on Software and Information Engineering, ICSIE 2019","April 9, 2019 - April 12, 2019",,"Healthcare sector is recognized as one the most important sectors for the well-being of humanity. The recent technological advancements has brought about great enhancements in healthcare delivery. Augmented Reality (AR) is a phenomenon whereby real life objects in a recognized environments are amplified by additional visual information in order to support the process of augmentation. The benefits afford the opportunity to diagnose patient disease conditions during surgical procedure with high accuracy and precisions thereby reducing the incidence of medical errors. In spite of the numerous benefits AR technology has offered, its widespread adoption in clinical perspectives most especially towards enhancing medical practice and education in developing countries and Sub-Saharan African countries in particular have received little academic attention. The objective of this paper therefore is to examine some of these factors and propose a theoretical model of adoption for AR technology which can be validated in future studies. © 2019 Association for Computing Machinery",24,Health care,Augmented reality - Developing countries - Medicine,Adoption - Benefits - Challenges - Healthcare delivery - Healthcare sectors - Surgical procedures - Technological advancement - Theoretical modeling,"461.6 Medicine and Pharmacology - 461.7 Health Care - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Appropriating 3D printers in augmented reality,"Jasche, Florian (1); Ludwig, Thomas (1) ","(1) Cyber-Physical Systems, University of Siegen, Siegen, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 901-903,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,10.1145/3340764.3345377,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"Digital fabrication technologies, such as 3D printers, are receiving more and more attention, not only from professionals but also from hobbyists. However, even though people have easier access to these devices, 3D printers remain a black box for many users. To support the appropriation of 3D printers, this demonstration presents a system which extends a physical printer to include virtual components using augmented reality (AR). With these components, we try to explain how the printer works and allow the user to operate the printer through an AR application. We extend existing software with a custom solution to create a unique user interface and user experience. Our user interface provides a new way of inspecting models in AR before they are printed. © 2019 Copyright held by the owner/author(s).",4,3D printers,Augmented reality - Human computer interaction - Printing presses - User interfaces,Appropriation - AR application - Black boxes - Custom solutions - Digital fabrication - User experience - Virtual components,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 745.1.1 Printing Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A demonstration of care: An augmented reality support system for geriatric inpatients with mild cognitive impairment,"Wolf, Dennis (1); Besserer, Daniel (1); Sejunaite, Karolina (1); Schuler, Anja (2); Riepe, Matthias (1); Rukzio, Enrico (1) ","(1) Ulm University, Ulm, Germany (2) BKH Guenzburg, Guenzburg, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,10.1145/3365610.3368472,3368472,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"Cognitive impairment such as memory loss, an impaired executive function and decreasing motivation can gradually undermine instrumental activities of daily living (IADL). With an older growing population, previous works have explored assistive technologies (ATs) to automate repetitive components of therapy and thereby increase patients’ autonomy and reduce dependence on carers. While most ATs were built around screens and projection-based augmented reality (AR), the potential of head-mounted displays (HMDs) for therapeutic assistance is still under-explored. In this interactive demonstration, we present cARe, an HMD-based AR framework that uses in-situ instructions and a guidance mechanism to assist patients with manual tasks. In a case study with six geriatric patients, we investigated the prototype’s feasibility during a cooking task in comparison to a regular paper-based recipe. Qualitative and quantitative results indicate that cARe has potential to offer assistance to older individuals with declining cognitive function in their day-to-day tasks and increase their independence in an enjoyable way. © 2019 Copyright held by the owner/author(s).",23,Mixed reality,Augmented reality - Brain - Helmet mounted displays - Patient treatment,Activities of Daily Living - Assistive technology - Cognitive impairment - Dementia - Head mounted displays - IADL - Mild cognitive impairments - Repetitive components,"461.1 Biomedical Engineering - 461.6 Medicine and Pharmacology - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Locating nearby physical objects in augmented reality,"Gruenefeld, Uwe (1); Prädel, Lars (1); Heuten, Wilko (1) ","(1) OFFIS - Institute for IT, Oldenburg, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,10.1145/3365610.3365620,3365620,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"Locating objects in physical environments can be an exhausting and frustrating task, particularly when these objects are out of the user’s view or occluded by other objects. With recent advances in Augmented Reality (AR), these environments can be augmented to visualize objects for which the user searches. However, it is currently unclear which visualization strategy can best support users in locating these objects. In this paper, we compare a printed map to three different AR visualization strategies: (1) in-view visualization, (2) out-of-view visualization, and (3) the combination of in-view and out-of-view visualizations. Our results show that in-view visualization reduces error rates for object selection accuracy, while additional out-of-view object visualization improves users’ search time performance. However, combining in-view and out-of-view visualizations leads to visual clutter, which distracts users. © 2019 Copyright held by the owner/author(s).",45,Visualization,Augmented reality - Location,Head-mounted - Object selection - Occlusion - Out-of-view - Physical environments - Physical objects - Search time - Visual clutter,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Investigating universal appliance control throughwearable augmented reality,"Becker, Vincent (1); Rauchenstein, Felix (1); Soros, Gabor (1) ","(1) Department of Computer Science, ETH, Zurich, Switzerland ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,11-Mar-19,"Proceedings of the 10th Augmented Human International Conference, AH 2019",2019,,,,9.78145E+12,10.1145/3311823.3311853,a41,"10th Augmented Human International Conference, AH 2019","March 11, 2019 - March 12, 2019",,"The number of interconnected devices around us is constantly growing. However, it may become challenging to control all these devices when control interfaces are distributed over mechanical elements, apps, and configuration webpages.We investigate interaction methods for smart devices in augmented reality. The physical objects are augmented with interaction widgets, which are generated on demand and represent the connected devices along with their adjustable parameters. For example, a loudspeaker can be overlaid with a controller widget for its volume. We explore three ways of manipulating the virtual widgets: (a) in-air finger pinching and sliding, (b) whole arm gestures rotating and waving, (c) incorporating physical objects in the surrounding and mapping their movements to the interaction primitives. We compare these methods in a user study with 25 participants and find significant differences in the preference of the users, the speed of executing commands, and the granularity of the type of control. © 2019 Association for Computing Machinery.",26,C (programming language),Augmented reality - Ubiquitous computing - User interfaces,Adjustable parameters - Appliance controls - Control interfaces - Interaction methods - Mechanical elements - Smart objects - Tangible user interfaces - Wearable computing,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Dronesar: Extending physical spaces in spatial augmented reality using projection on a drone,"Darbar, Rajkumar (1); Roo, Joan Sol (1); Lainé, Thibault (1); Hachet, Martin (1) ","(1) Inria, Bordeaux, France ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,26-Nov-19,"MUM 2019 - 18th International Conference on Mobile and Ubiquitous Multimedia, Proceedings",2019,,,,9.78145E+12,10.1145/3365610.3365631,3365631,"18th International Conference on Mobile and Ubiquitous Multimedia, MUM 2019",26-Nov-19,,"Spatial Augmented Reality (SAR) transforms real-world objects into interactive displays by projecting digital content using video projectors. SAR enables co-located collaboration immediately between multiple viewers without the need to wear any special glasses. Unfortunately, one major limitation of SAR is that visual content can only be projected onto its physical supports. As a result, displaying User Interfaces (UI) widgets such as menus and pop-up windows in SAR is very challenging. We are trying to address this limitation by extending SAR space in mid-air. In this paper, we propose DroneSAR, which extends the physical space of SAR by projecting digital information dynamically on the tracked panels mounted on a drone. DroneSAR is a proof of concept of novel SAR User Interface (UI), which provides support for 2D widgets (i.e., label, menu, interactive tools, etc.) to enrich SAR interactive experience. We also describe the implementation details of our proposed approach. © 2019 Association for Computing Machinery.",42,User interfaces,Augmented reality - Drones - Space-based radar - Stained glass,3D interactions - Co-located collaboration - Digital contents - Digital information - Interactive display - Projections - Real-world objects - Spatial augmented realities,"716.2 Radar Systems and Equipment - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 812.3 Glass",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Improving search time performance for locating out-of-view objects in augmented reality,"Gruenefeld, Uwe (1); Prädel, Lars (1); Heuten, Wilko (1) ","(1) OFFIS - Institute for IT Oldenburg, Germany ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 481-485,8-Sep-19,"Mensch und Computer 2019, MuC 2019 - Tagungsband",2019,,,,9.78145E+12,10.1145/3340764.3344443,,"2019 Conference on Mensch und Computer, MuC 2019","September 8, 2019 - September 11, 2019",,"Locating virtual objects (e.g., holograms) in head-mounted Augmented Reality (AR) can be an exhausting and frustrating task. This is mostly due to the limited field of view of current AR devices, which amplify the problem of objects receding from view. In previous work, EyeSee360 was developed to address this problem by visualizing the locations of multiple out-of-view objects. However, on small field of view devices such as the Hololens, EyeSee360 adds a lot of visual clutter that may negatively affect user performance. In this work, we compare three variants of EyeSee360 with different levels of information (assistance) to evaluate in how far they add visual clutter and thereby negatively affect search time performance. Our results show that variants of EyeSee360 with less assistance result into faster search times. © 2019 Association for Computing Machinery.",19,Location,Augmented reality - Clutter (information theory),Field of views - Out-of-view - Search time - User performance - User study - Virtual objects - Visual clutter,"716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Browsing the web of things in mobile augmented reality,"Zachariah, Thomas (1); Dutta, Prabal (1) ","(1) University of California, Berkeley, Berkeley; CA, United States ",HotMobile 2019 - Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications,ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 129-134,22-Feb-19,HotMobile 2019 - Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications,2019,,,,9.78145E+12,10.1145/3301293.3302359,3302359,"20th International Workshop on Mobile Computing Systems and Applications, HotMobile 2019","February 27, 2019 - February 28, 2019",,"With the current augmented reality and low-power radio technology present on mobile platforms, we can imagine a standard and physically tangible browsing mechanism for objects in the Web of Things. We explore a model for user interaction with IoT devices that makes use of mobile augmented reality to allow users to identify new devices or easily access regularly-used devices in their environment, enables immediate interaction with quickly-obtainable user interfaces from the web, and provides developers a convenient platform to display custom interfaces for their devices. This model represents a step towards software-based interaction that might, one day, feel as intuitive, accessible, and familiar as the physical interfaces we commonly encounter in our daily lives. © 2019 ACM.",22,User interfaces,Augmented reality - Display devices - Internet of things - Mobile computing - Radio broadcasting - Web browsers,Daily lives - Device discovery - Low power radios - Mobile - Mobile augmented reality - Mobile platform - Physical interface - User interaction,"716.3 Radio Systems and Equipment - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: CNS-1824277, Acronym: NSF, Sponsor: National Science Foundation; Number: -, Acronym: SRC, Sponsor: Semiconductor Research Corporation; ","We wish to thank our shepherd, Xia Zhou, and the anonymous reviewers for their detailed comments and feedback. This work was supported in part by the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. This material is also based upon work partially supported by the National Science Foundation under grant CNS-1824277, as well as by the NSF/Intel Partnership on CPS Security and Privacy under grant CNS-1822332.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality with multi-view merging for robot teleoperation,"Huang, Bidan (1); Timmons, Nicholas Gerard (2); Li, Qiang (1) ","(1) Tencent Robotics X Lab, Shenzhen, China (2) University of Cambridge, Cambridge, United Kingdom ",ACM/IEEE International Conference on Human-Robot Interaction,ARM; Cambridge Consultants; et al.; FN Robotics; Furhat Robotics; Halodi,IEEE Computer Society,,p 260-262,23-Mar-20,HRI 2020 - Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction,2020,,,21672148,9.78145E+12,10.1145/3371382.3378336,,"15th Annual ACM/IEEE International Conference on Human Robot Interaction, HRI 2020","March 23, 2020 - March 26, 2020",,"This paper proposes a user-friendly teleoperation interface for manipulation. We provide the user with a view of the scene augmented with depth information from local cameras to provide visibility in occluded areas during manipulation tasks. This gives an improved sense of the 3D environment which results in better task performance. Further, we monitor the pose of the robot's end effector in real-time so that we are able to superimpose a virtual representation into the scene when parts are occluded. The integration of these features enables the user to perform difficult manipulation tasks when the action environment is normally occluded in the main camera view. We performed preliminary studies with this new setup and users provided positive feedback regarding the proposed augmented reality (AR) system. © 2020 ACM.",10,Human robot interaction,Agricultural robots - Augmented reality - Cameras - End effectors - Man machine systems - Remote control - User interfaces,Action environment - Augmented reality systems - Depth information - Manipulation task - Robot teleoperation - Robot's end effectors - Teleoperation interface - Virtual representations,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731.1 Control Systems - 731.5 Robotics - 742.2 Photographic Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
No story without a backstory: The role and importance of the backstory in an augmented reality application for cultural heritage,"Vanoverschelde, Fauve (1) ","(1) Tourism and Recreation Studies Howest University of Applied Sciences, Kortrijk, Belgium ",NHT 2019 - Proceedings of the 8th International Workshop on Narrative and Hypertext,ACM SIGWEB,"Association for Computing Machinery, Inc",,p 1-3,12-Sep-19,NHT 2019 - Proceedings of the 8th International Workshop on Narrative and Hypertext,2019,,,,9.78145E+12,10.1145/3345511.3349282,,"8th International Workshop on Narrative and Hypertext, NHT 2019, in conjunction with 30th ACM Conference on Hypertext and Social Media, HT 2019",17-Sep-19,,"The research presented below is an early-stage research project regarding the role and importance of backstory in an augmented reality application for cultural heritage. Literature and desk research show little attention for backstory when developing a storyline for extended reality applications in the cultural heritage field. However when looking at cinematic productions, theme parks and games for example, it can be stated that backstory plays an important role as a storytelling element to create a cohesive and immersive narrative. The hypothesis presented by Scan4Stories is that adding a backstory to the applications designed for cultural heritage sites, could help enhance the immersivity of the experience. To test the hypothesis presented above, we will develop a prototype, examining how the backstory of characters can be presented to the audience in an extended reality application on a cultural heritage site. By eventually combining the results from literature research and the development of a prototype we aim to formulate preliminary results concerning the use of backstory in cultural heritage extended reality applications. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",17,Software prototyping,Augmented reality - Hypertext systems,Augmented reality applications - Backstory - Cultural heritage field - Cultural heritages - Extended Reality - Literature researches - Storytelling - Theme park,"723 Computer Software, Data Handling and Applications - 723.1 Computer Programming",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Amón_ra: Augmented reality to value historic urban landscape,"Arias-Méndez, Esteban (1); Porras-Alfaro, David (2); García-Baltodano, Kenia (2) ","(1) Escuela de Ingeniería en Computación, Instituto Tecnológico de Costa Rica, Cartago, Costa Rica (2) Escuela de Arquitectura y Urbanismo, Instituto Tecnológico de Costa Rica, San José, Costa Rica ",ACM International Conference Proceeding Series,ACM SIGCHI; ACM SIGCHI Latin American HCI Community,Association for Computing Machinery,,,30-Sep-19,CLIHC 2019 - Proceedings of the 9th Latin American Conference on Human Computer Interaction,2019,,,,9.78145E+12,10.1145/3358961.3358977,a15,"9th Latin American Conference on Human Computer Interaction, CLIHC 2019","September 30, 2019 - October 4, 2019",,"The main purpose of Amón_RA (Amón Augmented Reality in Spanish) project is 'to develop augmented reality technology for the enhancement and dissemination of the historic urban landscape of Barrio Amón'. To achieve this goal, the research project seeks the determination of contents for patrimonial heritage (urban, architectural, social and cultural analysis) of Barrio Amón with technological developments linked to the creation of a mobile application of Augmented Reality (AR). Amón_RA is a research project in development whose objective is to generate a mobile application of augmented reality in areas little explored within the Costa Rican context such as the enhancement of the city and heritage resources. Barrio Amón, as a study area, is a reference site of the historical-social development of the city of San José, of great patrimonial interest. © 2019 Association for Computing Machinery.",13,Human computer interaction,Augmented reality - Mobile computing,Augmented reality technology - Costa Rica - Heritage resources - Mobile applications - Social development - Technological development - TICs - Urban landscape,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Mobile Pervasive Augmented Reality Systems MPARS: The Role of User Preferences in the Perceived Quality of Experience in Outdoor Applications,"Pascoal, Rui (1); Almeida, Ana De (2); Sofia, Rute C. (3, 4, 5) ","(1) Instituto Universitário de Lisboa, Information Sciences Technologies and Architecture Research Center, Portugal (2) Instituto Universitário de Lisboa, Center for Informatics and Systems, University of Coimbra, Information Sciences Technologies and Architecture Research Center, Portugal (3) FORTISS, Research Institute of the Free State of Bavaria for Software Intensive Systems and Services, Germany (4) Universidade Lusófona de Humanidades e Tecnologias, Portugal (5) Information Sciences, Technologies and Architecture Research Center, Portugal ",ACM Transactions on Internet Technology,,Association for Computing Machinery,"v 20, n 1",,7-Feb-20,,2020,,15335399,15576051,,10.1145/3375458,3375458,,,,"After briefly introducing aspects concerning Mobile Augmented Reality Systems, this article delves into the evolution of these systems as pervasive technology. The work debates also on acceptance of this technology in the context of outdoor applications. The need to develop context-aware, close-to-real-time feedback mechanisms that take into consideration a continuous measurement of Quality of Experience is also discussed. For this purpose, the work goes over how to integrate user preferences into context-aware feedback systems, proposing a theoretical model for measuring Quality of Experience. The model is derived from an analysis of previous technology adoption models and incorporates the knowledge of user preferences. This knowledge has been gathered via a public questionnaire. © 2020 Association for Computing Machinery.",38,Quality of service,Augmented reality,Augmented reality systems - Continuous measurements - Mobile augmented reality - Pervasive technologies - Quality of experience (QoE) - Real-time feedback - Technology adoption models - Theoretical modeling,"723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Exploring training modes for industrial augmented reality learning,"Heinz, Mario (1); Büttner, Sebastian (1); Röcker, Carsten (1) ","(1) Institute Industrial IT OWL, University of Applied Sciences Lemgo, NRW, Germany ",ACM International Conference Proceeding Series,The Department of Computer Science and Engineering at UTA; The Human Centered Computing Laboratory (Heracleia) at UTA; The iPerform Industry-University NSF Center at UTA; The National Center for Scientific Research (NCSR)-Demokritos; The National Science Foundation (NSF),Association for Computing Machinery,,p 398-401,5-Jun-19,"Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019",2019,,,,9.78145E+12,10.1145/3316782.3322753,,"12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019","June 5, 2019 - June 7, 2019",,"In this paper, we present a conceptual approach and the Mirst prototype of a mobile training system to provide non-expert users with helpful information about the functionality of complex automated industrial systems. The system uses an augmented reality (AR) tablet application to visualize information about internal processes, sensor states, settings and hidden parts of a production system directly in the Mield of view of a user. The available information can be accessed via four different methods which combine elements of step-by-step tutorials and open exploration. Our prototype aims to support users to better understand automated systems. While such systems will become more complex in future, we believe that augmented reality is a key concept that could help humans to better understand and experience automated systems and its consequences in general. © 2019 Association for Computing Machinery.",16,Automation,Augmented reality - Industry 4.0,Automated systems - Conceptual approaches - Industrial augmented reality - Industrial systems - Learning - Production system - Tablet applications - Training Systems,"723 Computer Software, Data Handling and Applications - 731 Automatic Control Principles and Applications",,,"Number: -, Acronym: BMBF, Sponsor: Bundesministerium f&uuml;r Bildung und Forschung; ",This work is funded by the German Federal Ministry of Education and Research (BMBF) for project ADIMA under grant number 13FH019PX5.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"Overlays and goggles and projections, oh my!: Exploring public perceptions of augmented reality technologies","Thompson, Alexandra (1); Potter, Leigh Ellen (1) ","(1) IDEA Lab, Griffith University, Nathan; QLD, Australia ",ACM International Conference Proceeding Series,Curtin University; Edith Cowan University (ECU); Human Factors and Ergonomics Society of Australia (HFESA); Perth Convention Bureau; The University of Western Australia (UWA); UX Machines Pty Ltd,Association for Computing Machinery,,p 295-301,2-Dec-19,"Proceedings of the 31st Australian Conference on Human-Computer-Interaction, OzCHI 2019",2019,,,,9.78145E+12,10.1145/3369457.3369482,,"31st Australian Conference on Human-Computer-Interaction, OzCHI 2019","December 2, 2019 - December 5, 2019",,"Augmented reality (AR) technologies have been available to the general public in varying formats for several years, but confusion remains about what AR actually is, and what it can do. This paper explores how well mental models of the general public align with the standing definitions of AR from an academic perspective. We also seek to understand whether individual experience with augmented reality technologies, or self-rated willingness to adopt new technologies, correlate with the accuracy of an individual's understanding of AR. A pilot survey asking participants to describe augmented reality revealed a variety of mental models, some of which aligned with academically defined characteristics of AR. The accuracy of the responses decreased in participants with no handson AR experience, and willingness to adopt new technology proved to have little to no influence on response accuracy. This paper presents some initial trends in public perceptions of augmented reality technologies, but also highlights the need for more research to establish a better understanding of mental models of AR. © 2019 Association for Computing Machinery.",21,Mixed reality,Augmented reality - Cognitive systems - Human computer interaction - Sensory perception - Surveys,Augmented reality technology - Emerging technologies - General publics - Mental model - Pilot surveys - Public perception - Public understanding,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Student learning achievement through augmented reality in science subjects,"Kularbphettong, Kunyanuth (1); Vichivanives, Rujijan (1); Roonrakwit, Pattarapan (2) ","(1) Suan Sunandha Rajabhat University, I U-TongNok Road, Dusit Bangkok, Thailand (2) Silpakorn University, Faculty of Information and Communication, Bangkok, Thailand ",ACM International Conference Proceeding Series,International Association of Computer Science and Information Technology,Association for Computing Machinery,,p 228-232,28-Oct-19,"Proceedings of the 2019 11th International Conference on Education Technology and Computers, ICETC 2019",2019,,,,9.78145E+12,10.1145/3369255.3369282,,"11th International Conference on Education Technology and Computers, ICETC 2019","October 28, 2019 - October 31, 2019",,"Augmented Reality or AR is a new technology that combines the real world with the virtual world through the smart technology devices. Using Augmented Reality technology to manage learning in classrooms is a new dimension in educational media and students give attention in learning. Teaching and learning of science subjects is challenging to make students understand when there are limited of equipment and student do not practice by his/herself. Now Thailand is facing with an education crisis and needs to reform education by focus on critical thinking skills. Therefore, this research proposes learning model by using AR technique and game based learning to enhance students' ability and data was collected both questionnaire and log data from e-Learning and e-Learning class activities consisted of the ability to perform problem analysis and design, development and testing, and evolution and testing. The participants were 200 teachers and students in the secondary school in Bangkok and surrounding areas who had applied the Augmented Reality (AR) in science subjects. The results showed that the mean score of posttest was significantly higher than the pretest and the average mean score of exercises was at a high level. To evaluate the result of students' attitudes toward learning via the proposed model, the questionnaire and interview form were applied to test students and the finding revealed that this proposed model are effective tools to learn and enhance self-practice and syntactic coding and problem-solving ability and student have positive attitudes to-wards learning model. © 2019 Association for Computing Machinery.",9,Students,Ability testing - Augmented reality - E-learning - Education computing - Engineering education - Learning systems - Surveys - Virtual reality,Augmented reality technology - Critical thinking skills - Development and testing - Game-based Learning - Mobile applications - Problem-solving abilities - Student learning - Teaching and learning,"723 Computer Software, Data Handling and Applications - 901.2 Education - 912.4 Personnel",,,,Our thanks to ACM SIGCHI for allowing us to modify templates they had developed.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Virtual and augmented reality for positive social impact,"Ferris, Kate (1); Kelly, Ryan M. (1); Brown, Ross (2); Wadley, Greg (1); Baker, Steven (1); Waycott, Jenny (1); Velloso, Eduardo (1); Turkay, Selen (2) ","(1) School of Computing and Information Systems, University of Melbourne, Melbourne, Australia (2) School of Electrical Engineering and Computer Science, Queensland University of Technology, Brisbane, Australia ",ACM International Conference Proceeding Series,Curtin University; Edith Cowan University (ECU); Human Factors and Ergonomics Society of Australia (HFESA); Perth Convention Bureau; The University of Western Australia (UWA); UX Machines Pty Ltd,Association for Computing Machinery,,p 8-11,2-Dec-19,"Proceedings of the 31st Australian Conference on Human-Computer-Interaction, OzCHI 2019",2019,,,,9.78145E+12,10.1145/3369457.3369549,,"31st Australian Conference on Human-Computer-Interaction, OzCHI 2019","December 2, 2019 - December 5, 2019",,"This workshop invites researchers and industry practitioners designing Virtual Reality and Augmented Reality applications for positive social impact to join us in Perth, Australia, to discuss the research and industry landscape and identify opportunities for collaboration with like-minded professionals. Participants will have an opportunity to share their experiences with other attendees, and will discuss the challenges they have experienced in working with VR and AR applications. This workshop will aim to help researchers to overcome these challenges and, as an outcome, plans will be made for future workshops to establish and promote positive social impact as a long-Term research focus for VR and AR researchers in Australasia. © 2019 Association for Computing Machinery.",12,Economic and social effects,Augmented reality - Human computer interaction - Industrial research - Virtual reality,"AR application - Augmented reality applications - Australasia - Perth , Australia - Research focus - Social impact - Virtual and augmented reality","723 Computer Software, Data Handling and Applications - 901.3 Engineering Research - 971 Social Sciences",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Training assistant for industrial processes through augmented reality,"Romero, Jonathan A. (1); Quero, Washington D. (1); Sánchez, Jorge S. (1); Andaluz, Víctor H. (1) ",(1) ecu ,ACM International Conference Proceeding Series,International Association of Computer Science and Information Technology,Association for Computing Machinery,,p 308-315,28-Oct-19,"Proceedings of the 2019 11th International Conference on Education Technology and Computers, ICETC 2019",2019,,,,9.78145E+12,10.1145/3369255.3369295,,"11th International Conference on Education Technology and Computers, ICETC 2019","October 28, 2019 - October 31, 2019",,"This article presents an application of augmented reality, as a contribution to the industrial and education sector, through a technological tool for the assistance of operators in industrial processes. It facilitates the identification of equipment and instruments, granting the management and visualization of the parts constitutive of the elements of the process. Additionally, it guides in the calibration of instruments and the simulation of a closed loop control algorithm that allows control by regulation. Its development focuses on the recognition of equipment through a Smartphone for detect characteristic points of objects and image recognition, 3D modeling through CAD software and integration in a multiplatform, incorporation of animations and mathematical modeling of industrial processes that allows of the development of closed loop control algorithms. The tests performed on the augmented reality application demonstrated an easy handling and high interactivity with the user. The results provide greater knowledge in the operation and structure of the equipment, instrument and process. Finally, the results support the skills development for the tuning of PID controllers in industrial processes. © 2019 Association for Computing Machinery.",28,Process control,3D modeling - Augmented reality - Closed loop control systems - Computer aided design - Controllers - Image recognition - Object detection - Three dimensional computer graphics - Three term control systems,3D animation - Augmented reality applications - Characteristic point - Closed-loop control - Industrial processs - Skills development - Technological tools - Tuning of pid controllers,"723 Computer Software, Data Handling and Applications - 731.1 Control Systems - 732.1 Control Equipment",,,"Number: -, Acronym: ESPE, Sponsor: Universidad de las Fuerzas Armadas ESPE; Number: -, Acronym: UCV, Sponsor: Universidad Central de Venezuela; Number: -, Acronym: FESIDE, Sponsor: Fundaci&Atilde;&sup3;n Emilio Soldevilla para la Investigaci&Atilde;&sup3;n y el Desarrollo en Econom&Atilde;&shy;a de la Empresa; Number: CEPRA-XIII-2019-08, Acronym: -, Sponsor: -; ","The authors would like to thanks to the Corporaci&oacute;n Ecuatoriana para el Desarrollo de la Investigaci&oacute;n y Academia &ndash;CEDIA for the financing given to research, development, and innovation, through the CEPRA projects, especially the project CEPRA-XIII-2019-08; Sistema Colaborativo de Robots A&eacute;reos para Manipular Cargas con &Oacute;ptimo Consumo de Recursos; also to Universidad de las Fuerzas Armadas ESPE, Escuela Superior Polit&eacute;cnica de Chimborazo, Universidad Nacional de Chimborazo, Universidad Teconologica Indoamerica, Universidad Internacional de Ecuador and Universidad Central de Venezuela, and Grupo de Investigaci&oacute;n en Automatizaci&oacute;n, Rob&oacute;tica y Sistemas Inteligentes, GI-ARSI, for the support to develop this work.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Mobile augmented reality techniques for emergency response,"Campos, Alexandre (1); Correia, Nuno (1); Romão, Teresa (1); Nunes, Isabel (2); Simões-Marques, Mário (3) ","(1) NOVA LINCS, Faculdade de Ciências e Tecnologia, Universidade NOVA de Lisboa, Portugal (2) UNIDEMI, Faculdade de Ciências e Tecnologia, Universidade NOVA de Lisboa, Portugal (3) CINAV- Portuguese Navy ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 31-39,12-Nov-19,"Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, MobiQuitous 2019",2019,,,,9.78145E+12,10.1145/3360774.3360825,,"16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, MobiQuitous 2019","November 12, 2019 - November 14, 2019",,"In an emergency situation, each response agent must act quickly and accurately. The support of a mobile device that can provide an appropriate insight of the surroundings and that allows users to exchange information with the other members of the emergency teams, can prevent harm and even save many lives. This paper presents a mobile application that integrates a georeferenced system with augmented reality techniques, in order to serve the needs of the operatives in emergency situations. The work intends to introduce solutions which optimize the performance with which the user takes advantage of the mobile application, such as the organization of the data flow that is displayed and the augmentation of the surrounding area. User studies were conducted with members of the National Navy. The results were positive although there are still some aspects that should be improved. © 2019 Association for Computing Machinery.",17,Emergency services,Augmented reality - Mobile computing - Ubiquitous computing,Data flow - Emergency response - Emergency situation - Location-based systems - Mobile applications - Mobile augmented reality - User study,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 914.1 Accidents and Accident Prevention",,,"Number: PEstUID/CEC/04516/2019 NOVA LINCS, Acronym: -, Sponsor: -; ","This work is funded by FCT/MCTES PEstUID/CEC/04516/2019 NOVA LINCS, the Portuguese Ministry of Defense, and the Portuguese Navy.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A virtual & An augmented reality application of the Rashid Agha's Diwakhana in the citadel of Erbil using 360° 3D lidar,"AlRashid Agha, Rawan A. (1); Monnet, Wrya (1) ","(1) Department of Computer Science and Engineering, University of Kurdistan-Hewler, Erbil, Iraq ",ACM International Conference Proceeding Series,AUD; International Association of Researchers (IARES Inc.),Association for Computing Machinery,,,2-Dec-19,"Proceedings of the 2nd International Conference on Data Science, E-Learning and Information Systems, DATA 2019",2019,,,,9.78145E+12,10.1145/3368691.3368723,a32,"2nd International Conference on Data Science, E-Learning and Information Systems, DATA 2019","December 2, 2019 - December 5, 2019",,"Preserving the historical monuments as archaeological heritage has cultural and financial significance. However, the fragility of these constructions does not help in intrusive human intervention to preserve them. For example, intrusive research or the use of mechanical or chemical activities could damage the archaeological site entirely or partially. Modern technologies have helped to modernize archaeological research, documentation and restoration by finding alternative nonintrusive precise methods for object identification, modelling and virtualisation without damaging them. Nowadays, Lidar (Light detection and ranging) is a significant technology for archaeological research that provides detailed data of the site for wide-ranging spatial locations. It can achieve images in three dimensions with the help of hardware established on ground or a drone or even an airplane. The precise 3D information (point cloud) provided by the Lidar are used later for virtual reconstruction or restoration in case of unwanted destructive invasive catastrophes. Moreover, the acquired point cloud can also be utilised to design an augmented reality (AR) and a (VR) virtual reality application to be used on various platforms. In this work, an experimental setup is suggested, using a Lidar device, to scan façades of an archaeological site, obtaining the point cloud and virtualising as 3D model so that it can be used in AR and VR applications. This setup is used for one of the houses in Erbil's Citadel which is classified as a world archaeological heritage by UNISCO. © 2019 Association for Computing Machinery.",28,Optical radar,3D modeling - Architecture - Augmented reality - E-learning - Information systems - Information use - Learning systems - Restoration - Surface reconstruction - Three dimensional computer graphics - Virtual reality,Augmented reality applications - Erbil-Qalat - Historical monuments - LIDAR (light detection and ranging) - Object identification - Point cloud - Rasheed Agha's Diwakhana - Virtual reconstruction,"402 Buildings and Towers - 716.2 Radar Systems and Equipment - 723 Computer Software, Data Handling and Applications - 903.3 Information Retrieval and Use",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Safe laboratory practices & Procedures introduced to the students through an augmented reality application,"Hanafi, Anasse (1); Elaachak, Lotfi (1); Bouhorma, Mohammed (1); El Khalil Bennis (1) ","(1) LIST Laboratory, UAE University, P.O. Box 416, Morocco ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,,2-Oct-19,"Proceedings of the 4th International Conference on Smart City Applications, SCA 2019",2019,,,,9.78145E+12,10.1145/3368756.3369042,a62,"4th International Conference on Smart City Applications, SCA 2019","October 2, 2019 - October 4, 2019",,"The adoption of digital technology in both learning and teaching process, and the evolution of information technologies are opening new perspectives for the use of augmented reality (AR) and virtual reality (VR). Thus, these technologies can facilitate the assistance of learners working on complex learning situation by using AR and enable the implementation of VR trainings, thus improving their efficiency. However, integrating these new tools into the existing learning processes remains complex, due to the technological aspects and the data continuum to be implemented, through the identification of use cases and the associated gains and by the diversity actors and experts to involve in this process: the expert of the main field, the designer and the IT developer. In this paper we aim to develop an AR application by following a developing process that will led to the planned results. The proposed application will be dictated for learners in higher education context, especially newbies that will practice their first experiment in the laboratory e.g.' biology, chemistry' by giving them an interactive environment to understand the safety procedure followed during experiments in laboratories. © 2019 Association for Computing Machinery.",32,Engineering education,Augmented reality - Biology - Smart city - Virtual reality,Augmented reality applications - Developing process - Digital technologies - Interactive Environments - Laboratory practices - Learning - Learning and teachings - Technological aspects,"461.9 Biology - 723 Computer Software, Data Handling and Applications - 901.2 Education",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Improving learning through cloud-based mobile technologies and virtual and augmented reality for australian higher education,"Sandu, Nitirajsingh (1); Gide, Ergun (1); Karim, Shakir (1) ","(1) School of Engineering and Technology, CQUniversity, Sydney, Australia ",ACM International Conference Proceeding Series,Tsinghua University in Taiwan; University of Central Queensland; University of New South Wales; University of Technology Sydney,Association for Computing Machinery,,p 1-5,28-Jun-19,"Proceedings of the 2019 International Conference on Mathematics, Science and Technology Teaching and Learning, ICMSTTL 2019",2019,,,,9.78145E+12,10.1145/3348400.3348413,,"2019 International Conference on Mathematics, Science and Technology Teaching and Learning, ICMSTTL 2019","June 28, 2019 - June 30, 2019",,"Latest technological innovations like Cloud-based mobile technologies and virtual and Augmented Reality (AR) have the ability to create great learning experiences. However, few researches exist regarding how AR can be utilized and integrated to complement learning. Currently, the existing research focuses on integrating AR in classes to aid students but few dwells on integrating AR in online learning platforms applications. The increased use of mobile devices has allowed the introduction of online learning management systems that allow people to access e-books, papers, and articles online. As technological inventions continue to emerge, there is a need to investigate and internalize how these new technologies can complement learning. In this research, we investigate how the integration of VR and AR and Cloud-based mobile technologies can improve learning in Australian Higher Education. We conduct a survey to determine how the integration of AR and artificial in the Moodle learning management system can impact the user experiences. We then discuss the current development in virtual and AR and Cloud-based mobile technologies in the education system through a literature review. Our preliminary analysis shows that integration of AR and artificial intelligence may lead to better learning experiences. © 2019 Association for Computing Machinery.",15,E-learning,Artificial intelligence - Augmented reality - Electronic document exchange - Engineering education - Integration - Learning systems - Mobile cloud computing - Online systems - Teaching - Telecommunication equipment - Virtual reality,Improving learning - Learning experiences - Learning management system - Literature reviews - Preliminary analysis - Technological innovation - Technological inventions - Virtual and augmented reality,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 901.2 Education - 921.2 Calculus",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Heromirror interactive: A gesture controlled augmented reality gaming experience,"Matuszka, Tamás (1); Czuczor, Ferenc (2); Sóstai, Zoltán (3) ","(1) Department of Research and Development INDE R and D (2) Department of Software Development, INDE R and D (3) Department of Content Development, INDE R and D ","ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3306214.3338554,,"ACM SIGGRAPH 2019 Posters - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"Appropriately chosen user interfaces are essential parts of immersive augmented reality experiences. Regular user interfaces cannot be efficiently used for interactive, real-time augmented reality applications. In this study, a gesture controlled educational gaming experience is described where gesture recognition relies on deep learning methods. Our implementation is able to replace a depth-camera based gesture recognition system using conventional camera while ensuring the same level of recognition accuracy. © 2019 Copyright held by the owner/author(s).",6,Gesture recognition,Augmented reality - Cameras - Computer vision - Deep learning - Interactive computer graphics - User interfaces,Augmented reality applications - Augmented reality gaming - Conventional camera - Educational gaming - Gesture recognition system - Human experience - Immersive augmented realities - Recognition accuracy,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 742.2 Photographic Equipment",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Deep-childar bot: Educational activities and safety care augmented reality system with deep-learning for preschool,"Park, Yoon Jung (1); Ro, Hyocheol (1); Han, Tack-Don (1) ","(1) Media System Lab, Yonsei University, Korea, Republic of ","ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Posters, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3306214.3338589,,"ACM SIGGRAPH 2019 Posters - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"We propose a projection-based augmented reality (AR) robot system that provides pervasive support for the education and safety of preschoolers via a deep learning framework. This system can utilize real-world objects as metaphors for educational tools by performing object detection based on deep learning in real-time, and it can help recognize the dangers of real-world objects that may pose risks to children. We designed the system in a simple and intuitive way to provide user-friendly interfaces and interactions for children. Children's experiences through the proposed system can improve their physical, cognitive, emotional, and thinking abilities. © 2019 Copyright held by the owner/author(s).",5,Deep learning,Augmented reality - Interactive computer graphics - Object detection - Object recognition,Augmented reality systems - Educational activities - Educational game - Educational tools - Learning frameworks - Real-world objects - Robot system - User friendly interface,"723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: NRF, Sponsor: National Research Foundation of Korea; Number: -, Acronym: MSIP, Sponsor: Ministry of Science, ICT and Future Planning; Number: -, Acronym: NRF, Sponsor: National Research Foundation of Korea; ",This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIP) (No.NRF-2018R1A2A1A05078628).,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Foveated AR: dynamically-foveated augmented reality display,"Jonghyun Kim; Youngmo Jeong; Stengel, M.; Akscedilit, K.; Albert, R.; Boudaoud, B.; Greer, T.; Joohwan Kim; Lopes, W.; Majercik, Z.; Shirley, P.; Spjut, J.; McGuire, M.; Luebke, D. ",,ACM Transactions on Graphics,,"ACM, USA","v 38, n 4",99 (15 pp.),Jul-19,,,,0730-0301,,,10.1145/3306346.3322987,,,,,"We present a near-eye augmented reality display with resolution and focal depth dynamically driven by gaze tracking. The display combines a traveling microdisplay relayed off a concave half-mirror magnifier for the high-resolution foveal region, with a wide field-of-view peripheral display using a projector-based Maxwellian-view display whose nodal point is translated to follow the viewer's pupil during eye movements using a traveling holographic optical element. The same optics relay an image of the eye to an infrared camera used for gaze tracking, which in turn drives the foveal display location and peripheral nodal point. Our display supports accommodation cues by varying the focal depth of the microdisplay in the foveal region, and by rendering simulated defocus on the 'always in focus' scanning laser projector used for peripheral display. The resulting family of displays significantly improves on the field-of-view, resolution, and form-factor tradeoff present in previous augmented reality designs. We show prototypes supporting 30, 40 and 60 cpd foveal resolution at a net 85deg times 78deg field of view per eye.",,,augmented reality - computer displays - eye - helmet mounted displays - holographic optical elements - image resolution - mirrors - optical images - optical projectors - visual perception,gaze tracking - traveling microdisplay - concave half-mirror magnifier - high-resolution foveal region - field-of-view peripheral display - projector-based Maxwellian-view display - eye movements - traveling holographic optical element - foveal display location - peripheral nodal point - focal depth - always in focus scanning laser projector - previous augmented reality designs - 30 cpd foveal resolution - 40 cpd foveal resolution - 60 cpd foveal resolution - dynamically-foveated augmented reality display - near-eye augmented reality display,"B6135 Optical, image and video signal processing - C5260B Computer vision and image processing techniques - C6130V Virtual reality - C5540D Computer displays",G02B5/08 - G02B5/32 - G02B27/18 - G03B21/00 - G06F3/14 - G06T - H04N5/74,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
"Design, Large-Scale Usage Testing, and Important Metrics for Augmented Reality Gaming Applications","Roberto, P. (1); Emanuele, F. (2); Primo, Z. (2); Adriano, M. (2); Jelena, L. (3); Marina, P. (2) ","(1) DICEA, Univ. Politec. delle Marche, Ancona, Italy (2) DII, Univ. Politec. delle Marche, Ancona, Italy (3) Dept. of Eng. Sci., Uppsala Univ., Uppsala, Sweden ","ACM Transactions on Multimedia Computing, Communications and Applications",,"ACM, USA","v 15, n 2",41 (18 pp.),Jun-19,,,,1551-6857,,,10.1145/3311748,,,,,"Augmented Reality (AR) offers the possibility to enrich the real world with digital mediated content, increasing in this way the quality of many everyday experiences. While in some research areas such as cultural heritage, tourism, or medicine there is a strong technological investment, AR for game purposes struggles to become a widespread commercial application. In this article, a novel framework for AR kid games is proposed, already developed by the authors for other AR applications such as Cultural Heritage and Arts. In particular, the framework includes different layers such as the development of a series of AR kid puzzle games in an intermediate structure which can be used as a standard for different applications development, the development of a smart configuration tool, together with general guidelines and long-life usage tests and metrics. The proposed application is designed for augmenting the puzzle experience, but can be easily extended to other AR gaming applications. Once the user has assembled the real puzzle, AR functionality within the mobile application can be unlocked, bringing to life puzzle characters, creating a seamless game that merges AR interactions with the puzzle reality. The main goals and benefits of this framework can be seen in the development of a novel set of AR tests and metrics in the pre-release phase (in order to help the commercial launch and developers), and in the release phase by introducing the measures for long-life app optimization, usage tests and hint on final users together with a measuretodesign policy, providing a method for automatic testing of quality and popularity improvements. Moreover, smart configuration tools, as part of the general framework, enabling multi-app and eventually also multi-user development, have been proposed, facilitating the serialization of the applications. Results were obtained from a large-scale user test with about 4 million users on a set of eight gaming applications, providing the scientific community a workflow for implicit quantitative analysis in AR gaming. Different data analytics developed on the data collected by the framework prove that the proposed approach is affordable and reliable for long-life testing and optimization.",,,augmented reality - computer games - data analysis - mobile computing,AR tests - pre-release phase - commercial launch - long-life app optimization - final users - design policy - automatic testing - augmented reality gaming applications - puzzle reality - seamless game - life puzzle - mobile application - AR gaming applications - puzzle experience - long-life usage tests - general guidelines - applications development - AR kid puzzle games - AR applications - widespread commercial application - game purposes - strong technological investment - cultural heritage - everyday experiences - digital mediated content - important metrics - large-scale usage testing - long-life testing - data analytics - large-scale user test - multiuser development - smart configuration tool,"C7830D Computer games - C6190V Mobile, ubiquitous and pervasive computing - C6130V Virtual reality",A63F13/00 - G06F9/44,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Distance-driven user interface for collaborative exhibit viewing in augmented reality museum,"Li, Xiangdong (1); Chen, Wenqian (1); Wu, Yue (1) ","(1) Dept. of Design, Zhejiang University, Hangzhou, China ",UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,ACM SIGCHI; ACM SIGGRAPH,"Association for Computing Machinery, Inc",,p 42-43,14-Oct-19,UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,2019,,,,9.78145E+12,10.1145/3332167.3357109,,"32nd Annual ACM Symposium on User Interface Software and Technology, UIST 2019","October 20, 2019 - October 23, 2019",,"The rapid advancement of augmented reality technology brings museum visitors enhanced interaction and immersion experience. However, most existing augmented reality museums adopt individual-based user interfaces that hinder joint interaction across multiple users. We present the distance-driven user interface (DUI) to enable collaborative exhibit viewing in augmented reality museum. We classify the users in four groups according to the user-exhibit distance and assign each group specific interaction privileges - put simply, the DUI elicits the users standing far from an exhibit to explore more of the desired exhibit by approaching closer to the exhibit. We describe the DUI architecture and preliminarily evaluate users' acceptance and effectiveness of the DUI and find that, the DUI is interpretable, improves users' awareness of collaboration, and increases user interests to the exhibit with considerably improved willingness of approaching the exhibit. © 2019 Copyright is held by the owner/author(s).",,User interfaces,Augmented reality - Museums,Augmented reality technology - Awareness of collaboration - Collaborative interaction - Exhibit viewing - Individual-based - Museum visitor - Specific interaction - Users' acceptance,"402.2 Public Buildings - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: 2016YFB1001304, Acronym: -, Sponsor: -; Number: 61802341, Acronym: -, Sponsor: -; ",This work is supported by the National Key R&D Program project (2016YFB1001304) and National Nature Science Foundation project (61802341).,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Walking with adaptive augmented reality workspaces: Design and usage patterns,"Lages, Wallace S. (1); Bowman, Doug A. (1) ","(1) Center for Human-Computer Interaction, Blacksburg, United States ","International Conference on Intelligent User Interfaces, Proceedings IUI",ACM Special Interest Group on Artificial Intelligence (SIGAI); ACM Special Interest Group on Computer-Human Interaction (SIGCHI),Association for Computing Machinery,v Part F147615,p 356-366,2019,,2019,,,,,10.1145/3301275.3302278,,"24th ACM International Conference on Intelligent User Interfaces, IUI 2019","March 17, 2019 - March 20, 2019",,"Mobile augmented reality may eventually replace our smartphones as the primary way of accessing information on the go. However, current interfaces provide little support to walking and to the variety of actions we perform in the real world. To achieve its full potential, augmented reality interfaces must support the fluid way we move and interact in the physical world. We explored how different adaptation strategies can contribute towards this goal. We evaluated design alternatives through contextual studies and identified the key interaction patterns that interfaces for walking should support. We also identified desirable properties of adaptation-based interface techniques, which can be used to guide the design of the next-generation walking-centered augmented reality workspaces. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",32,User interfaces,Augmented reality,Adaptation strategies - Adaptive augmented realities - Adaptive interface - Design alternatives - Interaction pattern - Interface techniques - Mobile augmented reality - Wearable,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,"Number: -, Acronym: CNPq, Sponsor: Conselho Nacional de Desenvolvimento Cient&iacute;fico e Tecnol&oacute;gico; Number: -, Acronym: ONR, Sponsor: Office of Naval Research; ",The authors gratefully acknowledge funding support from the Im-mersive Sciences program in the Office of Naval Research and from the Brazilian National Council for Scientific and Technological Development. The authors also are thankful for the suggestions from the anonymous reviewers.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
An augmented reality application for mobile visualization of GIS-referenced landscape planning projects,"Kilimann, Jan-Erik (1); Heitkamp, Denis (1); Lensing, Philipp (1) ","(1) Osnabrück University of Applied Sciences, Osnabrück, Germany ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365712,a23,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"We introduce an augmented reality application that allows the representation of planned real world objects (e.g. wind turbines or power poles) at their actual geographic location. The application uses GPS for positioning, which is then supplemented by augmented reality feature tracking to get a constant and stable positional and rotational reading. As addition to the visualization, we use on-the-fly gathered sensor data to identify foreground objects. Thus, for practical scenarios our application depicts images with mostly correct occlusion between real and virtual objects. The application will be used to support urban and landscape planners in their process, especially for the purpose of public information and acceptance. It provides an advantage to current planning processes, where the representation of objects is limited to positions on maps, miniature models, or at best a photo montage where the object is placed into a still camera image. © 2019 Association for Computing Machinery.",13,Geographic information systems,Augmented reality - Data visualization - Flow visualization - Interactive computer graphics - Virtual reality - Visualization,Augmented reality applications - Foreground objects - Geographic location - Landscape planning - Mobile visualization - Planning process - Public information - Real-world objects,"631.1 Fluid Flow, General - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 903.3 Information Retrieval and Use",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Development of an augmented reality tour guide for a cultural heritage site,Seungbum Koo (1); Jinyoung Kim (2); Changhyuk Kim (3); Jeeyeop Kim (4); Hee Sung Cha (2) ,"(1) Eng. Res. Inst., Ajou Univ., Suwon, Korea, Republic of (2) Dept. of Archit. Eng., Ajou Univ., Suwon, Korea, Republic of (3) Korea Inst. of Civil Eng. & Building Technol., Goyang, Korea, Republic of (4) Dept. of Archit., Sungkyunkwan Univ., Suwon, Korea, Republic of ",Journal on Computing and Cultural Heritage,,"ACM, USA","v 12, n 4",24 (24 pp.),Dec. 2019,,,,1556-4673,,,10.1145/3317552,,,,,"In this article, the design, development, and evaluation of augmented reality (AR)-based mobile application for a tour guide are discussed. The objectives of this article are twofold. First, the research focuses on the development of a complete working set of a mobile tour application furnished with AR. For such an application to be successfully adopted by the general public, user requirements and application usability are investigated, and the application is designed and implemented to fulfill those findings. Second, the developed application is demonstrated by applying it to a UNESCO designated World Heritage site, Hwaseong Fortress in Suwon, South Korea, and evaluated via a survey instrument developed explicitly for mobile application evaluation. A systematically developed survey instrument from the fields of tourism, information systems, and human-computer interaction is tailored to fit into this research and employed for the application evaluation. The application's operation flow consists of three main functions: navigation to the points of interest, visualization of information with AR technology, and interactive learning activities with AR-based serious games. Efforts are made to provide a more immersive and interactive experience of the historical, cultural, and architectural details of the heritage site utilizing novel AR visualization methods. The evaluation returned positive results with suggestions of possible refinements for future works. The proposed device-aided tour mechanism is anticipated to enhance tourists' experiences as well as being important guidance in future mobile tourism application development as to how the application should be designed and implemented to be accepted by the general public.",,,augmented reality - interactive systems - learning (artificial intelligence) - mobile computing - travel industry - user interfaces,mobile application evaluation - systematically developed survey instrument - human-computer interaction - interactive learning activities - AR-based serious games - immersive experience - interactive experience - historical details - architectural details - device-aided tour mechanism - augmented reality tour guide - cultural heritage site - augmented reality-based mobile application - complete working set - mobile tour application - user requirements - application usability - world heritage site - mobile tourism application development - mobile tourism application development - AR visualization methods,"C7185 Administration of other service industries - C6190V Mobile, ubiquitous and pervasive computing - C6130V Virtual reality - C6170K Knowledge engineering techniques - C6180 User interfaces",G06F9/44 - G06N5/04 - G06Q50/10 - G06N20/00,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Rendering multi-party mobile augmented reality from edge,"Zhang, Lei (1, 2); Sun, Andy (2); Shea, Ryan (2); Liu, Jiangchuan (2); Zhang, Miao (2) ","(1) College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China (2) School of Computing Science, Simon Fraser University, Burnaby; BC, Canada ","Proceedings of the 29th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, NOSSDAV 2019",ACM SIGMM,"Association for Computing Machinery, Inc",,p 67-72,21-Jun-19,"Proceedings of the 29th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, NOSSDAV 2019",2019,,,,9.78145E+12,10.1145/3304112.3325612,,"29th ACM SIGMM Workshop on Network and Operating Systems Support for Digital Audio and Video, NOSSDAV 2019",21-Jun-19,,"Mobile augmented reality (MAR) augments a real-world environment (probably surrounding or close to the mobile user) by computergenerated perceptual information. Utilizing the emerging edge computing paradigm in MAR systems can reduce the power consumption and computation load for the mobile devices and improve responsiveness of the MAR service. Dierent from existing studies that mainly explored how to better enable the MAR services utilizing edge computing resources, our focus is to optimize the video generation stage of the edge-based MAR services-effciently using the available edge computing resources to render and encode the augmented reality as video streams to the mobile clients. Specically, for multi-party AR applications, we identify the advantages and disadvantages of two encoding schemes, namely colocated encoding and spilt encoding, and examine the trade-off between performance and scalability when the rendering and encoding tasks are colocated or split. Towards optimally placing AR video rendering and encoding in the edge, we formulate and solve the rendering and encoding task assignment problem for multi-party edge-based MAR services to maximize the QoS for the users and the edge computing eciency. The proposed task assignment scheme is proved to be superior through extensive trace-driven simulations and experiments on our prototype system. © 2019 Association for Computing Machinery.",12,Rendering (computer graphics),Audio systems - Augmented reality - Combinatorial optimization - Economic and social effects - Edge computing - Encoding (symbols) - Green computing - Mobile telecommunication systems - Signal encoding,Computing paradigm - Computing resource - Mobile - Mobile augmented reality - Perceptual information - Performance and scalabilities - Real world environments - Trace driven simulation,"716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing - 921.4 Combinatorial Mathematics, Includes Graph Theory, Set Theory - 971 Social Sciences",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A location-based social network system integrating mobile augmented reality and user generated content,"Yue, Yuanwen (1); Ding, Jiaqi (1); Kang, Yuhao (2); Wang, Yueyao (3); Wu, Kunlin (1); Fei, Teng (1) ","(1) Wuhan University, Hubei, China (2) Geospatial Data Science Lab., University of Wisconsin, Madison, United States (3) Peking University, Beijing, China ","LocalRec 2019 - Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising",DiDi; esri; et al.; here; lyft; Microsoft,"Association for Computing Machinery, Inc",,,5-Nov-19,"LocalRec 2019 - Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising",2019,,,,9.78145E+12,10.1145/3356994.3365507,3365507,"3rd ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising, LocalRec 2019, held in conjunction with the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM SIGSPATIAL 2019",5-Nov-19,,"Location-based social networks (LBSNs) enable individuals to connect tighter through users’ interdependency (e.g. friendship, common interests and shared knowledge) which derived from their physical locations and geo-tagged social media content. Nowadays, the development of mobile augmented reality (MAR) technology can enhance people’s perception and interaction with the world, which enrich the way people share their geo-referenced information. In this paper, we propose a LBSN smartphone system by integrating MAR and user generated content (UGC). Users can publish their own content through an augmented reality (AR) form on a 3D model which is associated with real-world coordinates, and interact with content published by others. A prototype system is implemented on the iOS platform to prove the efficiency of the framework. © 2019 Association for Computing Machinery.",18,Location based services,3D modeling - Augmented reality - Location - Social networking (online) - Telecommunication services,Common interests - Location-based social network systems - Location-based social networks - Mobile augmented reality - Physical locations - Prototype system - User generated content (UGC) - User-generated content,"716 Telecommunication; Radar, Radio and Television - 723 Computer Software, Data Handling and Applications",,,"Number: 201810486121, Acronym: -, Sponsor: National College Students Innovation and Entrepreneurship Training Program; ",This work was supported by the National Student Innovation and Entrepreneurship Training Program (No.201810486121).,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Sur.faced.io: Augmented reality content creation for your face and beyond by drawing on paper,"Chang, Yosun (1) ","(1) AReality3D Permute.xyz, San Francisco; CA, United States ","ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH),"Association for Computing Machinery, Inc",,,28-Jul-19,"ACM SIGGRAPH 2019 Appy Hour, SIGGRAPH 2019",2019,,,,9.78145E+12,10.1145/3305365.3329730,a6,"ACM SIGGRAPH 2019 Appy Hour - International Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2019",28-Jul-19,,"We summarize several methods we have used to create software and processes for automated methods for content creation for augmented reality, virtual reality, and other 3D medium uses and beyond. We utilize processes involving, machine learning semantic segmentation, computer vision geometry recognition for automated texture mapping, photogrammetry 3D reconstruction from 2D images and videogrammetry video content, and more. A practical use in industry is an emphasis for each software example, and many are associated with awards. © 2019 Copyright Held by the Owner/Author(s).",8,Interactive computer graphics,Augmented reality - Image segmentation - Semantics - Textures - Three dimensional computer graphics - Virtual reality,3D reconstruction - Augmented reality content - Automated methods - Content creation - Content generation - Texture mapping - Video contents - Videogrammetry,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
H space: Interactive augmented reality art,"Edmonds, Ernest (1); Hills, Damian (2); Ji, Yi (3) ","(1) IOCT, Leicester Media School, De Montfort University, Leicester, United Kingdom (2) Queensland College of Art, Griffith University, Brisbane, Australia (3) School of Art and Design, Guangdong University of Technology, Guangzou, China ","TEI 2020 - Proceedings of the 14th International Conference on Tangible, Embedded, and Embodied Interaction",ACM SIGCHI,"Association for Computing Machinery, Inc",,p 683-688,6-Feb-20,"TEI 2020 - Proceedings of the 14th International Conference on Tangible, Embedded, and Embodied Interaction",2020,,,,9.78145E+12,10.1145/3374920.3375291,,"14th International Conference on Tangible, Embedded, and Embodied Interaction, TEI 2020","February 9, 2020 - February 12, 2020",,"This artwork exploits recent research into augmented reality systems, such as the HoloLens, for building creative interaction in augmented reality. The work is being conducted in the context of interactive art experiences. The first version of the audience experience of the artwork, 'H Space', was informally tested in the SIGGRAPH 2018 Art Gallery context. Experiences with a later, improved, version was evaluated at Tsinghua University. The latest distributed version will be shown in Sydney. The paper describes the concept, the background in both the art and the technological domain and points to some of the key computer human interaction art research issues that the work highlights. © 2020 Association for Computing Machinery.",9,Arts computing,Augmented reality - Human computer interaction - Interactive computer graphics,Audience experience - Augmented reality arts - Augmented reality systems - Computer Human Interaction - HoloLens - Interactive arts - Recent researches - Tsinghua University,"723 Computer Software, Data Handling and Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
HypAR: Situated mineralogy exploration in augmented reality,"Engelke, Ulrich (1); Rogers, Casey (2); Klump, Jens (3); Lau, Ian (3) ","(1) CSIRO Data61, Kensington, Australia (2) Curtin University, Bentley, Australia (3) CSIRO Mineral Resources, Kensington, Australia ",Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,ACM SIGGRAPH,"Association for Computing Machinery, Inc",,,14-Nov-19,Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,2019,,,,9.78145E+12,10.1145/3359997.3365715,a26,"17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, VRCAI 2019","November 14, 2019 - November 16, 2019",,"Hyperspectral imaging, as a fast and cost effective method of mapping the composition of geological materials in context, is a key enabler for scientific discoveries in the geosciences. Being able to do this in-situ in real world context, possibly in real time, would have profound implications for geology and minerals exploration. This work addresses this important issue by developing an augmented reality application called HypAR that enables in-situ, interactive exploration of mineralogy spatially co-located and embedded with rock surfaces. User centred design is deployed to assure the utility and validity of the system. We describe the requirements analysis and design process for HypAR.We present a prototype using the Microsoft HoloLens that was implemented for a rock wall containing a wide range of minerals and materials from significant geological localities of Western Australia. We briefly discuss several use cases for which HypAR and extensions thereof may prove useful to geoscientists and other end users who have to make effective, informed decisions about the mineralogy of rock surfaces. © 2019 Association for Computing Machinery.",21,Hyperspectral imaging,Augmented reality - Cost effectiveness - Geology - Interactive computer graphics - Mineral exploration - Mineralogy - Minerals - Spectroscopy - User centered design - Virtual reality,Augmented reality applications - Cost-effective methods - Geosciences - Interaction design - Interactive exploration - Requirements analysis - Scientific discovery - Visual analytics,"481.1 Geology - 482 Mineralogy - 482.2 Minerals - 501.1 Exploration and Prospecting Methods - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 911.2 Industrial Economics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Is the pen mightier than the controller? A comparison of input devices for selection in virtual and augmented reality,"Pham, Duc-Minh (1); Stuerzlinger, Wolfgang (1) ","(1) Simon Fraser University, Surrey; BC, Canada ","Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,12-Nov-19,Proceedings - VRST 2019: 25th ACM Symposium on Virtual Reality Software and Technology,2019,,,,9.78145E+12,10.1145/3359996.3364264,3364264,"25th ACM Symposium on Virtual Reality Software and Technology, VRST 2019","November 12, 2019 - November 15, 2019",,"Controllers are currently the typical input device for commercial Virtual Reality (VR) systems. Yet, such controllers are not as efficient as other devices, including the mouse. This motivates us to investigate devices that substantially exceed the controller's performance, for both VR and Augmented Reality (AR) systems. We performed a user study to compare several input devices, including a mouse, controller, and a 3D pen-like device on a VR and AR pointing task. Our results show that the 3D pen significantly outperforms modern VR controllers in all evaluated measures and that it is comparable to the mouse. Participants also liked the 3D pen more than the controller. Finally, we show how 3D pen devices could be integrated into today's VR and AR systems. © 2019 Association for Computing Machinery.",38,Controllers,Augmented reality - Knobs - Mammals - Virtual reality,AR system - Augmented reality systems - Input devices - Pointing tasks - User study - Virtual and augmented reality,"723 Computer Software, Data Handling and Applications - 732.1 Control Equipment",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Visual-inertial state estimation with pre-integration correction for robust mobile augmented reality,"Yuan, Zikang (1); Zhu, Dongfu (1); Chi, Cheng (1); Tang, Jinhui (2); Liao, Chunyuan (3); Yang, Xin (1) ","(1) Huazhong University of Science and Technology, Wuhan, China (2) Nanjing University of Science and Technology, Nanjing, China (3) HiScene Information Technology Co., Ltd ",MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,ACM SIGMM,"Association for Computing Machinery, Inc",,p 1410-1418,15-Oct-19,MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia,2019,,,,9.78145E+12,10.1145/3343031.3351079,,"27th ACM International Conference on Multimedia, MM 2019","October 21, 2019 - October 25, 2019",,"Mobile devices equipped with a monocular camera and an inertial measurement unit (IMU) are ideal platforms for augmented reality (AR) applications. However, nontrivial noises in low-cost IMUs, which are usually equipped in consumer-level mobile devices, could lead to large errors in pose estimation and in turn significantly degrade the user experience in mobile AR apps. In this study, we propose a novel monocular visual-inertial state estimation approach for robust and accurate pose estimation even for low-cost IMUs. The core of our method is an IMU pre-integration correction approach which effectively reduces the negative impact of IMU noises using the visual constraints in a sliding window and the kinematic constraint. We seamlessly integrate the IMU pre-integration correction module into a tightly-coupled, sliding-window based optimization framework for state estimation. Experimental results on public dataset EUROC demonstrate the superiority of our method to the state-of-the-art VINS-Mono in terms of smaller absolute trajectory errors (ATE) and relative pose errors (RPE). We further apply our method to real AR applications on two types of consumer-level mobile devices equipped with low-cost IMUs, i.e. an off-the-shelf smartphone and an AR glass. Experimental results demonstrate that our method can facilitate robust AR with little drifts on the two devices1 © 2019 Association for Computing Machinery.",26,Cost estimating,Augmented reality - Errors - Integration - State estimation,Estimation approaches - Graph optimization - Inertial measurement unit - Kinematic constraints - Mobile augmented reality - Optimization framework - Pre-integration - Sliding window-based,"723 Computer Software, Data Handling and Applications - 731.1 Control Systems - 911 Cost and Value Engineering; Industrial Economics - 921.2 Calculus",,,"Number: 2017010201010111, Acronym: -, Sponsor: Wuhan Municipal Science and Technology Bureau; Number: 61502188, Acronym: NSFC, Sponsor: National Natural Science Foundation of China; Number: ZRMS2017000375, Acronym: -, Sponsor: Natural Science Foundation of Hubei Province; Number: 2019kfyRCPY118, Acronym: -, Sponsor: Fundamental Research Funds for the Central Universities; ","This work was supported by the National Natural Science Foundation of China (61872417, 61502188), the Wuhan Science and Technology Bureau under Award (2017010201010111), the Hubei Provincial Natural Science Foundation (ZRMS2017000375) and the Fundamental Research Funds for the Central Universities (2019kfyRCPY118).",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
EyeMRTk: A toolkit for developing eye gaze interactive applications in virtual and augmented reality,"Mardanbegi, Diako (1); Pfeiffer, Thies (2) ","(1) Lancaster University, United Kingdom (2) CITEC, Bielefeld University, Germany ",Eye Tracking Research and Applications Symposium (ETRA),ACM SIGCHI; ACM SIGGRAPH,Association for Computing Machinery,,,25-Jun-19,Proceedings - ETRA 2019: 2019 ACM Symposium On Eye Tracking Research and Applications,2019,,,,9.78145E+12,10.1145/3317956.3318155,a76,"11th ACM Symposium on Eye Tracking Research and Applications, ETRA 2019","June 25, 2019 - June 28, 2019",,"For head mounted displays, like they are used in mixed reality applications, eye gaze seems to be a natural interaction modality. EyeMRTK provides building blocks for eye gaze interaction in virtual and augmented reality. Based on a hardware abstraction layer, it allows interaction researchers and developers to focus on their interaction concepts, while enabling them to evaluate their ideas on all supported systems. In addition to that, the toolkit provides a simulation layer for debugging purposes, which speeds up prototyping during development on the desktop. © 2019 Association for Computing Machinery.",33,Eye tracking,Abstracting - Augmented reality - Helmet mounted displays - Mixed reality - Virtual reality,Eye gaze interactions - Gaze interaction - Hardware Abstraction Layers - Head mounted displays - Interaction concepts - Interactive applications - Unity - Virtual and augmented reality,"723 Computer Software, Data Handling and Applications - 903.1 Information Sources and Analysis",,,"Number: -, Acronym: DFG, Sponsor: Deutsche Forschungsgemeinschaft; Number: -, Acronym: -, Sponsor: Universit&Atilde;&curren;t Bielefeld; Number: -, Acronym: EPSRC, Sponsor: Engineering and Physical Sciences Research Council; Number: -, Acronym: DFG, Sponsor: Deutsche Forschungsgemeinschaft; Number: EXC 277, Acronym: -, Sponsor: -; ","This work was partly supported by the Cluster of Excellence Cognitive Interaction Technology &lsquo;CITEC&rsquo; (EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG). Diako is supported by the funding by the EPSRC project MODEM Grant No. EP/M006255/1.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality for early Alzheimer's disease diagnosis,"Vovk, Alla (1); Chan, Dennis (2); Patel, Ameera (3) ","(1) Microsoft Research Oxford Brookes University, Cambridge, United Kingdom (2) University of Cambridge, Cambridge, United Kingdom (3) Microsoft Research Cambridge, United Kingdom ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3313007,3313007,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"Alzheimer's disease (AD), the most common type of dementia, is characterised by gradual memory loss. There is an increasing global research effort into strategies for early clinic-based diagnosis at the stage where patients present with mild memory problems. Initiating treatment at this stage would slow the progression of the condition and enable more years of good quality life. This paper presents the ongoing development of an augmented reality system using HoloLens that is designed to test an early onset of Alzheimer's disease. The most important aspects in the early AD diagnostics are the symptoms that appear to be connected with early memory loss, in particular spatial memory. The ability to store and retrieve the memory of a particular event involving an association between items such as the place and the object properties is incorporated in a game environment. © 2019 Copyright held by the owner/author(s).",15,Neurodegenerative diseases,Augmented reality - Brain - Data storage equipment - Diagnosis - Human engineering - Mixed reality,Alzheimer - Alzheimer's disease - Augmented reality systems - Game environment - HoloLens - Memory problems - Object property - Research efforts,"461 Bioengineering and Biology - 722.1 Data Storage, Equipment and Techniques - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Exploring uses of augmented reality in participatory marketing,"Conway, Niamh (1); Soro, Alessandro (1); Brown, Ross (1); Turkay, Selen (1) ","(1) Queensland University of Technology (QUT) Brisbane, QLD, Australia ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3313086,3313086,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"This paper is an exploration of Augmented Reality applications to Participatory Marketing and overviews initial findings in this area of research. Participatory Marketing is the concept of marketing with customers rather than at them, and can potentially turn AR users form passive consumers to (pro-)active co-creators of this future media. We conducted a preliminary investigation to focus on possible challenges and opportunities. © 2019 Copyright held by the owner/author(s).",16,Commerce,Augmented reality - Human engineering - Marketing,Augmented reality applications - Marketing IS,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 911.4 Marketing",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Opportunities for in-home augmented reality guidance,"Herskovitz, Jaylin (1); Lasecki, Walter S. (1); Ofek, Eyal (2); Fourney, Adam (2) ","(1) University of Michigan, Ann Arbor; MI, United States (2) Microsof Research Redmond, WA, United States ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI EA 2019 - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290607.3312933,3312933,"2019 CHI Conference on Human Factors in Computing Systems, CHI EA 2019","May 4, 2019 - May 9, 2019",,"The use of Augmented Reality (AR) systems has been shown to be beneficial in guiding users through structured tasks when compared to traditional 2D instructions. In this work, we begin to examine the potential of such systems for home improvement tasks, which present some specific challenges (e.g., operating at both large and small scales, and coping with the diversity in home environments). Specifically, we investigate user performance of a common low-level task in this domain. We conducted a user study where participants mark points on a planar surface (as if to place a nail, or measure from) guided only by virtual cues. We observed that participants position themselves so as to minimize parallax by kneeling, leaning, or side-stepping, and when doing so, they are able to mark points with a high degree of accuracy. In cases where the targets fall above one's line of vision, participants are less able to compensate and make larger errors. We discuss initial insights from these observations and participant feedback, and present the first set of challenges that we believe designers and developers will face in this domain. © 2019 Copyright held by the owner/author(s).",14,Computer aided instruction,Augmented reality - Geometrical optics - Human engineering,Augmented reality systems - High degree of accuracy - Home environment - Participant feedback - Planar surface - Small scale - User performance - User study,"461.4 Ergonomics and Human Factors Engineering - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 741.1 Light/Optics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Edge assisted real-time object detection for mobile augmented reality,"Liu, Luyang (1); Li, Hongyu (1); Gruteser, Marco (1) ","(1) WINLAB, Rutgers University, North Brunswick; NJ, United States ","Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM",ACM SIGMobile,Association for Computing Machinery,,,7-Aug-19,MobiCom 2019 - Proceedings of the 25th Annual International Conference on Mobile Computing and Networking,2019,,,,9.78145E+12,10.1145/3300061.3300116,,"25th Annual International Conference on Mobile Computing and Networking, MobiCom 2019","October 21, 2019 - October 25, 2019",,"Most existing Augmented Reality (AR) and Mixed Reality (MR) systems are able to understand the 3D geometry of the surroundings but lack the ability to detect and classify complex objects in the real world. Such capabilities can be enabled with deep Convolutional Neural Networks (CNN), but it remains difficult to execute large networks on mobile devices. Offloading object detection to the edge or cloud is also very challenging due to the stringent requirements on high detection accuracy and low end-to-end latency. The long latency of existing offloading techniques can significantly reduce the detection accuracy due to changes in the user's view. To address the problem, we design a system that enables high accuracy object detection for commodity AR/MR system running at 60fps. The system employs low latency offloading techniques, decouples the rendering pipeline from the offloading pipeline, and uses a fast object tracking method to maintain detection accuracy. The result shows that the system can improve the detection accuracy by 20.2%-34.8% for the object detection and human keypoint detection tasks, and only requires 2.24ms latency for object tracking on the AR device. Thus, the system leaves more time and computational resources to render virtual elements for the next frame and enables higher quality AR/MR experiences. © 2019 Association for Computing Machinery.",64,Object detection,Augmented reality - Convolution - Deep neural networks - Edge computing - Mixed reality - Mobile computing - Neural networks - Object recognition - Pipelines - Rendering (computer graphics) - Tracking (position),Adaptive video streaming - Computational resources - Convolutional neural network - End to end latencies - Mobile augmented reality - Real time - Rendering pipelines - Stringent requirement,"619.1 Pipe, Piping and Pipelines - 716.1 Information Theory and Signal Processing - 723 Computer Software, Data Handling and Applications - 723.2 Data Processing and Image Processing",,,"Number: 1329939, Acronym: NSF, Sponsor: National Science Foundation; Number: 1827923, Acronym: -, Sponsor: -; ",We sincerely thank our shepherd Karthik Dantu and anonymous reviewers for their valuable comments. This material is based in part upon work supported by the National Science Foundation under Grant Nos. 1329939 and PAWR/COSMOS Grant Nos. 1827923.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
'What’s happening at that hip?': Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom,"Ferdous, Hasan Shahid (1); Hoang, Thuong (2); Joukhadar, Zaher (1); Reinoso, Martin N. (1); Vetere, Frank (1); Kelly, David (3); Remedios, Louisa (3) ","(1) School of Computing and Information Systems, University of Melbourne, Australia (2) School of Information Technology, Deakin University, Australia (3) Department of Physiotherapy, University of Melbourne, Melbourne, Australia ",Conference on Human Factors in Computing Systems - Proceedings,ACM SIGCHI,Association for Computing Machinery,,,2-May-19,CHI 2019 - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,,,,9.78145E+12,10.1145/3290605.3300464,,"2019 CHI Conference on Human Factors in Computing Systems, CHI 2019","May 4, 2019 - May 9, 2019",,"We present two studies to discuss the design, usability analysis, and educational outcome resulting from our system Augmented Body in physiotherapy classroom. We build on prior user-centric design work that investigates existing teaching methods and discuss opportunities for intervention. We present the design and implementation of a hybrid system for physiotherapy education combining an on-body projection based virtual anatomy supplemented by pen-based tablets to create real-time annotations. We conducted a usability evaluation of this system, comparing with projection only and traditional teaching conditions. Finally, we focus on a comparative study to evaluate learning outcome among students in actual classroom settings. Our studies showed increased usage of visual representation techniques in students’ note taking behavior and statistically significant improvement in some learning aspects. We discuss challenges for designing augmented reality systems for education, including minimizing attention split, addressing text-entry issues, and digital annotations on a moving physical body. © 2019 Copyright held by the owner/author(s).",25,Students,Augmented reality - Human engineering - Hybrid systems - Physical therapy - User centered design,Annotation - Augmented reality systems - Design and implementations - Educational systems - Pen-based interaction - Usability evaluation - User-centric designs - Visual representations,"461.4 Ergonomics and Human Factors Engineering - 461.5 Rehabilitation Engineering and Assistive Technology - 723 Computer Software, Data Handling and Applications - 921 Mathematics",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Poster: A testbed implementation of NDN-based edge computing for mobile augmented reality,"Ullah, Rehmat (1); Rehman, Muhammad Atif Ur (1); Kim, Byung Seo (1) ","(1) Hongik University, Sejong, Korea, Republic of ",HotMobile 2019 - Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications,ACM SIGMOBILE,"Association for Computing Machinery, Inc",,p 181,22-Feb-19,HotMobile 2019 - Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications,2019,,,,9.78145E+12,10.1145/3301293.3309565,3309565,"20th International Workshop on Mobile Computing Systems and Applications, HotMobile 2019","February 27, 2019 - February 28, 2019",,"Future Augmented Reality (AR) applications require fast information response time and significant computational power and memory for many of its tasks. To enable future AR applications, in this poster, we combine Named Data Networking (NDN) and Edge Computing (EC) in order to achieve fast information response time. The outcomes are implemented and evaluated through testbed and simulations. © 2019 Authors.",1,Edge computing,Augmented reality - Mobile computing - Response time (computer systems) - Testbeds,AR application - Computational power - Fast response time - Future internet - Information response - Mobile augmented reality - Named data networkings,"723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,This research was supported by the International Research & Development Program of the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT. (No. NRF-2018K1A3A1A39086819),,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
"A case for a richer, Bi-directional interface between augmented reality applications and the network","Ahsen, Tooba (1); Dogar, Fahad (1) ","(1) Tufts University, United States ","International Conference on Intelligent User Interfaces, Proceedings IUI",ACM Special Interest Group on Artificial Intelligence (SIGAI); ACM Special Interest Group on Computer-Human Interaction (SIGCHI),Association for Computing Machinery,,p 59-60,16-Mar-19,"Proceedings of the 24th International Conference on Intelligent User Interfaces, IUI 2019",2019,,,,9.78145E+12,10.1145/3308557.3308679,,"24th International Conference on Intelligent User Interfaces, IUI 2019","March 16, 2019 - March 20, 2019",,"Network impairments (e.g., latency, outages, etc) can have an adverse impact on user experience, especially for interactive applications (e.g., augmented reality (AR)). To effectively deal with such problems, we propose a bi-directional interface between the applications and the network, which facilitates intelligent decision making by both the application and the network. © 2019 ACM.",9,User interfaces,Augmented reality - Decision making,Augmented reality applications - Bi-directional - Collaboration - Intelligent decision making - Interactive applications - Network impairments - User experience,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 912.2 Management",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Adaptive projection augmented reality with object recognition based on deep learning,"Park, Yoon Jung (1); Ro, Hyocheol (1); Byun, Jung-Hyun (1); Han, Tack-Don (1) ","(1) Yonsei University, Seoul, Korea, Republic of ","International Conference on Intelligent User Interfaces, Proceedings IUI",ACM Special Interest Group on Artificial Intelligence (SIGAI); ACM Special Interest Group on Computer-Human Interaction (SIGCHI),Association for Computing Machinery,,p 51-52,16-Mar-19,"Proceedings of the 24th International Conference on Intelligent User Interfaces, IUI 2019",2019,,,,9.78145E+12,10.1145/3308557.3308678,,"24th International Conference on Intelligent User Interfaces, IUI 2019","March 16, 2019 - March 20, 2019",,"This study describes an Adaptive Projection Augmented Reality (AR) system that can provide information in real-time using object recognition. This approach is based on deep-learning through the construction of a 3D space from the real world. Through single system, a projector-camera unit with pan-tilt mechanism, the 360-degree space of user surroundings can be built into a 3-dimensional space and accomplish object and user recognition. a projection AR environment can be generated instantaneously. Information relevant to real-world objects can be provided through real-time interactions between the user and objects. Using spatial interaction, it also allows for achieving intuitive interactions with projection information, user interface (UI), and contents without touch sensors. Several scenarios for the use of this system are described below. © 2019 ACM.",6,Deep learning,Augmented reality - Object recognition - User interfaces,3-dimensional spaces - Augmented reality systems - Intuitive interaction - Projector-camera - Real time interactions - Real-world objects - Spatial interaction - User interaction,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Augmented reality based on stem for supporting science literacy in vocational education,"Agustina, W.W. (1); Sumarto, S. (1); Trisno, B. (1) ","(1) Pendidikan Teknol. dan Kejuruan, Univ. Pendidikan Indonesia, Bandung, Indonesia ",Journal of Physics: Conference Series,,"IOP Publishing, UK",v 1375,012088 (9 pp.),2019,,,,1742-6596,,,10.1088/1742-6596/1375/1/012088,,Annual Conference of Science and Technology,30 Aug. 2018,"Malang, Indonesia","AR development is becoming very popular in various fields of science. Some researchers have demonstrated that AR has the potential in science, technology, engineering, and mathematics (STEM). Puse of total AR integrated STEM needs to be studied more deeply, how STEM-AR characteristics, and how the potential of AR-based STEM to support the scientific literacy of students. Searches through relevant sources, the online database of which seven ACM Digital Library, ERIC, ieeexplore, ISI Web of Science, ScienceDirect, Scopus, and Springer. The results showed that the use of STEM-AR potentially an effort to improve science literacy.",43,,augmented reality - computer aided instruction - digital libraries - natural sciences computing,science literacy - augmented reality - vocational education - STEM-AR characteristics - AR-based STEM - seven ACM Digital Library - STEM-AR potentially an effort,C6130V Virtual reality - C7210 Information services and centres - C7210N Information networks - C7810C Computer-aided instruction - C7300 Natural sciences computing,G09B5/00,General or Review (GEN); Theoretical or Mathematical (THR),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Robots that make sense: Transparent intelligence through augmented reality,"Rotsidis, Alexandros (1); Theodorou, Andreas (2); Wortham, Robert H. (1) ","(1) University of Bath, Bath, United Kingdom (2) Umeå University, Umeå, Sweden ",CEUR Workshop Proceedings,,CEUR-WS,v 2327,,2019,"ACMIUI-WS 2019 - Joint Proceedings of the ACM IUI 2019 Workshops, co-located with the 24th ACM Conference on Intelligent User Interfaces, ACM IUI 2019",2019,,16130073,,,,,"2019 Joint ACM IUI Workshops, ACMIUI-WS 2019",20-Mar-19,,"Autonomous robots can be difficult to understand by their developers, let alone by end users. Yet, as they become increasingly integral parts of our societies, the need for affordable easy to use tools to provide transparency grows. The rise of the smartphone and the improvements in mobile computing performance have gradually allowed Augmented Reality (AR) to become more mobile and affordable. In this paper we review relevant robot systems architecture and propose a new software tool to provide robot transparency through the use of AR technology. Our new tool, ABOD3-AR provides real-time graphical visualisation and debugging of a robot’s goals and priorities as a means for both designers and end users to gain a better mental model of the internal state and decision making processes taking place within a robot. We also report on our on-going research programme and planned studies to further understand the effects of transparency to naive users and experts. © 2019 Copyright for the individual papers by the papers’ authors. Copying permitted for private and academic purposes. This volume is published and copyrighted by its editors.",20,Intelligent robots,Artificial intelligence - Augmented reality - Decision making - Program debugging - Robots - Transparency - User interfaces,Decision making process - Integral part - Internal state - Mental model - Mobile augmented reality - Real time - Research programmes - Robot system,"722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 731.5 Robotics - 731.6 Robot Applications - 741.1 Light/Optics - 912.2 Management",,,"Number: EP/L016540/1, Acronym: EPSRC, Sponsor: Engineering and Physical Sciences Research Council; ",&lowast;We thank the EPSRC grant [EP/L016540/1] for funding Rotsidis and Theodorou. Both authors contributed equally to the paper.,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Robot Teleoperation with Augmented Reality Virtual Surrogates,"Walker, M.E. (1); Hedayati, H. (1); Szafir, D. (2) ","(1) Dept. of Comput. Sci., Univ. of Colorado Boulder, Boulder, CO, United States (2) Dept. of Comput. Sci., ATLAS Inst. Univ. of Colorado Boulder, Boulder, CO, United States ",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE Robot. & Autom. Soc.,"IEEE, Piscataway, NJ, USA",,202-10,2019,,,,,,978-1-5386-8555-6,10.1109/HRI.2019.8673306,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,"Daegu, South Korea","Teleoperation remains a dominant control paradigm for human interaction with robotic systems. However, teleoperation can be quite challenging, especially for novice users. Even experienced users may face difficulties or inefficiencies when operating a robot with unfamiliar and/or complex dynamics, such as industrial manipulators or aerial robots, as teleoperation forces users to focus on low-level aspects of robot control, rather than higher level goals regarding task completion, data analysis, and problem solving. We explore how advances in augmented reality (AR) may enable the design of novel teleoperation interfaces that increase operation effectiveness, support the user in conducting concurrent work, and decrease stress. Our key insight is that AR may be used in conjunction with prior work on predictive graphical interfaces such that a teleoperator controls a virtual robot surrogate, rather than directly operating the robot itself, providing the user with foresight regarding where the physical robot will end up and how it will get there. We present the design of two AR interfaces using such a surrogate: one focused on real-time control and one inspired by waypoint delegation. We compare these designs against a baseline teleoperation system in a laboratory experiment in which novice and expert users piloted an aerial robot to inspect an environment and analyze data. Our results revealed that the augmented reality prototypes provided several objective and subjective improvements, demonstrating the promise of leveraging AR to improve human-robot interactions.",41,,augmented reality - human-robot interaction - industrial manipulators - mobile robots - telerobotics,concurrent work - predictive graphical interfaces - teleoperator - virtual robot surrogate - physical robot - real-time control - baseline teleoperation system - aerial robot - augmented reality prototypes - human-robot interactions - robot teleoperation - augmented reality virtual surrogates - dominant control paradigm - human interaction - robotic systems - novice users - experienced users - complex dynamics - industrial manipulators - teleoperation forces users - low-level aspects - robot control - higher level goals,C6130V Virtual reality - C6180R Human-robot interaction - C3390C Mobile robots - C3390M Manipulators,B25J - B25J13/00 - G05D1/00,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Augmented Reality for Quick and Intuitive Robotic Packing Re-Programming,"Araiza-Illan, D. (1); De San Bernabe, A. (1); Fang Hongchao (1); Leong Yong Shin (1) ","(1) Adv. Remanufacturing & Technol. Centre, Agency for Sci., Technol. & Res., Singapore, Singapore ",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE Robot. & Autom. Soc.,"IEEE, Piscataway, NJ, USA",,664,2019,,,,,,978-1-5386-8555-6,10.1109/HRI.2019.8673327,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,"Daegu, South Korea","Current manufacturing applications are subject to constant changes in production orders for their robotic systems to adapt to the dynamic nature of the market. Hence, reprogramming robots needs to be a fast, easy and effective process. In this demonstration, we present an augmented reality (AR) interface using HoloLens. Our interface provides an intuitive platform to re-program a robotic packing application through simple hand gestures and the information gathered by the HoloLens' spatial mapping functionality.",3,,augmented reality - control engineering computing - gesture recognition - industrial robots - mobile robots - packaging - production engineering computing - robot programming,production orders - robotic systems - augmented reality interface - intuitive platform - robotic packing application - manufacturing applications - robotic packing reprogramming - augmented reality - AR interface - HoloLens,C3320 Control applications to materials handling - C7420 Control engineering computing - C7480 Production engineering computing - C3390C Mobile robots - C6130V Virtual reality - E1810 Packaging - E0410D Industrial applications of IT - E1550A Robotics,B65B - G05B15/00 - G05D1/00 - G06K9/00,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Audio augmented reality for human-object interactions,"Yang, Jing (1); Mattern, Friedemann (1) ","(1) ETH Zurich, Switzerland ",UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,p 408-412,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,10.1145/3341162.3349302,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,"In the past, augmented reality (AR) research focused mostly on visual augmentation, which requires a visual rendering device like head-mounted displays that are usually obtrusive, expensive, and socially unaccepted. In contrast, wearable audio headsets are already popularized and the auditory sense also plays an important role in everyday interactions with the environment. In this PhD project, we explore audio augmented reality (AAR) that augments objects with 3D sounds, which are spatialized virtually but are perceived as originating from real locations in the space. We intend to design, implement, and evaluate such AAR systems that enhance people’s intuitive and immersive interactions with objects in various consumer and industrial scenarios. By exploring AAR using pervasive and wearable devices, we hope to contribute to the vision of ubiquitous AR. © 2019 Association for Computing Machinery.",22,Wearable computers,Audio acoustics - Augmented reality - Helmet mounted displays - Ubiquitous computing,Audio augmented reality - Auditory sense - Head mounted displays - Human-object interaction - Industrial scenarios - Spatial audio - Wearable - Wearable devices,"722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 751.1 Acoustic Waves",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Projection-Based Augmented Reality Robot Prototype with Human-Awareness,Hyocheol Ro (1); Jung-Hyun Byun (1); Inhwan Kim (1); Yoon Jung Park (1); Kyuri Kim (1); Tack-Don Han (1) ,"(1) Media Syst. Lab., Yonsei Univ., Seoul, Korea, Republic of ",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE Robot. & Autom. Soc.,"IEEE, Piscataway, NJ, USA",,598-9,2019,,,,,,978-1-5386-8555-6,10.1109/HRI.2019.8673173,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,"Daegu, South Korea","Since projection augmented reality (AR) robot can provide a lot of information through projector, it can be useful in museums and art galleries that need to provide information to the crowd. Therefore, it is necessary to continue to interact with people, and human-aware path planning is also needed. We prototyped projection AR mobile robot implemented human-aware path planning and wrote about future research direction.",4,,art - augmented reality - control engineering computing - mobile robots - museums - path planning,projection-based augmented reality robot prototype - art galleries - human-aware path planning - projection AR mobile robot - museums,C3390C Mobile robots - C6130V Virtual reality - C7420 Control engineering computing - C7820 Humanities computing - C3120C Spatial variables control,G05B15/00 - G05D1/00 - G05D3/00 - B60W30/095,Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Activity recognition in outdoor sports environments: Smart data for end-users involving mobile pervasive augmented reality systems,"Pascoal, Rui Miguel (1); De Almeida, Ana (2); Sofia, Rute C. (3, 4) ","(1) Instituto Universitário de Lisboa (ISCTE-IUL), Information Sciences, Technologies and Architecture Research Center (ISTAR-IUL), Portugal (2) Instituto Universitário de Lisboa (ISCTE-IUL), Center for Informatics and Systems of the University of Coimbra (CISUC), Information Sciences, Technologies and Architecture Research Center (ISTAR-IUL), Portugal (3) Information Sciences, Technologies and Architecture Research Center (ISTAR-IUL), Portugal (4) Universidade Lusófona de Humanidades e Tecnologias Cognitive and People-Centric Computing (COPELABS), Portugal ",UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,emteq; et al.; Facebook; Google; Huawei; Nokia Bell Labs,"Association for Computing Machinery, Inc",,p 446-453,9-Sep-19,UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,2019,,,,9.78145E+12,10.1145/3341162.3349299,,"2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 2019 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2019","September 9, 2019 - September 13, 2019",,"Activity recognition is an increasingly relevant topic in the context of the most varied end-user services. In outdoor environments, activity recognition based on close-to-real-time information is key in providing awareness to the user about their surroundings in a timely and user-friendly manner, thus allowing to the user to improve its overall use (Quality of Experience). In this context, it is relevant to understand how data extracted from multiple sensors can be fused, interpreted and classified, to best provide feedback to the user. Having as target case Mobile Augmented Reality Systems for outdoor environments, this paper presents a first analysis on how smart data captured via multiple sensors can assist activity recognition and adequate feedback to the user. The paper also debates the existent restrictions imposed by applications’ usage in these environments, describing possible use scenarios and presenting results of an experiment for discriminating activities when using common sensors, such as the accelerometer. © 2019 Association for Computing Machinery.",18,Pattern recognition,Augmented reality - Feedback - Quality of service - Sports - Ubiquitous computing - Wearable computers,Activity recognition - Augmented reality systems - Auto-Adjustment System - Mobile augmented reality - Outdoor sports - Quality of experience (QoE) - Real-time information - SMART datum,"461.3 Biomechanics, Bionics and Biomimetics - 461.4 Ergonomics and Human Factors Engineering - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications - 731.1 Control Systems",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
XRaSE: towards virtually tangible software using augmented reality,"Mehra, R. (1); Sharma, V.S. (1); Kaulgud, V. (1); Podder, S. (1) ","(1) Accenture Labs., Bangalore, India ",2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). Proceedings,,"IEEE Computer Society, Los Alamitos, CA, USA",,1194-7,2019,,,,,,978-1-7281-2508-4,10.1109/ASE.2019.00135,,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,"San Diego, CA, USA","Software engineering has seen much progress in recent past including introduction of new methodologies, new paradigms for software teams, and from smaller monolithic applications to complex, intricate, and distributed software applications. However, the way we represent, discuss, and collaborate on software applications throughout the software development life cycle is still primarily using the source code, textual representations, or charts on 2D computer screens - the confines of which have long limited how we visualize and comprehend software systems. In this paper, we present XRaSE, a novel prototype implementation that leverages augmented reality to visualize a software application as a virtually tangible entity. This immersive approach is aimed at making activities like application comprehension, architecture analysis, knowledge communication, and analysis of a software's dynamic aspects, more intuitive, richer and collaborative.",16,,augmented reality - software architecture - software engineering - source code (software) - virtual reality,software engineering - distributed software applications - software application - software development life cycle - software systems - XRaSE - augmented reality - virtually tangible entity - application comprehension - virtually tangible software - architecture analysis - knowledge communication,C6110B Software engineering techniques - C6130V Virtual reality,G06F9/44,Practical (PRA),,,,Inspec,"Copyright 2020, The Institution of Engineering and Technology",Engineering Village,,,
Peppy: A paper-based augmented reality application to help children against dysgraphia,"Abid, Maira (1); Bhimra, Muhammad Ahmed (1); Mubeen, Muhammad (1); Zahid, Azan Bin (1); Shahid, Suleman (1) ","(1) Lahore University of Management Science, Lahore, Pakistan ","Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",Boise Osmo; Boise State University; et al.; Langan Barber Foundation; St. Luke's; STEM Action Center,"Association for Computing Machinery, Inc",,p 544-549,12-Jun-19,"Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",2019,,,,9.78145E+12,10.1145/3311927.3325311,,"18th ACM International Conference on Interaction Design and Children, IDC 2019","June 12, 2019 - June 15, 2019",,"Over the years, researchers have found an increase in learning problem among young children especially related to writing. Among these is a drop in dexterity or developmental dysgraphia due to a lack of fine motor skills. Also discovered is that handwritten activities on paper help work out these problems and with the advancement in technology in today's day and age, better combative measures can be taken against these problems. With Peppy, a mobile application using augmented reality (AR), our aim is to fight these problems using technology as well as paper by augmenting it into something that is both interactive and useful for child development. Although educational AR applications already exist, they don't focus on improving children's fine motor skills using paper-based exercises. Peppy brings enjoyable, thought-provoking and intriguing paper prototypes consisting of colouring, games, and puzzles to life through AR. © 2019 Association for Computing Machinery.",11,Paper,Augmented reality - Smartphones,Augmented reality applications - Child development - Children - Dysgraphia - Fine motors - Learning problem - Mobile applications - Paper prototypes,"718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications - 811.1 Pulp and Paper",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Prototyp AR: Prototyping and simulating complex systems with paper craft and augmented reality,"Kang, Seokbin (1); Norooz, Leyla (2); Bonsignore, Elizabeth (2); Byrne, Virginia (3); Clegg, Tamara (2, 3); Froehlich, Jon E. (4) ","(1) Computer Science, University of Washington, United States (2) College of Information Studies, University of Washington, United States (3) College of Education, University of Washington, United States (4) University of Maryland, Allen School of Computer Science and Engineering, University of Washington, United States ","Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",Boise Osmo; Boise State University; et al.; Langan Barber Foundation; St. Luke's; STEM Action Center,"Association for Computing Machinery, Inc",,p 253-266,12-Jun-19,"Proceedings of the 18th ACM International Conference on Interaction Design and Children, IDC 2019",2019,,,,9.78145E+12,10.1145/3311927.3323135,,"18th ACM International Conference on Interaction Design and Children, IDC 2019","June 12, 2019 - June 15, 2019",,"We introduce PrototypAR, an Augmented Reality (AR) system that allows children to rapidly build complex systems using paper crafts and to test their designs in a digital environment. PrototypAR combines lo-fidelity prototyping to facilitate iterative design, real-time AR feedback to scaffold learning, and a virtual simulation environment to support personalized experiments. Informed by three participatory design sessions, we developed three PrototypAR applications: build-a-bike, build-a-camera, and build-an-aquarium-each highlights different aspects of our system. To evaluate PrototypAR, we conducted four single-session qualitative evaluations with 21 children working in teams. Our findings show how children build and explore complex systems models, how they use AR scaffolds, and the challenges they face when conducting experiments with their own prototypes. © 2019 Copyright held by the owner/author(s).",78,Software prototyping,Augmented reality - Scaffolds - Virtual reality,Augmented reality systems - Children - Digital environment - Learning - Participatory design - Qualitative evaluations - Tangible interaction - Virtual simulation environments,"405.1 Construction Equipment - 723 Computer Software, Data Handling and Applications - 723.1 Computer Programming",,,"Number: IIS-1652339, Acronym: NSF, Sponsor: Norsk Sykepleierforbund; Number: IIS-441184, Acronym: NSF, Sponsor: Norsk Sykepleierforbund; ","We thank the children and researchers in the KIDSteam program at the University of Maryland, College Park. This work was supported by NSF Grant IIS-441184 and IIS-1652339.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Artitser: A mobile augmented reality in classroom interactive learning tool on biological science for junior high school students,"Ramos, Mary Joy H. (1); Comendador, Benilda Eleonor V. (1) ","(1) Polytechnic University of the Philippines, Graduate Program Office, Sta. Mesa, Manila, Philippines ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 135-139,27-May-19,ICETT 2019 - 2019 5th International Conference on Education and Training Technologies,2019,,,,9.78145E+12,10.1145/3337682.3337700,,"5th International Conference on Education and Training Technologies, ICETT 2019","May 27, 2019 - May 29, 2019",,"The authors implemented an Augment Reality (AR) to develop a mobile application called ARTitser that was utilized for Junior High School students as a supportive tool to learn Biological Science. The said application runs in iOS that integrates AR technology education which may help the teachers in facilitating delivery of the daily lessons using a realistic representation of objects for a better learning experience. It may also assist the teachers to monitor the performance of the students when using AR lesson. Moreover, the students may enjoy ARTitser application because they will see 3D objects associated to biology lesson. Based on the result of the survey, the respondents highly recommended the ARTitser with a grand mean of 4.52 which is highly acceptable. Thus, the said tool can be implemented in the Junior High School students which may motivate as well as improve the performance of the students. © 2019 Association for Computing Machinery.",11,Students,Augmented reality - Biology - Educational technology - Learning systems - Surveys,Biological science - Interactive learning tools - Junior high schools - Learning experiences - Mobile applications - Mobile augmented reality - Mobile Learning - Technology education,"461.9 Biology - 723 Computer Software, Data Handling and Applications - 901.2 Education",,,,,,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
A virtual and augmented reality platform for the training of first responders of the ambulance bus,"Koutitas, George (1); Smith, Kenneth Scott (2); Lawrence, Grayson (3); Metsis, Vangelis (4); Stamper, Clayton (4); Trahan, Mark (2); Lehr, Ted (5) ","(1) Electrical and Computer Engineering, Texas State University, San Marcos; TX, United States (2) School of Social Work, Texas State University, San Marcos; TX, United States (3) School of Art and Design, Texas State University, San Marcos; TX, United States (4) Computer Science Texas State University, San Marcos; TX, United States (5) Austin Smart City City of Austin, Austin; TX, United States ",ACM International Conference Proceeding Series,The Department of Computer Science and Engineering at UTA; The Human Centered Computing Laboratory (Heracleia) at UTA; The iPerform Industry-University NSF Center at UTA; The National Center for Scientific Research (NCSR)-Demokritos; The National Science Foundation (NSF),Association for Computing Machinery,,p 299-302,5-Jun-19,"Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019",2019,,,,9.78145E+12,10.1145/3316782.3321542,,"12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2019","June 5, 2019 - June 7, 2019",,"AmBus is a bus-sized ambulance that EMS personnel utilize during large-scale emergencies. Although EMS personnel receive annual training, evidence shows current training efforts leave some personnel unfamiliar with the AmBus system and unprepared to respond to an emergency. This work presents a novel interactive training application, utilizing emerging technologies in virtual and augmented reality, that can be delivered remotely to the distributed EMS personnel before they assemble, or as they are assembling. Our initial findings show that such an application can better prepare first responders to be as effective as possible in using the life-saving features of the AmBus. The methodology described in this work can be expanded to include other first responders, and, ultimately, lives may be saved because personnel are better prepared. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",9,Personnel training,Ambulances - Augmented reality - E-learning - Virtual reality,Annual training - Emerging technologies - First responders - Interactive training - Large-scale emergency - Virtual and augmented reality,"462.1 Biomedical Equipment, General - 723 Computer Software, Data Handling and Applications - 912.4 Personnel",,,,"The team would like to thank Commander Noble from Austin/Travis county EMS and our contact at the City of Austin: Marbenn Cayetano; for without their help, this project would not be possible. We would like to also acknowledge our student team: Jose Banuelos, James Bellian, Dante Cash, Elija Gaytan, Victoria Humphrey, Shivesh Jadon, Chloe Kjosa, Lorena Martinez, Samantha Roberts, Kayla Roebuck, Chaitanya Vyas, and Shashwat Vyas for their hard work and dedication.",,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Six types of audio that DEFY reality!: A taxonomy of audio augmented reality with examples,"Krzyzaniak, Michael (1); Frohlich, David (2); Jackson, Philip J.B. (3) ","(1) RITMO, University of Oslo, Oslo, Norway (2) DWRC, University of Surrey, Guildford, United Kingdom (3) CVSSP, University of Surrey, Guildford, United Kingdom ",ACM International Conference Proceeding Series,beLa; et al.; Holon; Routledge; Volvo; xln audio,Association for Computing Machinery,,p 160-167,18-Sep-19,"Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound, AM 2019",2019,,,,9.78145E+12,10.1145/3356590.3356615,,"14th International Audio Mostly Conference: A Journey in Sound, AM 2019","September 18, 2019 - September 20, 2019",,"In this paper we examine how the term 'Audio Augmented Reality' (AAR) is used in the literature, and how the concept is used in practice. In particular, AAR seems to refer to a variety of closely related concepts. In order to gain a deeper understanding of disparate work surrounding AAR, we present a taxonomy of these concepts and highlight both canonical examples in each category, as well as edge cases that help define the category boundaries. © 2019 ACM.",43,Audio acoustics,Acoustic waves - Argon - Audition - Augmented reality - Display devices - Taxonomies - Visual communication,Audio - Audio augmented reality - Audio overlay - Digital mediation - Enchantment - Listening - Music - Telepresence,"461.4 Ergonomics and Human Factors Engineering - 717.1 Optical Communication Systems - 722.2 Computer Peripheral Equipment - 723 Computer Software, Data Handling and Applications - 751.1 Acoustic Waves - 804 Chemical Products Generally - 903 Information Science",,,"Number: -, Acronym: -, Sponsor: Innovate UK; ","The work presented here was the result of several long discussions with many people. Thanks in particular to Will Buchanan, Russ Bradbury, and Charlotte Edmondson at RPPtv for their boundless creativity and insight. This work was cofunded by RPPtv and Innovate UK.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
The impression of virtual experience: Mobile augmented reality cloud solution,"Zhao, Ying (1); Tao, Wenyuan (1); Own, Chung-Ming (1) ","(1) School of Computer Software, Tianjin University, Jinnan Qu, Tianjin Shi, China ",ACM International Conference Proceeding Series,,Association for Computing Machinery,,p 40-49,12-Nov-19,"Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, MobiQuitous 2019",2019,,,,9.78145E+12,10.1145/3360774.3360811,,"16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, MobiQuitous 2019","November 12, 2019 - November 14, 2019",,"Mobile devices have become an enormous platform for augmented reality (AR) technology to meet the need of users to experience the charm of AR technology in anytime and anywhere. However, the development of mobile AR technology is currently in a dilemma. Both extensibility and real-time are often challenging to be taken into account. In our study, we explored cloud-based solutions and designed two systems, S-MARC, and D-MARC. The first system boldly tried the idea of cloud technology supporting mobile AR and explored more possibilities for combining location information. The cloud-based mobile AR design optimization scheme is proposed in our study details. Besides, we invited some users to carry out experimental studies, and their user evaluations showed that the mobile AR cloud solution was implemented in both systems and received excellent feedback in the review. The results show that the proposed two schemes are superior to others in the performance and efficiency, and help to improve the current dilemma of mobile AR and build up users' confidence in the application of mobile AR. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",37,User experience,Augmented reality - Cloud computing - Mobile computing - Ubiquitous computing,Cloud technologies - Cloud-based - Design optimization - First systems - Location information - Mobile augmented reality - Patch selection - User evaluations,"716 Telecommunication; Radar, Radio and Television - 722.4 Digital Computers and Systems - 723 Computer Software, Data Handling and Applications - 723.5 Computer Applications",,,,,Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
Perspectives on how to evaluate augmented reality technology tools for education: a systematic review,"da Silva, M.M.O. (1); Teixeira, J.M.X.N. (1); Cavalcante, P.S. (2); Teichrieb, V. (1) ","(1) Voxar Labs., Univ. Fed. de Pernambuco, Recife, Brazil (2) Centro de Educ., Univ. Fed. de Pernambuco, Recife, Brazil ",Journal of the Brazilian Computer Society,,"Springer, Germany","v 25, n 1",3 (18 pp.),Dec. 2019,,,,0104-6500,,,10.1186/s13173-019-0084-8,,,,,"Education has benefited from augmented reality's (AR) potential to promote interactive experiences both inside and outside the classroom. A systematic review was conducted on how AR's impact in the learning process has been evaluated. We selected papers from 2009 to 2017 in three databases, IEEE, ACM, and Science Direct, using an open-source crawler, and in one Brazilian Conference, SBIE. We followed the PRISMA protocol. Forty-five works were selected and used to extract data for our research. They were also analyzed according to quantitative and qualitative criteria. The results from all the papers are available in an online database. Results evidenced an increase in the number of papers evaluating the AR's impact in education. They also showed that AR has been applied in different areas and contexts. Most papers reported positive outcomes as a result of AR insertion. However, most studies lacked the involvement of the teacher and the use of multiple metrics to evaluate educational gains.",83,,augmented reality - computer aided instruction,ACM database - IEEE database - educational gains - qualitative criteria - quantitative criteria - PRISMA protocol - Brazilian Conference - open-source crawler - Science Direct - learning process - classroom - systematic review - augmented reality technology tools,C7810C Computer-aided instruction - C6130V Virtual reality,G09B5/00,Bibliography (BIB); Practical (PRA),,,,Inspec,"Copyright 2019, The Institution of Engineering and Technology",Engineering Village,,,
Leveraging augmented reality to create apps for people with visual disabilities: A case study in indoor navigation,"Yoon, Chris (1); Louie, Ryan (2); Ryan, Jeremy (2); Vu, Minh Khang (2); Bang, Hyegi (2); Derksen, William (2); Ruvolo, Paul (2) ","(1) Stanford University, Palo Alto, United States (2) Olin College of Engineering, Needham, United States ",ASSETS 2019 - 21st International ACM SIGACCESS Conference on Computers and Accessibility,ACM SIGACCESS,"Association for Computing Machinery, Inc",,p 210-221,24-Oct-19,ASSETS 2019 - 21st International ACM SIGACCESS Conference on Computers and Accessibility,2019,,,,9.78145E+12,10.1145/3308561.3353788,,"21st International ACM SIGACCESS Conference on Computers and Accessibility, ASSETS 2019","October 28, 2019 - October 30, 2019",,"The introduction of augmented reality technology to iOS and Android enables, for the first time, mainstream smartphones to estimate their own motion in 3D space with high accuracy. For assistive technology researchers, this development presents a potential opportunity. In this spirit, we present our work leveraging these technologies to create a smartphone app to empower people who are visually impaired to more easily navigate indoor environments. Our app, Clew, allows users to record routes and then load them, at any time, providing automatic guidance (using haptic, speech, and sound feedback) along the route. We present our user-centered design process, Clew's system architecture and technical details, and both small and large-scale evaluations of the app. We discuss opportunities, pitfalls, and design guidelines for utilizing augmented reality for orientation and mobility apps. Our work expands the capabilities of technology for orientation and mobility that can be distributed on a mass scale. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",41,User centered design,Augmented reality - Indoor positioning systems - Smartphones,Assistive - Assistive technology - Augmented reality technology - In-door navigations - Indoor environment - System architectures - Technical details - User-centered design process,"718.1 Telephone Systems and Equipment - 723 Computer Software, Data Handling and Applications",,,,"We would like to acknowledge The Peabody Foundation of Boston, MA for funding this work. Additionally, Diego Berny, Dieter Brehm, Daniel Connolly, Laura Etori, Anna Griffin, Lauren Gulland, Megan Ku, Simran Malhi, Louise Nielsen, Xierui Shen, and Emily Wang contributed to this work.",Open Access,Compendex,"Compilation and indexing terms, Copyright 2020 Elsevier Inc.",Engineering Village,,,
