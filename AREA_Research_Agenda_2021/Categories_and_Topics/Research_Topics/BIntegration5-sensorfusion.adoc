[[ra-BIntegration5-sensorfusion]]

# Dynamic Distribution of Processing capacity via a Mesh network of Wearable Devices

# Description
Deploying Enterprise AR solutions via head mounted displays is inherently advantageous when simultaneous use of both hands is required. However, currently HMD performance is limited due to technological constraints, such as battery life or heat dissipation needs. One solution is to offload and distribute processing needs  dynamically across other devices worn by the user (wrist, dedicated sensors, exoskeletons, etc.) into a ‘mesh network’ of processors, hosted in the various wearables. This would not only to decrease processing requirements of an HMD but also enrich data feeds necessary and would provide additional data about the wearer and the environment. (e.g. pose/posture of different body parts). This however presents challenges in reintegrating separate data streams into a cohesive model either on the edge or in the mist/fog/cloud.

This topic will include in its scope calculating the utility/benefit ratio of different approaches focusing modeling optimums across different configurations.


# Prior Research
To be generated via FindAR

# Key Words
Body-worn mesh networks, sensor configuration, sensor control, head-mounted display device, wearables, distributed computing, cloud computing, sensor fusion, optimization, body area networks, off-display sensors, off-display processing

## FindAR Terms
body sensor networks, personal area networks, mobile cloud computing, distributed computing, body area networks, wireless sensor networks, sensor fusion

# Research Agenda Categories
Display Devices, Standards, Business, Technology

# Stakeholders
HMD manufacturers, AR display device designers, sensor developers, body area network manufacturers, OEM manufacturers, integrated solution and software developers

# Position on X and Y axes (1-5)

# Reasons this topic is important for AREA members
Employees in AREA member companies frequently need simultaneous use of both their hands. Use of head-mounted displays represents a significant productivity improvement for those who need to use both hands and contextually-positioned information to perform complex tasks. This research topic will contribute addressing current limitations of head-mounted displays and increases the quality of contextual data. The outcomes will reduce the barriers to AR display device adoption in the enterprise.

# Possible Methodologies
Studies of wearable sensors, low-latency body area network technologies and sensor fusion processors and their synchronization will contribute significantly to the goals of this project. A research platform composed of configurable head-mounted AR display components could be designed. Sensor data tracing systems may need to be tested or developed to provide a complete understanding of architectural choices.

# Expected Impact Timeframe
Long-term

# Research Program
This topic is at the intersection of multiple domains including but not limited to sensor configuration and control, sensor registrations, data filtering, sensor fusion, user context capture and body area networks. The result could contribute to design of new HMD architectures, high-performance network-based resources and services (eg., 5G).

# Miscellaneous Notes
Studies of optical and IMU sensor fusion for AR HMDs date have been published as early as 2003. A https://www.researchgate.net/publication/281764749_An_Inertial_and_Optical_Sensor_Fusion_Approach_for_Six_Degree-of-Freedom_Pose_Estimation[recent publication on the topic] of 6DOF pose proves that the approach is very reliable.

# Authors
Peter Orban, Christine Perey
