[[ra-BIntegration1-sensorfusion]]

# Title
Wearable Sensor Fusion: Integrating Data streams from HMDs and other Wearables

# Description
Deploying Enterprise AR solutions via head mounted displays is inherently advantageous when simultaneous use of both hands is required. However, currently HMD performance is limited due to technological constraints, such as battery life or heat dissipation needs. One solution is to offload and distribute sensor data collection dynamically across other devices worn by the user (wrist, dedicated sensors, exoskeletons, etc) into a ‘mesh network’ of wearables. This would not only to decrease processing requirements but also enrich data feeds necessary and would provide additional data about the wearer and the environment. (e.g. pose/posture of different body parts). This however presents challenges in reintegrating separate data streams into a cohesive model either on the edge or in the mist/fog/cloud.
This topic will include in its scope calculating the utility/benefit ratio of different approaches focusing modeling optimums across different configurations.

# Prior Research
To be generated via FindAR

# Key Words
Wearables, distributed computing, cloud, fog, mist, optimization,

## FindAR Terms
Data integration, integration, business

# Stakeholders
IT leaders, IT managers, OEM manufacturers, ISVs

# Position on X and Y axes (1-5)

# Reasons this topic is important for AREA members
Wearables represent a significant potential in improving the productivity of all who need to use both hands for work. Any strategy and solution that helps to address their current limitations will help remove the barriers to adoption in the Enterprise.

# Possible Methodologies
Lab experiments and in-field tests

# Expected Impact Timeframe
Long-term

# Research Program
Text

# Miscellaneous Notes
Text

# Author
Peter Orban
